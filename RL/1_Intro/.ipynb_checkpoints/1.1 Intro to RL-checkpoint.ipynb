{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intro to RL w Python\n",
    "\n",
    "### Markov Decision Processes\n",
    "__Markov__ implies the nest state only depends on the current state (not the past) We can make a problem Markov by bundling up all the information of the past along with knowledge available to the agent into a single state vector. This state might encompass all information about the environment, in which case the environment is __fully observable__.\n",
    "\n",
    "__MDPs__ Are characterized by having state, rewards, and actions.  \n",
    "\n",
    "<img src='https://miro.medium.com/max/1362/1*7cuAqjQ97x1H_sBIeAVVZg.png'>\n",
    "\n",
    "### Types of MDPs\n",
    "__Bandits__ where actions taken have no effect on the environment. These are very useful in business for many things. __No Delayed rewards__ -> Since the environment isn't affected by agent's actions, there is no \"long-term planning\" involved the agent simply makes the most optimal action in each state. The environment might change on its own, but these changes cannot be anticipated as a result of the agent's decisions.\n",
    "\n",
    "__MDP/POMDP__ Agent may have full observability or partial observability.\n",
    "\n",
    "__Deterministic/Stochastic__ State change as a result of prev. state and action may be a function (same every time) or it may be random. \n",
    "\n",
    "*In the case of stochasticity, can an agent's actions change the probabilities of the state function -> Maybe?  Envokes the thought that an agent's actions can definitely change the reward function as well as the state function\n",
    "\n",
    "This is incorrect, the reward function is fixed, the state-state transition function is what changes as a result of agent action. For stochasticity this means changes in probabilities, for deterministic this means changes in the function*\n",
    "\n",
    "### RL Algorithm Components\n",
    "\n",
    "__Model__ does the agent model the dynamics of the environment. (optional) \n",
    "\n",
    "__Policy__ What is the rule that maps agent's state to an action (oftentimes this is a greedy rule)\n",
    "\n",
    "__Value Function__ Given current state, what is the expected CUMULATIVe future reward for all available actions.\n",
    "\n",
    "Most State of the art DL models use neural networks to approximate the value function. *I'm not sure if these architectures are model-less approaches... I also don't know what an actor-critic algorithm is.*\n",
    "\n",
    "### Methods of Reinforcement Learning\n",
    "\n",
    "__Dynamic Programming__ perfectly optimized choices for a perfectly known environment. \n",
    "\n",
    "__TD Learning__ Start with a guess of expected rewards and use trial and error to update expectations\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reinforcement Learning\n",
    "__Agents__ take action in some kind of __environment__ to get a reward. Interested in a long term strategy for the agent. Trade off of exploitation vs exploration.\n",
    "\n",
    "__Optimal Control__ create a controller to minimize a measure of a dynamic system's behavior over time. -> dynamic programming\n",
    "\n",
    "__Trial-and-Error__ Reinforce vs punish certain types of actions by the agent.\n",
    "\n",
    "__Temporal Difference (TD) Learning__ learn how to predict a quantity that depends on future values of a given signal. Differences in predictions over successive time steps to drive the learning process. Prediction at current time step is updated to bring ti closer to the predicted signal at the next time step. Used in RL to predict the total amount of reward expected over the future, then used to predict other quantities.\n",
    "\n",
    "### Terminology\n",
    "__Agent__ system embedded in an environment takes acrions to change the state of the environment.\n",
    "\n",
    "__Environment__ Often in RL are Markov Decision Processes a tuple\n",
    "(S,A,P,R,l)\n",
    "- S is a finite set of states.\n",
    "- A is a finite set of actions.\n",
    "- P is a state transition prob matrix\n",
    "- R is a reward function\n",
    "- l a discount factor\n",
    "\n",
    "State transition probs also enforce the rules of the environment, impossible moves have a strate transition prob of 0.\n",
    "\n",
    "__Reward__ Maps states to the reward. This is info that the agent uses to learn how to navigate the environment. Much effort is designing the reward function (and avoiding the problem of sparse rewards.) The reward function calculates the discounted reward of all future timestamps. (discounted by l -[0,1])\n",
    "\n",
    "__Value Function__  the expected return starting from state s. Tells how beneficial being in a certain state is.\n",
    "- State Value Function - value for being in state s\n",
    "- Action Value Function - Value for using action a in a certain state s.\n",
    "\n",
    "__Policy__ - Defines the behavior of the agent in the Markov Decison Process (MDP). Policies are distributions over actions given states. prob of taking action a from state s."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Further Reading\n",
    "\n",
    "[Wikipedia RL](https://en.wikipedia.org/wiki/Reinforcement_learning#Comparison_of_reinforcement_learning_algorithms)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
