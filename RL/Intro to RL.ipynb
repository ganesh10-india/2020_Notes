{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intro to RL w Python\n",
    "\n",
    "\n",
    "### Reinforcement Learning\n",
    "__Agents__ take action in some kind of __environment__ to get a reward. Interested in a long term strategy for the agent. Trade off of exploitation vs exploration.\n",
    "\n",
    "__Optimal Control__ create a controller to minimize a measure of a dynamic system's behavior over time. -> dynamic programming\n",
    "\n",
    "__Trial-and-Error__ Reinforce vs punish certain types of actions by the agent.\n",
    "\n",
    "__Temporal Difference (TD) Learning__ learn how to predict a quantity that depends on future values of a given signal. Differences in predictions over successive time steps to drive the learning process. Prediction at current time step is updated to bring ti closer to the predicted signal at the next time step. Used in RL to predict the total amount of reward expected over the future, then used to predict other quantities.\n",
    "\n",
    "### Terminology\n",
    "__Agent__ system embedded in an environment takes acrions to change the state of the environment.\n",
    "\n",
    "__Environment__ Often in RL are Markov Decision Processes a tuple\n",
    "(S,A,P,R,l)\n",
    "- S is a finite set of states.\n",
    "- A is a finite set of actions.\n",
    "- P is a state transition prob matrix\n",
    "- R is a reward function\n",
    "- l a discount factor\n",
    "\n",
    "State transition probs also enforce the rules of the environment, impossible moves have a strate transition prob of 0.\n",
    "\n",
    "__Reward__ Maps states to the reward. This is info that the agent uses to learn how to navigate the environment. Much effort is designing the reward function (and avoiding the problem of sparse rewards.) The reward function calculates the discounted reward of all future timestamps. (discounted by l -[0,1])\n",
    "\n",
    "__Value Function__  the expected return starting from state s. Tells how beneficial being in a certain state is.\n",
    "- State Value Function - value for being in state s\n",
    "- Action Value Function - Value for using action a in a certain state s.\n",
    "\n",
    "__Policy__ - Defines the behavior of the agent in the Markov Decison Process (MDP). Policies are distributions over actions given states. prob of taking action a from state s.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Further Reading\n",
    "\n",
    "[Wikipedia RL](https://en.wikipedia.org/wiki/Reinforcement_learning#Comparison_of_reinforcement_learning_algorithms)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
