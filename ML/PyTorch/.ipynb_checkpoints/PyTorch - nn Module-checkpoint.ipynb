{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch NN Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def _print(val):\n",
    "    print(val, val.shape, '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models in PyTorch\n",
    "Model can be definde by subclassing the torch.nn.Module class. Define the parameters, then define the forward propagation of inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class myNetwork(nn.Module):\n",
    "    def __init(self):\n",
    "        super(myNetwork, self).__init__()\n",
    "        self.linear1 = nn.Linear(D_in, H_out)\n",
    "        self.linear2 = nn.Linear(H_out, D_out)\n",
    "    def forward(self, data):\n",
    "        out = F.relu(self.linear1(data))\n",
    "        out = self.linear2(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving and Loading Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = {\n",
    "    'epoch':epoch+1\n",
    "    'state_dict':model.state_dict()\n",
    "    'optim_dict':optimider.state_dict()\n",
    "}\n",
    "utils.save_checkpoint(state,\n",
    "                     isbest=True,\n",
    "                     checkpoint='filepath')\n",
    "utils.load_checkpoint(restore_path, \n",
    "                      model, \n",
    "                      optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convolutions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([20, 16, 50, 100])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([20, 33, 26, 100])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = nn.Conv2d(16,33,(3,5), \n",
    "              stride=(2,1), \n",
    "              padding = (4,2), \n",
    "              dilation=(3,1))\n",
    "input = Variable(torch.randn(20,16,50,100))\n",
    "print(input.shape)\n",
    "m(input).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pooling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 0.,  1.,  2.,  3.],\n",
      "         [ 4.,  5.,  6.,  7.],\n",
      "         [ 8.,  9., 10., 11.],\n",
      "         [12., 13., 14., 15.]]], dtype=torch.float64) torch.Size([1, 4, 4]) \n",
      "\n",
      "tensor([[[ 5.,  7.],\n",
      "         [13., 15.]]], dtype=torch.float64) torch.Size([1, 2, 2]) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Max Pooling - cuts the size in have by retaining only the max \n",
    "value in a subset of a matrix. Outputs a smaller matrix with\n",
    "the combined max values for the subcells.\n",
    "\n",
    "Reduces the number of parameters to learn and provides basic \n",
    " translation invariance to the internal representation.\n",
    "\"\"\"\n",
    "m = nn.MaxPool2d((2,2))\n",
    "x = torch.arange(16, dtype = torch.float64).reshape(-1,4).unsqueeze(0)\n",
    "pooled= m(x)\n",
    "_print(x)\n",
    "_print(pooled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 2.5000,  4.5000],\n",
      "         [10.5000, 12.5000]]], dtype=torch.float64) torch.Size([1, 2, 2]) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "m = nn.AvgPool2d(2)\n",
    "_print(m(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Non-Linear Activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.6745,  0.9691,  1.0134],\n",
      "        [-0.3971,  0.3070, -0.0195],\n",
      "        [ 0.0943, -0.8966,  0.7574]]) torch.Size([3, 3]) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "x = Variable(torch.randn(3,3))\n",
    "_print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0000, 0.9691, 1.0134],\n",
      "        [0.0000, 0.3070, 0.0000],\n",
      "        [0.0943, 0.0000, 0.7574]]) torch.Size([3, 3]) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "m = nn.ReLU()\n",
    "_print(m(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-6.7450e-03,  9.6910e-01,  1.0134e+00],\n",
      "        [-3.9710e-03,  3.0700e-01, -1.9512e-04],\n",
      "        [ 9.4303e-02, -8.9660e-03,  7.5740e-01]]) torch.Size([3, 3]) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "m = nn.LeakyReLU()\n",
    "_print(m(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.3375, 0.7249, 0.7337],\n",
      "        [0.4020, 0.5762, 0.4951],\n",
      "        [0.5236, 0.2897, 0.6808]]) torch.Size([3, 3]) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "m = nn.Sigmoid()\n",
    "_print(m(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.2234, 0.5986, 0.4695],\n",
      "        [0.2948, 0.3087, 0.1671],\n",
      "        [0.4819, 0.0927, 0.3634]]) torch.Size([3, 3]) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "m = nn.Softmax(dim=0)\n",
    "_print(m(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "nn.BatchNorm1d(\n",
    "    num_features,\n",
    "    eps=1e-05,\n",
    "    momentum=0.1,\n",
    "    affine=True,\n",
    "    track_running_stats=True,\n",
    ")\n",
    "Docstring:     \n",
    "Applies Batch Normalization over a 2D or 3D input (a mini-batch of 1D\n",
    "inputs with optional additional channel dimension) as described in the paper\n",
    "`Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift`_ .\n",
    "\"\"\"\n",
    "\n",
    "nn.BatchNorm1d(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recurrent Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 3, 20])\n"
     ]
    }
   ],
   "source": [
    "    \n",
    "\"\"\"\n",
    "Applies a multi-layer Elman RNN with :math:`tanh` or :math:`ReLU` non-linearity to an\n",
    "input sequence.\n",
    "\n",
    "\n",
    "For each element in the input sequence, each layer computes the following\n",
    "function:\n",
    "\n",
    ".. math::\n",
    "    h_t = \\text{tanh}(W_{ih} x_t + b_{ih} + W_{hh} h_{(t-1)} + b_{hh})\n",
    "\n",
    "where :math:`h_t` is the hidden state at time `t`, :math:`x_t` is\n",
    "the input at time `t`, and :math:`h_{(t-1)}` is the hidden state of the\n",
    "previous layer at time `t-1` or the initial hidden state at time `0`.\n",
    "If :attr:`nonlinearity` is ``'relu'``, then `ReLU` is used instead of `tanh`.\n",
    "\n",
    "Args:\n",
    "    input_size: The number of expected features in the input `x`\n",
    "    hidden_size: The number of features in the hidden state `h`\n",
    "    num_layers: Number of recurrent layers. E.g., setting ``num_layers=2``\n",
    "        would mean stacking two RNNs together to form a `stacked RNN`,\n",
    "        with the second RNN taking in outputs of the first RNN and\n",
    "        computing the final results. Default: 1\n",
    "    nonlinearity: The non-linearity to use. Can be either ``'tanh'`` or ``'relu'``. Default: ``'tanh'``\n",
    "    bias: If ``False``, then the layer does not use bias weights `b_ih` and `b_hh`.\n",
    "        Default: ``True``\n",
    "    batch_first: If ``True``, then the input and output tensors are provided\n",
    "        as `(batch, seq, feature)`. Default: ``False``\n",
    "    dropout: If non-zero, introduces a `Dropout` layer on the outputs of each\n",
    "        RNN layer except the last layer, with dropout probability equal to\n",
    "        :attr:`dropout`. Default: 0\n",
    "    bidirectional: If ``True``, becomes a bidirectional RNN. Default: ``False``\n",
    "\n",
    "Inputs: input, h_0\n",
    "    - **input** of shape `(seq_len, batch, input_size)`: tensor containing the features\n",
    "      of the input sequence. The input can also be a packed variable length\n",
    "      sequence. See :func:`torch.nn.utils.rnn.pack_padded_sequence`\n",
    "      or :func:`torch.nn.utils.rnn.pack_sequence`\n",
    "      for details.\n",
    "    - **h_0** of shape `(num_layers * num_directions, batch, hidden_size)`: tensor\n",
    "      containing the initial hidden state for each element in the batch.\n",
    "      Defaults to zero if not provided. If the RNN is bidirectional,\n",
    "      num_directions should be 2, else it should be 1.\n",
    "\n",
    "Outputs: output, h_n\n",
    "    - **output** of shape `(seq_len, batch, num_directions * hidden_size)`: tensor\n",
    "      containing the output features (`h_t`) from the last layer of the RNN,\n",
    "      for each `t`.  If a :class:`torch.nn.utils.rnn.PackedSequence` has\n",
    "      been given as the input, the output will also be a packed sequence.\n",
    "\n",
    "      For the unpacked case, the directions can be separated\n",
    "      using ``output.view(seq_len, batch, num_directions, hidden_size)``,\n",
    "      with forward and backward being direction `0` and `1` respectively.\n",
    "      Similarly, the directions can be separated in the packed case.\n",
    "    - **h_n** of shape `(num_layers * num_directions, batch, hidden_size)`: tensor\n",
    "      containing the hidden state for `t = seq_len`.\n",
    "\n",
    "      Like *output*, the layers can be separated using\n",
    "      ``h_n.view(num_layers, num_directions, batch, hidden_size)``.\n",
    "\n",
    "Shape:\n",
    "    - Input1: :math:`(L, N, H_{in})` tensor containing input features where\n",
    "      :math:`H_{in}=\\text{input\\_size}` and `L` represents a sequence length.\n",
    "    - Input2: :math:`(S, N, H_{out})` tensor\n",
    "      containing the initial hidden state for each element in the batch.\n",
    "      :math:`H_{out}=\\text{hidden\\_size}`\n",
    "      Defaults to zero if not provided. where :math:`S=\\text{num\\_layers} * \\text{num\\_directions}`\n",
    "      If the RNN is bidirectional, num_directions should be 2, else it should be 1.\n",
    "    - Output1: :math:`(L, N, H_{all})` where :math:`H_{all}=\\text{num\\_directions} * \\text{hidden\\_size}`\n",
    "    - Output2: :math:`(S, N, H_{out})` tensor containing the next hidden state\n",
    "      for each element in the batch\n",
    "\n",
    "Attributes:\n",
    "    weight_ih_l[k]: the learnable input-hidden weights of the k-th layer,\n",
    "        of shape `(hidden_size, input_size)` for `k = 0`. Otherwise, the shape is\n",
    "        `(hidden_size, num_directions * hidden_size)`\n",
    "    weight_hh_l[k]: the learnable hidden-hidden weights of the k-th layer,\n",
    "        of shape `(hidden_size, hidden_size)`\n",
    "    bias_ih_l[k]: the learnable input-hidden bias of the k-th layer,\n",
    "        of shape `(hidden_size)`\n",
    "    bias_hh_l[k]: the learnable hidden-hidden bias of the k-th layer,\n",
    "        of shape `(hidden_size)`\n",
    "\n",
    ".. note::\n",
    "    All the weights and biases are initialized from :math:`\\mathcal{U}(-\\sqrt{k}, \\sqrt{k})`\n",
    "    where :math:`k = \\frac{1}{\\text{hidden\\_size}}`\n",
    "\n",
    ".. include:: cudnn_persistent_rnn.rst\n",
    "\n",
    "Examples::\n",
    "\n",
    "    >>> rnn = nn.RNN(10, 20, 2)\n",
    "    >>> input = torch.randn(5, 3, 10)\n",
    "    >>> h0 = torch.randn(2, 3, 20)\n",
    "    >>> output, hn = rnn(input, h0)\n",
    "\"\"\"\n",
    "#nn.RNN(input_size, hidden_size, num_recurrent_layers, nonlinearity,)\n",
    "x = Variable(torch.randn(5,3,10))\n",
    "rnn = nn.RNN(10,20,2)\n",
    "h0 = Variable(torch.randn(2,3,20))\n",
    "out, hn = rnn(x, h0)\n",
    "print(out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 3, 20])\n"
     ]
    }
   ],
   "source": [
    "lstm = nn.LSTM(10,20,2)\n",
    "c0 = Variable(torch.randn(2,3,20))\n",
    "out, hn = lstm(x,(h0,c0))\n",
    "print(out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 3, 20])\n"
     ]
    }
   ],
   "source": [
    "gru = nn.GRU(10,20,2)\n",
    "out,hnn = gru(x, h0)\n",
    "print(out.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([128, 2])\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "nn.Linear(in_features, out_features, bias=True)\n",
    "Docstring:     \n",
    "Applies a linear transformation to the incoming data: :math:`y = xA^T + b`\n",
    "\n",
    "Args:\n",
    "    in_features: size of each input sample\n",
    "    out_features: size of each output sample\n",
    "\"\"\"\n",
    "x = Variable(torch.randn(128,20))\n",
    "m = nn.Linear(20,2)\n",
    "print(m(x).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0000, -0.0000],\n",
       "        [-1.4435, -0.1157],\n",
       "        [ 0.0000, -2.5524],\n",
       "        [ 0.0000, -0.0000],\n",
       "        [ 2.6681, -1.1160]])"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#zeroes out some of the values in the input tensor\n",
    "m = nn.Dropout(p=0.5)\n",
    "input = Variable(torch.randn(5, 2))\n",
    "output = m(input)\n",
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-1.0281, -0.1043, -0.9801],\n",
       "         [ 0.0454,  1.2075, -0.5769],\n",
       "         [ 0.0454,  1.2075, -0.5769]],\n",
       "\n",
       "        [[ 0.7194,  1.8701, -0.5869],\n",
       "         [ 0.8274,  0.9918,  0.6741],\n",
       "         [-1.0281, -0.1043, -0.9801]]], grad_fn=<EmbeddingBackward>)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the input is an integer that selects the embedding\n",
    "embedding = nn.Embedding(10,3)\n",
    "x = torch.tensor([[2,1,1],[3,5,2]], dtype = torch.long)\n",
    "embedding(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cosine Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([100])\n"
     ]
    }
   ],
   "source": [
    "input1 = Variable(torch.randn(100, 128))\n",
    "input2 = Variable(torch.randn(100, 128))\n",
    "cos = nn.CosineSimilarity()\n",
    "print(cos(input1,input2).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.1297, grad_fn=<L1LossBackward>)\n"
     ]
    }
   ],
   "source": [
    "loss = nn.L1Loss()\n",
    "pred = Variable(torch.randn(1,10), requires_grad=True)\n",
    "target = Variable(torch.randn(1,10))\n",
    "print(loss(pred,target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.8602, grad_fn=<MseLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "loss = nn.MSELoss()\n",
    "print(loss(pred,target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.7376, grad_fn=<NllLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "loss = nn.CrossEntropyLoss()\n",
    "target = Variable(torch.LongTensor(1).random_(5))\n",
    "target = Variable(torch.randint(5, (1,),dtype=torch.long))\n",
    "print(loss(pred,target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([4])"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\program files\\python36\\lib\\site-packages\\ipykernel_launcher.py:2: UserWarning: nn.init.normal is now deprecated in favor of nn.init.normal_.\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[-1.1021, -2.0721, -1.0736, -0.4213,  1.4890],\n",
       "        [-0.1656, -0.0502, -1.2592, -0.7588, -0.2377],\n",
       "        [ 0.2969,  0.1076,  0.6622,  0.1856, -0.0085]])"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w = torch.empty(3,5)\n",
    "nn.init.normal(w) #operates inplace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
