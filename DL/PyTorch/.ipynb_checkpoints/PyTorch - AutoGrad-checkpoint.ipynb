{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch - AutoGrad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def _print(val):\n",
    "    print(val,'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None \n",
      "\n",
      "<AddBackward0 object at 0x0000023BFACF1EB8> \n",
      "\n"
     ]
    }
   ],
   "source": [
    "x = torch.ones(2,2, requires_grad=True)\n",
    "y = x + 2\n",
    "_print(x.grad_fn) #first Tensor in graph has no grad_fn\n",
    "_print(y.grad_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[27., 27.],\n",
      "        [27., 27.]], grad_fn=<MulBackward0>) tensor(27., grad_fn=<MeanBackward0>)\n"
     ]
    }
   ],
   "source": [
    "z = y*y*3\n",
    "out = z.mean()\n",
    "print(z,out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to compute the derivatives, you can call .backward() on a Tensor. If Tensor is a scalar (i.e. it holds a one element data), you donâ€™t need to specify any arguments to backward(), however if it has more elements, you need to specify a gradient argument that is a tensor of matching shape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[3., 3.],\n",
      "        [3., 3.]], grad_fn=<AddBackward0>) \n",
      "\n",
      "tensor([[4.5000, 4.5000],\n",
      "        [4.5000, 4.5000]]) \n",
      "\n",
      "None \n",
      "\n",
      "None \n",
      "\n",
      "None \n",
      "\n"
     ]
    }
   ],
   "source": [
    "_print(y)\n",
    "out.backward()\n",
    "_print(x.grad) #doutdx = (1/4)*6*3 = 4.5\n",
    "_print(y.grad) #dydx  = 1\n",
    "_print(z.grad)  #dzdy = 6y\n",
    "_print(out.grad) #doutdz = 1/z.shape[0] = 1/4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The derivative of a vector valued function z = f(y) is the __Jacobian Matrix.__ \n",
    "- ((dz1/dy1, ... , dz1/dyn)\n",
    "- (dz2/dy1, ... , dz2/dyn)\n",
    "- (       , ... ,        )\n",
    "- ( dzn/dy1, ... , dzn/dyn))\n",
    " \n",
    " Autograd is a engine for computing a vector - Jacobian product. At each step in the the jacobian of the Function is multiplied by the Gradient-vector of the previous step.\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Autograd Mechanics\n",
    "\n",
    "[link](https://pytorch.org/docs/stable/notes/autograd.html)\n",
    "\n",
    "Understanding Autograd leads to more efficient code.Tensors have __.requires_grad__ atrribute that allows exclusion of subgraphs from gradient computation. If a Function has an input that requires_gradient, the output will also require_gradient.\n",
    "\n",
    "You can freeze some parameters to make them untrainable -> makes the entire training process more memory and computationally efficient. You can freeze the encoder and only require gradients for the encoder.\n",
    "\n",
    "Autograd creates a directed acyclic graph with leaves are the input tensors and roots are the output tensors. Graph of Function objects (which can be .aaply()ed to the inputs) During forward pass, builds up a graph representing the function that computes the gradient. \n",
    "\n",
    "The graph is computed from scratch at each iteration, allowing control flow statements (if,then,else) You can change the size of the graph at every iteration, what is executed is differentiated.\n",
    "\n",
    "### In-Place Operations\n",
    "In-place operations are discouraged. They dont help memory usage much. Autograd has buffering and aggressive memory freeing schedules that make memory management very efficient.\n",
    "\n",
    "- In-place ops can overwrite values required for gradient calculation\n",
    "- Every in-place op requires a complete rewrite of the computational graph. Out-of-place ops create new objects that reference the old graph \n",
    "\n",
    "*\"while in-place operations, require changing the creator of all inputs to the Function representing this operation. This can be tricky, especially if there are many Tensors that reference the same storage (e.g. created by indexing or transposing), and in-place functions will actually raise an error if the storage of modified inputs is referenced by any other Tensor.\"*\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variables (Deprecated)\n",
    "\n",
    "__Variable__ wraps a Tensor. Support all Tensor methods, as well as the __.backward()__ method to perform backpropagation. Have some variable that __requires_grad__, that leads to computation of the loss function. Loss.backward() will compute the gradient of the loss function w.r.t the trainable parameteres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1.],\n",
      "        [1., 1.]])\n",
      "False\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable\n",
    "\n",
    "X = Variable(torch.ones(2,2),)\n",
    "print(X)\n",
    "print(X.requires_grad) #variables do not default .requires_grad=True\n",
    "X.requires_grad = True\n",
    "print(X.requires_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To train a part of a pretrained model, set __.requires_grad = True__ at the entrance of the subgraph to be trained. \n",
    "\n",
    "Add a Function to create another Variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[3., 3.],\n",
      "        [3., 3.]], grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "y = X + 2 #dx= 1\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[18., 18.],\n",
      "        [18., 18.]], grad_fn=<MulBackward0>)\n",
      "tensor(18., grad_fn=<MeanBackward0>)\n"
     ]
    }
   ],
   "source": [
    "z = 2*(y**2) #dy = 4*y\n",
    "print(z)\n",
    "out = z.mean() #dz =1/n = 1/4\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute Gradient\n",
    "Starts computation of gradient at Variable.backward() (this is usually the Cost function in Deep Learning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dout/dxi = 1/4 * 4(y) = xi + 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[3., 3.],\n",
      "        [3., 3.]])\n"
     ]
    }
   ],
   "source": [
    "out.backward()\n",
    "print(X.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[13.5000, 13.5000],\n",
      "        [13.5000, 13.5000]])\n"
     ]
    }
   ],
   "source": [
    "X = Variable(torch.ones(2,2),requires_grad=True)\n",
    "y = X + 2\n",
    "z = 2*(y**3)\n",
    "out = z.mean()\n",
    "out.backward()\n",
    "#dout/dxi = 1/4(dz/dx) = 1/4(6*y**2)(dy/dx) = 1/4(6*(X+2)**2)\n",
    "# = 1.5(9) = 13.5\n",
    "print(X.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[42.6667, 42.6667, 42.6667],\n",
      "        [42.6667, 42.6667, 42.6667],\n",
      "        [42.6667, 42.6667, 42.6667]])\n"
     ]
    }
   ],
   "source": [
    "X = Variable(torch.ones(3,3),requires_grad=True)\n",
    "y = 4*X + 2\n",
    "z = 8*(y**2)\n",
    "out = z.mean()\n",
    "out.backward()\n",
    "#dout/dxi = 1/9(dz/dx) = 1/9(16y)(dy/dx) = 1/9(16*(4*X+2))(4)\n",
    "# = 42.667\n",
    "print(X.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dynamic Computation Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 10])\n",
      "tensor(29824080.)\n",
      "torch.Size([64, 10])\n"
     ]
    }
   ],
   "source": [
    "dtype = torch.FloatTensor\n",
    "N, Din, H, Dout = 64, 1000, 100, 10\n",
    "\n",
    "X = Variable(torch.randn(N, Din).type(dtype), requires_grad=False)\n",
    "y = Variable(torch.randn(N, Dout).type(dtype), requires_grad=False)\n",
    "\n",
    "w1 = Variable(torch.randn(Din, H).type(dtype), requires_grad=True)\n",
    "w2 = Variable (torch.randn(H, Dout).type(dtype), requires_grad=True)\n",
    "\n",
    "learning_rate = 0.00001\n",
    "for i in range(2):\n",
    "    y_pred = X.mm(w1).clamp(min=0).mm(w2)\n",
    "    print(y_pred.shape)\n",
    "    \n",
    "    loss = (y_pred - y).pow(2).sum()\n",
    "    \n",
    "    if i%100==0:\n",
    "        print(loss.data)\n",
    "    \n",
    "    loss.backward()\n",
    "\n",
    "    \n",
    "    w1.data -= learning_rate * w1.grad.data\n",
    "    w2.data -= learning_rate * w2.grad.data\n",
    "    \n",
    "    #reset gradients!!\n",
    "    w1.grad.data.zero_()\n",
    "    w2.grad.data.zero_()\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extending Function\n",
    "\n",
    "__Function__ *Records operation history and defines formulas for differentiating ops.*\n",
    "\n",
    "*Every operation performed on Tensor s creates a new function object, that performs the computation, and records that it happened. The history is retained in the form of a DAG of functions, with edges denoting data dependencies (input <- output). Then, when backward is called, the graph is processed in the topological ordering, by calling backward() methods of each Function object, and passing returned gradients on to next Function s.*\n",
    "\n",
    "*Normally, the only way users interact with functions is by creating subclasses and defining new operations. This is a recommended way of extending torch.autograd.*\n",
    "\n",
    "*Each function object is meant to be used only once (in the forward pass).*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save whichever tensor you need for the derivative function\n",
    "\n",
    "class relu(torch.autograd.Function):\n",
    "    \n",
    "    @staticmethod\n",
    "    def forward(ctx, input):\n",
    "        ctx.save_for_backward(input)\n",
    "        return input.clamp(min=0)\n",
    "    \n",
    "    #staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        input, = ctx.saved_tensors\n",
    "        grad_input = grad_output.clone()\n",
    "        grad_input[input<0] = 0\n",
    "        return grad_input\n",
    "    \n",
    "    \n",
    "class Exp(Function):\n",
    "\n",
    "    @staticmethod\n",
    "    def forward(ctx, i):\n",
    "        result = i.exp()\n",
    "        ctx.save_for_backward(result)\n",
    "        return result\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        result, = ctx.saved_tensors\n",
    "        return grad_output * result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Access Data\n",
    "Access data of a tensor with __tensor.data__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[288., 288., 288.],\n",
       "        [288., 288., 288.],\n",
       "        [288., 288., 288.]])"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z.data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Locally Disabling Gradient computation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "print(z.requires_grad)\n",
    "with torch.no_grad():\n",
    "    h = z*2\n",
    "    print(h.requires_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Profiler\n",
    "Autograd includes a profiler that lets you inspect the cost of different operators inside your model - both on the CPU and GPU. There are two modes implemented at the moment - CPU-only using profile. and nvprof based (registers both CPU and GPU activity) using emit_nvtx."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------  ---------------  ---------------  ---------------  ---------------  ---------------  ---------------  ---------------  ---------------  ---------------  \n",
      "Name      Self CPU total %  Self CPU total   CPU total %      CPU total        CPU time avg     CUDA total %     CUDA total       CUDA time avg    Number of Calls  \n",
      "--------  ---------------  ---------------  ---------------  ---------------  ---------------  ---------------  ---------------  ---------------  ---------------  \n",
      "mul       46.43%           84.253us         46.43%           84.253us         42.127us         NaN              0.000us          0.000us          2                \n",
      "pow       22.32%           40.506us         22.32%           40.506us         40.506us         NaN              0.000us          0.000us          1                \n",
      "add       16.07%           29.165us         16.07%           29.165us         29.165us         NaN              0.000us          0.000us          1                \n",
      "mean      15.18%           27.544us         15.18%           27.544us         27.544us         NaN              0.000us          0.000us          1                \n",
      "--------  ---------------  ---------------  ---------------  ---------------  ---------------  ---------------  ---------------  ---------------  ---------------  \n",
      "Self CPU time total: 181.468us\n",
      "CUDA time total: 0.000us\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with torch.autograd.profiler.profile() as prof:\n",
    "    X = Variable(torch.ones(3,3),requires_grad=True)\n",
    "    y = torch.mul(X, 4) + 2\n",
    "    z = 8*(y**2)\n",
    "    out = z.mean()\n",
    "print(prof.key_averages().table(sort_by='self_cpu_time_total'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
