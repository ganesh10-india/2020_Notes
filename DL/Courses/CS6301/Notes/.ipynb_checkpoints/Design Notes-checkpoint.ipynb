{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network Design\n",
    "\n",
    "[Source](https://github.com/arthurredfern/UT-Dallas-CS-6301-CNNs/blob/master/Lectures/xNNs_050_Design.pdf)\n",
    "\n",
    "## Keep your goals Simple\n",
    "- The simpler the task, the more training data there is. The more data, the better a NN will perform\n",
    "- Handle complex problems with a shallow combination of simple problems.\n",
    "\n",
    "## CNN Design Overview\n",
    "- CNN exploits spatial structure in data. correlations between loaclized regions of pixels.\n",
    "- Encoder (Tail & Body for feature extraction) + decoder (head for prediction)\n",
    "    - Commonly: Serial, Parallel, Dense, or Residual\n",
    "    \n",
    "- __Tail__- few specialized layers with weaker features and better localization.\n",
    "- __Body__ - layers that define the network. strong features with worse localization.\n",
    "- __Head__ - task specific layers that produce the output of the network\n",
    "\n",
    "\n",
    "### How CNNs work (Image Classification)\n",
    "\n",
    "Classification network inputs an image and ouputs a vector, whose largest elements are the predicted class.\n",
    "\n",
    "#### Head\n",
    "\n",
    "The __head__ estimates the dominant object in a specific region by using the feature vector in the corresponding region of the feature maps at the body ouput. \n",
    "- __Global average pooling__ will extract information from each feature map (presence of feature across all areas) and the head will use a linear combination of these global features to predict the dominant object of the image.\n",
    "\n",
    "Global average Pooling or vectorization can be used to conver the output of the body to a 1xN vector for input into the Linear layers of the head.\n",
    "\n",
    "GAP is more popular since it allows arbitrary sized input images and also reduces the computational complexity of the decoding phase.\n",
    "\n",
    "The __linear classifier__ uses softmax layer during training and argmax to select ouput during inference\n",
    "\n",
    "Different head designs can be created to accomplish different goals, and more than one head can be attached to the end of a tail and body.\n",
    "\n",
    "#### Tail and Body\n",
    "\n",
    "Tail and body transform entire regions of the input image to feature vectures across the ouput feature maps. The __receptive field__ is the area of pixels from the input that can be mapped to a feature vector output.\n",
    "\n",
    "##### Tail\n",
    "- High compute, high feature map memory low parameter memory, lots of spatial redundancy.\n",
    "- Aggressive down sampling and aggressive increase in number of channels.\n",
    "\n",
    "Common architectures:\n",
    "- conv (7,7) stride 2, 64 channels -> max pool (3x3) stride 2\n",
    "- conv(3,3) stride 2, 32 channels -> conv (3,3) s=1, c=64 -> conv(3,3) s=2, c=64\n",
    "\n",
    "\n",
    "##### Body\n",
    "Blocks followed by down-sampling. Gradual reduction in memory required for feature maps. Leads to a loss in spatial resolution (pooling throws out pixels) Increase in number of channels\n",
    "\n",
    "Common design practices:\n",
    "- reduce cols and rows by 1/2 and double the number of channels\n",
    "    - data volume shrinks by a factor of 2 (reduces compute by factor of 2)\n",
    "  \n",
    "#### Receptive field size at the head\n",
    "Shows the area of information that the classifier has to work with at inference. May affect images with higher resolution.\n",
    "\n",
    "__Calculation__\n",
    "1. Start at the output of the body and set r.f.=1\n",
    "2. Move backwards in network.\n",
    "3. Everytime you reach a filter, increase the receptive field size by F-1.\n",
    "4. Everytime you reach a down sample multiply the receptive field by S, then subtract by S -1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN Networks\n",
    "\n",
    "#### Serial\n",
    "Sequenctial set of IO ops (LeNer, AlexNet, VGG, MobileNet V1)\n",
    "- VGGNet introduced stacked 3x3 conv filters. Used vectorization at the head\n",
    "- MobileNet V1 uses cascade of 3x3 spatial and 1x1 channel convs. Utilized global avg pool at head.\n",
    "\n",
    "\n",
    "#### Parallel \n",
    "Input split -> parallel ops -> output combine. (GoogLeNet, Inception V3,V4, SqueezeNet)\n",
    "- GoogLeNet/Inceptions - parallel paths with different receptive field sizes referred to as inception modules\n",
    "\n",
    "#### Dense\n",
    "Input split into a trasformation path and an identity path. Results are concatenated (Adds width to the netwprk w/ features at different depths) (DenseNet)\n",
    "\n",
    "#### Residual\n",
    "Instead of concat as in DenseNet perform an add on the two splits."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notes for Vision Optimized CNNs\n",
    "- Target a feature size of 6-8x6-8 before GAP\n",
    "- Early body blocks will have a high compute cost, while late blocks will require more memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:deep-learning]",
   "language": "python",
   "name": "conda-env-deep-learning-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
