{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Theory\n",
    "\n",
    "\n",
    "4. Let x be the K x 1 vector output of the last layer of a xNN and e = crossEntropy(p*, softMax(x)) be the error where p* is a K x 1 vector with a 1 in position k* representing the correct class and 0s elsewhere. Derive ∂e/∂x. Large portions of this are shown in the slides, however, the purpose of this question is for you to derive all of the parts yourself to gain more confidence with error gradients. Here’s a cookbook of steps and hints:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.1. Derive the gradient of the cross entropy for a 1 hot label at position k*. Use the derivative rule for log (assume base e) and note that only 1 element of the gradient is non zero.\n",
    "\n",
    "$$e = crossEntropy(p^*,softMax(x))$$\n",
    "\n",
    "$$crossEntropy(p^*,p_x)=-\\sum p^*log(p_x)$$\n",
    "$$=-log(p_{x_*})$$\n",
    "$$=-log(softMax(x_*))$$\n",
    "\n",
    "$$\\frac{\\delta e}{\\delta softMax} = [0,0,...,\\frac{-1}{softMax(x_*)},...,0,0]^T$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.2. Derive the Jacobian of the soft max. Use the derivative quotient rule and note 2 cases: i != j and i == j (where i and j refer to the Jacobian row and col). Apply a common trick for functions with exponentials and re write the derivatives in terms of original function.\n",
    "\n",
    "$$softMax(x) = \\frac{e^{x_j}}{\\sum e^{x_i}}$$\n",
    "\n",
    "$$\\frac{\\delta softmax(i=j)}{\\delta x} = \\frac{(\\sum e^{x_i}*e^{x_j}) - (e^{x_j} *e^{x_i})}{(\\sum e^{x_i})^2}$$\n",
    "\n",
    "\n",
    "$$=\\frac{e^{x_j}}{\\sum e^{x_i}}*\\frac{\\sum e^{x_i}-e^{x_i}}{\\sum e^{x_i}}$$\n",
    "\n",
    "\n",
    "$$=S_j(1-S_i)$$\n",
    "\n",
    "\n",
    "<br>\n",
    "\n",
    "$$\\frac{\\delta softmax(i\\neq j)}{\\delta x} = \\frac{0 - (e^{x_j} *e^{x_i})}{(\\sum e^{x_i})^2}$$\n",
    "\n",
    "$$=-S_i*S_j$$\n",
    "\n",
    "For any i,j in the Jacobian matrix, \n",
    "- if $i=j$ : $\\frac{\\delta Softmax}{\\delta x} = $ Softmax(i)(1-Softmax(j)\n",
    "- if $i \\neq j$: $\\frac{\\delta Softmax}{\\delta x} = $ -Softmax(i)Softmax(j)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.3. Apply the chain rule to derive the gradient of e = crossEntropy(p*, softMax(x)) as the Jacobian matrix times the gradient vector. Take advantage of only 1 element of the gradient vector being non zero effectively selecting the corresponding col of the Jacobian matrix.\n",
    "\n",
    "\n",
    "$$\\frac{\\delta e}{\\delta x} = \\frac{\\delta e}{\\delta softMax}\\frac{\\delta Softmax}{\\delta x}$$\n",
    "\n",
    "$$ = [p_0,...,p_{k^*}-1,...,p_N]$$\n",
    "\n",
    "Through matrix-vector multiplication where we will multiply the gradient (\\frac{\\delta e}{\\delta softMax}) across each column of the Jacobian. The result will be a vector formed from the $k^*$th row of each Jacobian column multiplied by $\\frac{-1}{P_{k*}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.4. Note the beautiful and numerically stable result\n",
    "\n",
    "\n",
    "4.5. Remind yourself in the future when implementing classification networks in\n",
    "software, use a single call to the high level library’s built in combined soft max cross entropy function if it’s available instead of making 2 calls to separate soft max and cross entropy functions. But realize that some libraries combine separate functions as an optimization step behind the scenes for you so if it’s not available then it’s probably still ok.\n",
    "\n",
    "\n",
    "\n",
    "5. Consider a simple residual block of the form y = x + f(H x + v) where x is a K x 1 input feature vector, H is K x K linear transformation matrix, v is a K x 1 bias vector, f is a ReLU pointwise nonlinearity and y is a K x 1 output feature vector. Assume that ∂e/∂y is given. Write out a single expression using the chain rule for ∂e/∂x in terms of ∂e/∂y and the Jacobians of the other operations. For the ReLU, define the Jacobian as a K x K diagonal matrix I{0, 1}. Note the clean flow of the gradient from the output to the input, this is a key for training deep networks.\n",
    "\n",
    "\n",
    "$$\\frac{\\delta y}{\\delta x} = Identity(k,k) + I(0,1)H$$\n",
    "$$\\frac{\\delta e}{\\delta x}  = \\frac{\\delta e}{\\delta y} \\cdot (Identity(k,k) + I(0,1)H)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Write out the gradient descent update for H and v in the above example. Define intermediate feature maps as necessary. Note the need to save feature maps from the forward pass which has memory implications for xNN training.\n",
    "\n",
    "$$\\frac{\\delta e}{\\delta v} = \\frac{\\delta e}{\\delta y} \\cdot I(0,1)$$\n",
    "$$\\frac{\\delta e}{\\delta H} = \\frac{\\delta e}{\\delta y}I(0,1)H$$\n",
    "\n",
    "$$v_{t+1}=v_{t}-\\alpha \\frac{\\delta e}{\\delta v}$$\n",
    "$$H_{t+1}=H_{t}-\\alpha \\frac{\\delta e}{\\delta H}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Practice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-24T00:42:10.518979Z",
     "start_time": "2020-02-24T00:42:07.352033Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.1.0'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import os\n",
    "import datetime\n",
    "import pathlib\n",
    "import numpy as np\n",
    "import PIL.Image as Image\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "#tf.debugging.set_log_device_placement(True)\n",
    "\n",
    "AUTOTUNE = tf.data.experimental.AUTOTUNE\n",
    "BATCH_SIZE = 64\n",
    "OG_IMAGENET_SIZE = (224,224)\n",
    "IMAGE_SIZE = (64,64)\n",
    "\n",
    "tf.__version__ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Extraction w MobileNet_V2\n",
    "\n",
    "Data is downloaded from the [official ImageNet website](http://image-net.org/small/download.php). Each image has a resolution of 64x64 with three channels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-24T00:42:19.680695Z",
     "start_time": "2020-02-24T00:42:10.633639Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['E:\\\\Data\\\\Imagenet\\\\imageNet_64-64\\\\train_64x64\\\\0224781.png',\n",
       " 'E:\\\\Data\\\\Imagenet\\\\imageNet_64-64\\\\train_64x64\\\\0224782.png',\n",
       " 'E:\\\\Data\\\\Imagenet\\\\imageNet_64-64\\\\train_64x64\\\\0224783.png',\n",
       " 'E:\\\\Data\\\\Imagenet\\\\imageNet_64-64\\\\train_64x64\\\\0224784.png',\n",
       " 'E:\\\\Data\\\\Imagenet\\\\imageNet_64-64\\\\train_64x64\\\\0224785.png']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_root = pathlib.Path(\"E://Data/Imagenet/imageNet_64-64/train_64x64\")\n",
    "file_names = list(map(lambda x: str(data_root/x), os.listdir(data_root)))\n",
    "file_names[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-24T00:42:33.477577Z",
     "start_time": "2020-02-24T00:42:29.338078Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(b'E:\\\\Data\\\\Imagenet\\\\imageNet_64-64\\\\train_64x64\\\\0224781.png', shape=(), dtype=string)\n"
     ]
    }
   ],
   "source": [
    "def decode_image(path):\n",
    "    img = tf.io.read_file(path)\n",
    "    img = tf.image.decode_png(img, channels=3)\n",
    "    img = tf.image.convert_image_dtype(img, tf.float32)\n",
    "    img = tf.image.resize(img, OG_IMAGENET_SIZE)\n",
    "    return img\n",
    "\n",
    "filename_dataset = tf.data.Dataset.from_tensor_slices(file_names)\n",
    "for x in filename_dataset.take(1):\n",
    "    print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-24T00:42:43.269570Z",
     "start_time": "2020-02-24T00:42:43.128880Z"
    }
   },
   "outputs": [],
   "source": [
    "inference_data = filename_dataset.map(decode_image, num_parallel_calls = AUTOTUNE)\n",
    "inference_data = tf.data.Dataset.zip((filename_dataset, inference_data))\n",
    "inference_data = inference_data.batch(BATCH_SIZE)\n",
    "inference_data = inference_data.prefetch(AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-24T00:42:52.893666Z",
     "start_time": "2020-02-24T00:42:52.887709Z"
    }
   },
   "outputs": [],
   "source": [
    "labels_path = tf.keras.utils.get_file('ImageNetLabels.txt','https://storage.googleapis.com/download.tensorflow.org/data/ImageNetLabels.txt')\n",
    "imagenet_labels = np.array(open(labels_path).read().splitlines())\n",
    "def get_classes(predictions):\n",
    "    return imagenet_labels[np.argmax(predictions, axis=-1)] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-24T00:43:05.449716Z",
     "start_time": "2020-02-24T00:43:02.558430Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "classifier_url =\"https://tfhub.dev/google/tf2-preview/mobilenet_v2/classification/2\" \n",
    "classifier = tf.keras.Sequential([\n",
    "    hub.KerasLayer(classifier_url, input_shape = OG_IMAGENET_SIZE+(3,))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-02-24T00:42:10.036Z"
    }
   },
   "outputs": [],
   "source": [
    "for files, x in inference_data:\n",
    "    with tf.device('/GPU:0'):\n",
    "        pred = classifier.predict(x)\n",
    "    classes = get_classes(pred)\n",
    "    for i, file in enumerate(files):\n",
    "        f = pathlib.Path(file.numpy().decode('ascii'))\n",
    "        if not os.path.exists(f.parent/classes[i]):\n",
    "            os.mkdir(f.parent / classes[i])\n",
    "        else:    \n",
    "            pass\n",
    "        os.rename(f, f.parent / classes[i] / f.parts[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Data Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-02-24T00:42:10.881Z"
    }
   },
   "outputs": [],
   "source": [
    "# data\n",
    "DATA_NUM_CLASSES        = 10\n",
    "DATA_CHANNELS           = 3\n",
    "DATA_ROWS               = 32\n",
    "DATA_COLS               = 32\n",
    "DATA_CROP_ROWS          = 28\n",
    "DATA_CROP_COLS          = 28\n",
    "\n",
    "# model\n",
    "MODEL_LEVEL_0_REPEATS   = 3\n",
    "MODEL_LEVEL_1_REPEATS   = 3\n",
    "MODEL_LEVEL_2_REPEATS   = 3\n",
    "\n",
    "# training\n",
    "TRAINING_BATCH_SIZE      = 32\n",
    "TRAINING_SHUFFLE_BUFFER  = 5000\n",
    "TRAINING_LR_MAX          = 0.001\n",
    "# TRAINING_LR_SCALE        = 0.1\n",
    "# TRAINING_LR_EPOCHS       = 2\n",
    "TRAINING_LR_INIT_SCALE   = 0.01\n",
    "TRAINING_LR_INIT_EPOCHS  = 5\n",
    "TRAINING_LR_FINAL_SCALE  = 0.01\n",
    "TRAINING_LR_FINAL_EPOCHS = 25\n",
    "\n",
    "# training (derived)\n",
    "TRAINING_NUM_EPOCHS = TRAINING_LR_INIT_EPOCHS + TRAINING_LR_FINAL_EPOCHS\n",
    "TRAINING_LR_INIT    = TRAINING_LR_MAX*TRAINING_LR_INIT_SCALE\n",
    "TRAINING_LR_FINAL   = TRAINING_LR_MAX*TRAINING_LR_FINAL_SCALE\n",
    "\n",
    "SAVE_MODEL_PATH = 'E://Models/ImageNet_64/'\n",
    "!mkdir -p \"$SAVE_MODEL_PATH\"\n",
    "\n",
    "AUTOTUNE = tf.data.experimental.AUTOTUNE\n",
    "\n",
    "def get_label(file_path):\n",
    "  # convert the path to a list of path components\n",
    "  parts = tf.strings.split(file_path, os.path.sep)\n",
    "  # The second to last is the class-directory\n",
    "  return parts[-2] == CLASSES\n",
    "\n",
    "def decode_image(img):\n",
    "    img = tf.image.decode_image(img, channels=3)\n",
    "    img = tf.image.convert_image_dtype(img, tf.float32)\n",
    "    img = tf.image.random_flip_left_right(img)\n",
    "    #img = tf.image.random_crop(img, size=[DATA_CROP_ROWS, DATA_CROP_COLS, 3])\n",
    "    return img\n",
    "\n",
    "def process_path(path):\n",
    "    \"\"\"\n",
    "    Input: file_path of a sample image\n",
    "    Output: image in 3x64x64 float32 Tensor and one hot tensor\n",
    "    \"\"\"\n",
    "    label = get_label(path)\n",
    "    image = tf.io.read_file(path)\n",
    "    image = decode_image(image)\n",
    "    return image, label\n",
    "\n",
    "\n",
    "def prepare_for_training(ds, cache=False, shuffle_buffer_size=TRAINING_SHUFFLE_BUFFER):\n",
    "    if cache:\n",
    "        if isinstance(cache, str):\n",
    "            ds = ds.cache(cache)\n",
    "        else:\n",
    "            ds = ds.cache()\n",
    "    ds = ds.shuffle(buffer_size=shuffle_buffer_size)\n",
    "    ds = ds.repeat()\n",
    "    ds = ds.batch(TRAINING_BATCH_SIZE)\n",
    "    ds = ds.prefetch(buffer_size=AUTOTUNE)\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_root = pathlib.Path(\"E://Data/Imagenet/imageNet_64-64/train_64x64\")\n",
    "list_files = tf.data.Dataset.list_files(str(data_root/'*/*'))\n",
    "#map the above function to file_name dataset\n",
    "train_imgs = list_files.map(process_path, num_parallel_calls=AUTOTUNE) \n",
    "train_ds_cachefile = prepare_for_training(train_imgs, cache=SAVE_MODEL_PATH+\"cache.tfcache\", shuffle_buffer_size=TRAINING_SHUFFLE_BUFFER)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Specifications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create and compile model\n",
    "def create_model(level_0_repeats, level_1_repeats, level_2_repeats):\n",
    "\n",
    "    # encoder - input\n",
    "    model_input = keras.Input(shape=(DATA_CROP_ROWS, DATA_CROP_COLS, DATA_CHANNELS), name='input_image')\n",
    "    x           = model_input\n",
    "    \n",
    "    # encoder - level 0\n",
    "    for n0 in range(level_0_repeats):\n",
    "        # x = keras.layers.Conv2D(32, 3, strides=1, padding='same', activation='relu', use_bias=True)(x)\n",
    "        x = keras.layers.Conv2D(32, 3, strides=1, padding='same', activation=None, use_bias=False)(x)\n",
    "        x = keras.layers.BatchNormalization(axis=-1, momentum=0.99, epsilon=0.001, center=True, scale=True)(x)\n",
    "        x = keras.layers.ReLU()(x)\n",
    "    x = keras.layers.MaxPooling2D(pool_size=(3, 3), strides=(2, 2), padding='same')(x)\n",
    "\n",
    "    # encoder - level 1\n",
    "    for n1 in range(level_1_repeats):\n",
    "        # x = keras.layers.Conv2D(64, 3, strides=1, padding='same', activation='relu', use_bias=True)(x)\n",
    "        x = keras.layers.Conv2D(64, 3, strides=1, padding='same', activation=None, use_bias=False)(x)\n",
    "        x = keras.layers.BatchNormalization(axis=-1, momentum=0.99, epsilon=0.001, center=True, scale=True)(x)\n",
    "        x = keras.layers.ReLU()(x)\n",
    "    x = keras.layers.MaxPooling2D(pool_size=(3, 3), strides=(2, 2), padding='same')(x)\n",
    "        \n",
    "    # encoder - level 2\n",
    "    for n2 in range(level_2_repeats):\n",
    "        # x = keras.layers.Conv2D(128, 3, strides=1, padding='same', activation='relu', use_bias=True)(x)\n",
    "        x = keras.layers.Conv2D(128, 3, strides=1, padding='same', activation=None, use_bias=False)(x)\n",
    "        x = keras.layers.BatchNormalization(axis=-1, momentum=0.99, epsilon=0.001, center=True, scale=True)(x)\n",
    "        x = keras.layers.ReLU()(x)\n",
    "\n",
    "    # encoder - output\n",
    "    encoder_output = x\n",
    "\n",
    "    # decoder\n",
    "    y              = keras.layers.GlobalAveragePooling2D()(encoder_output)\n",
    "    decoder_output = keras.layers.Dense(DATA_NUM_CLASSES, activation='softmax')(y)\n",
    "    \n",
    "    # forward path\n",
    "    model = keras.Model(inputs=model_input, outputs=decoder_output, name='cifar_model')\n",
    "\n",
    "    # loss, backward path (implicit) and weight update\n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(TRAINING_LR_MAX), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    # return model\n",
    "    return model\n",
    "\n",
    "# create and compile model\n",
    "model = create_model(MODEL_LEVEL_0_REPEATS, MODEL_LEVEL_1_REPEATS, MODEL_LEVEL_2_REPEATS)\n",
    "\n",
    "# model description and figure\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# learning rate schedule\n",
    "def lr_schedule(epoch):\n",
    "\n",
    "    # staircase\n",
    "    # lr = TRAINING_LR_MAX*math.pow(TRAINING_LR_SCALE, math.floor(epoch/TRAINING_LR_EPOCHS))\n",
    "\n",
    "    # linear warmup followed by cosine decay\n",
    "    if epoch < TRAINING_LR_INIT_EPOCHS:\n",
    "        lr = (TRAINING_LR_MAX - TRAINING_LR_INIT)*(float(epoch)/TRAINING_LR_INIT_EPOCHS) + TRAINING_LR_INIT\n",
    "    else:\n",
    "        lr = (TRAINING_LR_MAX - TRAINING_LR_FINAL)*max(0.0, math.cos(((float(epoch) - TRAINING_LR_INIT_EPOCHS)/(TRAINING_LR_FINAL_EPOCHS - 1.0))*(math.pi/2.0))) + TRAINING_LR_FINAL\n",
    "\n",
    "    # debug - learning rate display\n",
    "    # print(epoch)\n",
    "    # print(lr)\n",
    "\n",
    "    return lr\n",
    "\n",
    "# plot training accuracy and loss curves\n",
    "def plot_training_curves(history):\n",
    "\n",
    "    # training and validation data accuracy\n",
    "    acc     = history.history['accuracy']\n",
    "    val_acc = history.history['val_accuracy']\n",
    "\n",
    "    # training and validation data loss\n",
    "    loss     = history.history['loss']\n",
    "    val_loss = history.history['val_loss']\n",
    "\n",
    "    # plot accuracy\n",
    "    plt.figure(figsize=(8, 8))\n",
    "    plt.subplot(2, 1, 1)\n",
    "    plt.plot(acc, label='Training Accuracy')\n",
    "    plt.plot(val_acc, label='Validation Accuracy')\n",
    "    plt.legend(loc='lower right')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.ylim([min(plt.ylim()), 1])\n",
    "    plt.title('Training and Validation Accuracy')\n",
    "\n",
    "    # plot loss\n",
    "    plt.subplot(2, 1, 2)\n",
    "    plt.plot(loss, label='Training Loss')\n",
    "    plt.plot(val_loss, label='Validation Loss')\n",
    "    plt.legend(loc='upper right')\n",
    "    plt.ylabel('Cross Entropy')\n",
    "    plt.ylim([0, 2.0])\n",
    "    plt.title('Training and Validation Loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.show()\n",
    "\n",
    "# callbacks (learning rate schedule, model checkpointing during training)\n",
    "callbacks = [keras.callbacks.LearningRateScheduler(lr_schedule),\n",
    "             keras.callbacks.ModelCheckpoint(filepath=SAVE_MODEL_PATH+'model_{epoch}.h5', save_best_only=True, monitor='val_loss', verbose=1)]\n",
    "\n",
    "# training\n",
    "initial_epoch_num = 0\n",
    "history           = model.fit(x=dataset_train, epochs=TRAINING_NUM_EPOCHS, verbose=1, callbacks=callbacks, validation_data=dataset_test, initial_epoch=initial_epoch_num)\n",
    "\n",
    "# example of restarting training after a crash from the last saved checkpoint\n",
    "# model             = create_model(MODEL_LEVEL_0_REPEATS, MODEL_LEVEL_1_REPEATS, MODEL_LEVEL_2_REPEATS)\n",
    "# model.load_weights(SAVE_MODEL_PATH+'model_X.h5') # replace X with the last saved checkpoint number\n",
    "# initial_epoch_num = X                            # replace X with the last saved checkpoint number\n",
    "# history           = model.fit(x=dataset_train, epochs=TRAINING_NUM_EPOCHS, verbose=1, callbacks=callbacks, validation_data=dataset_test, initial_epoch=initial_epoch_num)\n",
    "\n",
    "# plot accuracy and loss curves\n",
    "plot_training_curves(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test\n",
    "test_loss, test_accuracy = model.evaluate(x=dataset_test)\n",
    "print('Test loss:     ', test_loss)\n",
    "print('Test accuracy: ', test_accuracy)\n",
    "\n",
    "# example of saving and loading the model in Keras H5 format\n",
    "# this saves both the model and the weights\n",
    "# model.save('./save/model/model.h5')\n",
    "# new_model       = keras.models.load_model('./save/model/model.h5')\n",
    "# predictions     = model.predict(x=dataset_test)\n",
    "# new_predictions = new_model.predict(x=dataset_test)\n",
    "# np.testing.assert_allclose(predictions, new_predictions, atol=1e-6)\n",
    "\n",
    "# example of saving and loading the model in TensorFlow SavedModel format\n",
    "# this saves both the model and the weights\n",
    "# keras.experimental.export_saved_model(model, './save/model/')\n",
    "# new_model       = keras.experimental.load_from_saved_model('./save/model/')\n",
    "# predictions     = model.predict(x=dataset_test)\n",
    "# new_predictions = new_model.predict(x=dataset_test)\n",
    "# np.testing.assert_allclose(predictions, new_predictions, atol=1e-6)\n",
    "\n",
    "# example of getting a list of all feature maps\n",
    "# feature_map_list = [layer.output for layer in model.layers]\n",
    "# print(feature_map_list)\n",
    "\n",
    "# example of creating a model encoder\n",
    "# replace X with the layer number of the encoder output\n",
    "# model_encoder    = keras.Model(inputs=model.input, outputs=model.layers[X].output)\n",
    "# model_encoder.summary()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:deep-learning]",
   "language": "python",
   "name": "conda-env-deep-learning-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
