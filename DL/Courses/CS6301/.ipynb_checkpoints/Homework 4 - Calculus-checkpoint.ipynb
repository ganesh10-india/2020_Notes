{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Theory\n",
    "\n",
    "\n",
    "4. Let x be the K x 1 vector output of the last layer of a xNN and e = crossEntropy(p*, softMax(x)) be the error where p* is a K x 1 vector with a 1 in position k* representing the correct class and 0s elsewhere. Derive ∂e/∂x. Large portions of this are shown in the slides, however, the purpose of this question is for you to derive all of the parts yourself to gain more confidence with error gradients. Here’s a cookbook of steps and hints:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.1. Derive the gradient of the cross entropy for a 1 hot label at position k*. Use the derivative rule for log (assume base e) and note that only 1 element of the gradient is non zero.\n",
    "\n",
    "$$e = crossEntropy(p^*,softMax(x))$$\n",
    "\n",
    "$$crossEntropy(p^*,p_x)=-\\sum p^*log(p_x)$$\n",
    "$$=-log(p_{x_*})$$\n",
    "$$=-log(softMax(x_*))$$\n",
    "\n",
    "$$\\frac{\\delta e}{\\delta softMax} = [0,0,...,\\frac{-1}{softMax(x_*)},...,0,0]^T$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.2. Derive the Jacobian of the soft max. Use the derivative quotient rule and note 2 cases: i != j and i == j (where i and j refer to the Jacobian row and col). Apply a common trick for functions with exponentials and re write the derivatives in terms of original function.\n",
    "\n",
    "$$softMax(x) = \\frac{e^{x_j}}{\\sum e^{x_i}}$$\n",
    "\n",
    "$$\\frac{\\delta softmax(i=j)}{\\delta x} = \\frac{(\\sum e^{x_i}*e^{x_j}) - (e^{x_j} *e^{x_i})}{(\\sum e^{x_i})^2}$$\n",
    "\n",
    "\n",
    "$$=\\frac{e^{x_j}}{\\sum e^{x_i}}*\\frac{\\sum e^{x_i}-e^{x_i}}{\\sum e^{x_i}}$$\n",
    "\n",
    "\n",
    "$$=S_j(1-S_i)$$\n",
    "\n",
    "\n",
    "<br>\n",
    "\n",
    "$$\\frac{\\delta softmax(i\\neq j)}{\\delta x} = \\frac{0 - (e^{x_j} *e^{x_i})}{(\\sum e^{x_i})^2}$$\n",
    "\n",
    "$$=-S_i*S_j$$\n",
    "\n",
    "For any i,j in the Jacobian matrix, \n",
    "- if $i=j$ : $\\frac{\\delta Softmax}{\\delta x} = $ Softmax(i)(1-Softmax(j)\n",
    "- if $i \\neq j$: $\\frac{\\delta Softmax}{\\delta x} = $ -Softmax(i)Softmax(j)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.3. Apply the chain rule to derive the gradient of e = crossEntropy(p*, softMax(x)) as the Jacobian matrix times the gradient vector. Take advantage of only 1 element of the gradient vector being non zero effectively selecting the corresponding col of the Jacobian matrix.\n",
    "\n",
    "\n",
    "$$\\frac{\\delta e}{\\delta x} = \\frac{\\delta e}{\\delta softMax}\\frac{\\delta Softmax}{\\delta x}$$\n",
    "\n",
    "$$ = [p_0,...,p_{k^*}-1,...,p_N]$$\n",
    "\n",
    "Through matrix-vector multiplication where we will multiply the gradient (\\frac{\\delta e}{\\delta softMax}) across each column of the Jacobian. The result will be a vector formed from the $k^*$th row of each Jacobian column multiplied by $\\frac{-1}{P_{k*}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.4. Note the beautiful and numerically stable result\n",
    "\n",
    "\n",
    "4.5. Remind yourself in the future when implementing classification networks in\n",
    "software, use a single call to the high level library’s built in combined soft max cross entropy function if it’s available instead of making 2 calls to separate soft max and cross entropy functions. But realize that some libraries combine separate functions as an optimization step behind the scenes for you so if it’s not available then it’s probably still ok.\n",
    "\n",
    "\n",
    "\n",
    "5. Consider a simple residual block of the form y = x + f(H x + v) where x is a K x 1 input feature vector, H is K x K linear transformation matrix, v is a K x 1 bias vector, f is a ReLU pointwise nonlinearity and y is a K x 1 output feature vector. Assume that ∂e/∂y is given. Write out a single expression using the chain rule for ∂e/∂x in terms of ∂e/∂y and the Jacobians of the other operations. For the ReLU, define the Jacobian as a K x K diagonal matrix I{0, 1}. Note the clean flow of the gradient from the output to the input, this is a key for training deep networks.\n",
    "\n",
    "\n",
    "$$\\frac{\\delta y}{\\delta x} = Identity(k,k) + I(0,1)H$$\n",
    "$$\\frac{\\delta e}{\\delta x}  = \\frac{\\delta e}{\\delta y} \\cdot (Identity(k,k) + I(0,1)H)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Write out the gradient descent update for H and v in the above example. Define intermediate feature maps as necessary. Note the need to save feature maps from the forward pass which has memory implications for xNN training.\n",
    "\n",
    "$$\\frac{\\delta e}{\\delta v} = \\frac{\\delta e}{\\delta y} \\cdot I(0,1)$$\n",
    "$$\\frac{\\delta e}{\\delta H} = \\frac{\\delta e}{\\delta y}I(0,1)H$$\n",
    "\n",
    "$$v_{t+1}=v_{t}-\\alpha \\frac{\\delta e}{\\delta v}$$\n",
    "$$H_{t+1}=H_{t}-\\alpha \\frac{\\delta e}{\\delta H}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Practice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-25T05:10:09.035976Z",
     "start_time": "2020-02-25T05:10:04.538324Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.1.0'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import os\n",
    "import datetime\n",
    "import pathlib\n",
    "import numpy as np\n",
    "import PIL.Image as Image\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import math\n",
    "from tqdm.notebook import tqdm\n",
    "#tf.debugging.set_log_device_placement(True)\n",
    "\n",
    "AUTOTUNE = tf.data.experimental.AUTOTUNE\n",
    "BATCH_SIZE = 64\n",
    "OG_IMAGENET_SIZE = (224,224)\n",
    "IMAGE_SIZE = (64,64)\n",
    "\n",
    "tf.__version__ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Extraction w MobileNet_V2\n",
    "\n",
    "I Used the TinyImageNet dataset found [here](https://tiny-imagenet.herokuapp.com/). To extract the data into a standard format I had to restructure the train/val folders with pythonic folder restructuring. The test data was without labels, so I used MovileNet V2 to label these.\n",
    "\n",
    "__My main reason for using Tiny Imagenet, is that the assigned dataset is currently in the process of decompressing with 32 hours remaining...__ (I had to switch the processed data to a different hard-drive because my external HDD drive couldnt handle the IO)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "\"\"\"\n",
    "Preprocessing for the Tiny ImageNet dataset to add labels to directories\n",
    "\"\"\"\n",
    "with open(os.path.join(dir_path,'words.txt'), 'r') as f:\n",
    "    ids = f.readlines()\n",
    "ids = list(map(lambda x: x.rstrip().split('\\t'), ids))\n",
    "ids = {row[0]:row[1].split(',')[0] for row in ids}\n",
    "\n",
    "for folder in os.listdir(os.path.join(dir_path,\"train\")):\n",
    "    path = os.path.join(dir_path,'train',folder)\n",
    "    if folder in ids:\n",
    "        os.rename(path, os.path.join(dir_path,'train',ids[folder]))\n",
    "    else:\n",
    "        print(\"{} not found.\".format(folder))\n",
    "        \n",
    "for folder in os.listdir(data_root):\n",
    "    if len(os.listdir(data_root/folder))==2:\n",
    "        for item in os.listdir(data_root/folder):\n",
    "            if item.endswith(\".txt\"): #remove the bounding box file\n",
    "                os.remove(data_root/folder/item)\n",
    "                \n",
    "            if os.path.isdir(data_root/folder/item): # move the images up a directory\n",
    "                for file in os.listdir(data_root/folder/item):\n",
    "                    os.rename(data_root/folder/item/file, data_root/folder/file)\n",
    "                os.rmdir(data_root/folder/item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-25T05:04:51.985033Z",
     "start_time": "2020-02-25T05:04:51.899572Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['F:\\\\Data\\\\Imagenet\\\\tiny-imagenet-200\\\\test\\\\test_6.JPEG',\n",
       " 'F:\\\\Data\\\\Imagenet\\\\tiny-imagenet-200\\\\test\\\\test_7.JPEG',\n",
       " 'F:\\\\Data\\\\Imagenet\\\\tiny-imagenet-200\\\\test\\\\test_8.JPEG',\n",
       " 'F:\\\\Data\\\\Imagenet\\\\tiny-imagenet-200\\\\test\\\\test_9.JPEG',\n",
       " 'F:\\\\Data\\\\Imagenet\\\\tiny-imagenet-200\\\\test\\\\test_10.JPEG']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_root = pathlib.Path(\"F://Data/Imagenet/tiny-imagenet-200/test/\")\n",
    "file_names = list(data_root.glob(\"*.jpeg\"))\n",
    "file_names = [str(f) for f in file_names]\n",
    "file_names[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-25T05:05:31.929241Z",
     "start_time": "2020-02-25T05:05:31.878388Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(b'F:\\\\Data\\\\Imagenet\\\\tiny-imagenet-200\\\\test\\\\test_6.JPEG', shape=(), dtype=string)\n"
     ]
    }
   ],
   "source": [
    "def decode_image(path):\n",
    "    img = tf.io.read_file(path)\n",
    "    img = tf.image.decode_jpeg(img, channels=3)\n",
    "    img = tf.image.convert_image_dtype(img, tf.float32)\n",
    "    img = tf.image.resize(img, OG_IMAGENET_SIZE)\n",
    "    return img\n",
    "\n",
    "filename_dataset = tf.data.Dataset.from_tensor_slices(file_names)\n",
    "for x in filename_dataset.take(1):\n",
    "    print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-25T05:05:34.284773Z",
     "start_time": "2020-02-25T05:05:34.179372Z"
    }
   },
   "outputs": [],
   "source": [
    "inference_data = filename_dataset.map(decode_image, num_parallel_calls = AUTOTUNE)\n",
    "inference_data = tf.data.Dataset.zip((filename_dataset, inference_data))\n",
    "inference_data = inference_data.batch(BATCH_SIZE)\n",
    "inference_data = inference_data.prefetch(AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-25T05:05:39.433137Z",
     "start_time": "2020-02-25T05:05:39.413177Z"
    }
   },
   "outputs": [],
   "source": [
    "labels_path = tf.keras.utils.get_file('ImageNetLabels.txt','https://storage.googleapis.com/download.tensorflow.org/data/ImageNetLabels.txt')\n",
    "imagenet_labels = np.array(open(labels_path).read().splitlines())\n",
    "def get_classes(predictions):\n",
    "    return imagenet_labels[np.argmax(predictions, axis=-1)] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-25T05:05:43.073589Z",
     "start_time": "2020-02-25T05:05:40.280220Z"
    }
   },
   "outputs": [],
   "source": [
    "classifier_url =\"https://tfhub.dev/google/tf2-preview/mobilenet_v2/classification/2\" \n",
    "classifier = tf.keras.Sequential([\n",
    "    hub.KerasLayer(classifier_url, input_shape = OG_IMAGENET_SIZE+(3,))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-25T05:06:51.594351Z",
     "start_time": "2020-02-25T05:05:43.970891Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "377f594d41634d78b9f5a3702314d424",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=156.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "num_files = len(file_names)//BATCH_SIZE\n",
    "for files, x in tqdm(inference_data, total=num_files):\n",
    "    with tf.device('/GPU:0'):\n",
    "        pred = classifier.predict(x)\n",
    "    classes = get_classes(pred)\n",
    "    for i, file in enumerate(files):\n",
    "        f = pathlib.Path(file.numpy().decode('ascii'))\n",
    "        if not os.path.exists(f.parent/classes[i]):\n",
    "            os.mkdir(f.parent / classes[i])\n",
    "        else:    \n",
    "            pass\n",
    "        os.rename(f, f.parent / classes[i] / f.parts[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Data Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-25T05:10:10.878710Z",
     "start_time": "2020-02-25T05:10:10.861754Z"
    }
   },
   "outputs": [],
   "source": [
    "# data\n",
    "DATA_NUM_CLASSES        = 200\n",
    "DATA_CHANNELS           = 3\n",
    "DATA_ROWS               = 64\n",
    "DATA_COLS               = 64\n",
    "#DATA_CROP_ROWS          = 28\n",
    "#DATA_CROP_COLS          = 28\n",
    "\n",
    "# model\n",
    "MODEL_LEVEL_0_REPEATS   = 3\n",
    "MODEL_LEVEL_1_REPEATS   = 3\n",
    "MODEL_LEVEL_2_REPEATS   = 3\n",
    "\n",
    "# training\n",
    "TRAINING_BATCH_SIZE      = 32\n",
    "TRAINING_SHUFFLE_BUFFER  = 5000\n",
    "TRAINING_LR_MAX          = 0.001\n",
    "# TRAINING_LR_SCALE        = 0.1\n",
    "# TRAINING_LR_EPOCHS       = 2\n",
    "TRAINING_LR_INIT_SCALE   = 0.01\n",
    "TRAINING_LR_INIT_EPOCHS  = 5\n",
    "TRAINING_LR_FINAL_SCALE  = 0.01\n",
    "TRAINING_LR_FINAL_EPOCHS = 25\n",
    "\n",
    "# training (derived)\n",
    "TRAINING_NUM_EPOCHS = TRAINING_LR_INIT_EPOCHS + TRAINING_LR_FINAL_EPOCHS\n",
    "TRAINING_LR_INIT    = TRAINING_LR_MAX*TRAINING_LR_INIT_SCALE\n",
    "TRAINING_LR_FINAL   = TRAINING_LR_MAX*TRAINING_LR_FINAL_SCALE\n",
    "\n",
    "DATA_PATH = 'F://Data/ImageNet/tiny-imagenet-200/'\n",
    "SAVE_MODEL_PATH = pathlib.Path('F://Models/ImageNet_64/1/')\n",
    "#!mkdir -p \"$SAVE_MODEL_PATH\"\n",
    "\n",
    "AUTOTUNE = tf.data.experimental.AUTOTUNE\n",
    "\n",
    "def get_label(file_path):\n",
    "  # convert the path to a list of path components\n",
    "  parts = tf.strings.split(file_path, os.path.sep)\n",
    "  # The second to last is the class-directory\n",
    "  return parts[-2] == CLASSES\n",
    "\n",
    "def decode_image(img):\n",
    "    img = tf.image.decode_image(img, channels=3)\n",
    "    img = tf.image.convert_image_dtype(img, tf.float32)\n",
    "    img = tf.image.random_flip_left_right(img)\n",
    "    #img = tf.image.random_crop(img, size=[DATA_CROP_ROWS, DATA_CROP_COLS, 3])\n",
    "    return img\n",
    "\n",
    "def process_path(path):\n",
    "    \"\"\"\n",
    "    Input: file_path of a sample image\n",
    "    Output: image in 3x64x64 float32 Tensor and one hot tensor\n",
    "    \"\"\"\n",
    "    label = get_label(path)\n",
    "    image = tf.io.read_file(path)\n",
    "    image = decode_image(image)\n",
    "    return image, label\n",
    "\n",
    "\n",
    "def prepare_dataset(data_path, cache=False, shuffle_buffer_size=TRAINING_SHUFFLE_BUFFER):\n",
    "    list_files = tf.data.Dataset.list_files(str(data_path/'*/*'))\n",
    "    #map the above function to file_name dataset\n",
    "    ds = list_files.map(process_path, num_parallel_calls=AUTOTUNE) \n",
    "    if cache:\n",
    "        if isinstance(cache, str):\n",
    "            ds = ds.cache(cache)\n",
    "        else:\n",
    "            ds = ds.cache()\n",
    "    ds = ds.shuffle(buffer_size=shuffle_buffer_size)\n",
    "    ds = ds.repeat()\n",
    "    ds = ds.batch(TRAINING_BATCH_SIZE)\n",
    "    ds = ds.prefetch(buffer_size=AUTOTUNE)\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-25T05:10:33.300879Z",
     "start_time": "2020-02-25T05:10:11.303138Z"
    }
   },
   "outputs": [],
   "source": [
    "data_root = pathlib.Path(DATA_PATH)\n",
    "CLASSES = os.listdir(data_root/'train')\n",
    "train_ds_cachefile = prepare_dataset(data_root/'train', \n",
    "                                          cache=str(SAVE_MODEL_PATH.parent/\"cache.tfcache\"), \n",
    "                                          shuffle_buffer_size=TRAINING_SHUFFLE_BUFFER)\n",
    "\n",
    "valid_ds_cachefile = prepare_dataset(data_root/'val', \n",
    "                                          cache=str(SAVE_MODEL_PATH.parent/\"valid_cache.tfcache\"), \n",
    "                                          shuffle_buffer_size=TRAINING_SHUFFLE_BUFFER)\n",
    "\n",
    "test_ds_nocache = prepare_dataset(data_root/'test', \n",
    "                                          cache=False, \n",
    "                                          shuffle_buffer_size=TRAINING_SHUFFLE_BUFFER)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Specifications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-25T05:10:33.463496Z",
     "start_time": "2020-02-25T05:10:33.439562Z"
    }
   },
   "outputs": [],
   "source": [
    "class ResNet_Block(tf.keras.Model):\n",
    "    def __init__(self, filters):\n",
    "        super(ResNet_Block, self).__init__()\n",
    "        \n",
    "        self.conv1 = tf.keras.layers.Conv2D(filters, (1,1))\n",
    "        self.bn1 = tf.keras.layers.BatchNormalization()\n",
    "        \n",
    "        self.conv2 = tf.keras.layers.Conv2D(filters,(3,3), padding='same')\n",
    "        self.bn2 = tf.keras.layers.BatchNormalization()\n",
    "        \n",
    "        self.conv3 = tf.keras.layers.Conv2D(filters, (1,1))\n",
    "        self.bn3 = tf.keras.layers.BatchNormalization()\n",
    "    \n",
    "    def call(self, x, training=False):\n",
    "        f_x = self.bn1(self.conv1(x), training=training)\n",
    "        f_x = tf.nn.relu(f_x)\n",
    "        f_x = self.bn2(self.conv2(f_x), training=training)\n",
    "        f_x = tf.nn.relu(f_x)\n",
    "        f_x = self.bn3(self.conv3(f_x), training=training)\n",
    "        f_x = tf.nn.relu(f_x)\n",
    "        return x + f_x\n",
    "\n",
    "\n",
    "def create_model(conv_block, level_repeats=[3,3,3], initial_filter_size=32):\n",
    "    # encoder - input\n",
    "    model_input = tf.keras.Input(shape=(DATA_ROWS, DATA_COLS, DATA_CHANNELS), name='input_image')\n",
    "    x           = model_input\n",
    "    \n",
    "    x = tf.keras.layers.Conv2D(initial_filter_size, \n",
    "                               (7,7), \n",
    "                               strides=(1,1),\n",
    "                               activation='relu',\n",
    "                               padding='same')(x)\n",
    "    \n",
    "    \n",
    "    filter_size = initial_filter_size\n",
    "    for i in range(len(level_repeats)):\n",
    "        x = tf.keras.layers.Conv2D(filter_size, (3,3), strides=(2,2), activation='relu', padding='same')(x)\n",
    "        for n0 in range(level_repeats[i]):\n",
    "            x = conv_block(filter_size)(x)    \n",
    "        filter_size *= 2\n",
    "        \n",
    "        \n",
    "    encoder_output = x\n",
    "    # decoder\n",
    "    y              = tf.keras.layers.GlobalAveragePooling2D()(encoder_output)\n",
    "    decoder_output = tf.keras.layers.Dense(DATA_NUM_CLASSES, activation='softmax')(y)\n",
    "    \n",
    "    # forward path\n",
    "    model = tf.keras.Model(inputs=model_input, outputs=decoder_output, name='Mymodel')\n",
    "    # loss, backward path (implicit) and weight update\n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(TRAINING_LR_MAX), loss='categorical_crossentropy', metrics=['acc'])\n",
    "    # return model\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-25T05:10:35.082602Z",
     "start_time": "2020-02-25T05:10:33.604628Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"Mymodel\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_image (InputLayer)     [(None, 64, 64, 3)]       0         \n",
      "_________________________________________________________________\n",
      "conv2d (Conv2D)              (None, 64, 64, 64)        9472      \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 32, 32, 64)        36928     \n",
      "_________________________________________________________________\n",
      "res_net__block (ResNet_Block (None, 32, 32, 64)        46016     \n",
      "_________________________________________________________________\n",
      "res_net__block_1 (ResNet_Blo (None, 32, 32, 64)        46016     \n",
      "_________________________________________________________________\n",
      "res_net__block_2 (ResNet_Blo (None, 32, 32, 64)        46016     \n",
      "_________________________________________________________________\n",
      "conv2d_11 (Conv2D)           (None, 16, 16, 128)       73856     \n",
      "_________________________________________________________________\n",
      "res_net__block_3 (ResNet_Blo (None, 16, 16, 128)       182144    \n",
      "_________________________________________________________________\n",
      "res_net__block_4 (ResNet_Blo (None, 16, 16, 128)       182144    \n",
      "_________________________________________________________________\n",
      "res_net__block_5 (ResNet_Blo (None, 16, 16, 128)       182144    \n",
      "_________________________________________________________________\n",
      "conv2d_21 (Conv2D)           (None, 8, 8, 256)         295168    \n",
      "_________________________________________________________________\n",
      "res_net__block_6 (ResNet_Blo (None, 8, 8, 256)         724736    \n",
      "_________________________________________________________________\n",
      "res_net__block_7 (ResNet_Blo (None, 8, 8, 256)         724736    \n",
      "_________________________________________________________________\n",
      "res_net__block_8 (ResNet_Blo (None, 8, 8, 256)         724736    \n",
      "_________________________________________________________________\n",
      "global_average_pooling2d (Gl (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 200)               51400     \n",
      "=================================================================\n",
      "Total params: 3,325,512\n",
      "Trainable params: 3,317,448\n",
      "Non-trainable params: 8,064\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# create and compile model\n",
    "model = create_model(ResNet_Block, level_repeats=[3,3,3], initial_filter_size=64)\n",
    "# model description and figure\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-25T05:10:35.393972Z",
     "start_time": "2020-02-25T05:10:35.210472Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# learning rate schedule\n",
    "def lr_schedule(epoch):\n",
    "\n",
    "    # staircase\n",
    "    # lr = TRAINING_LR_MAX*math.pow(TRAINING_LR_SCALE, math.floor(epoch/TRAINING_LR_EPOCHS))\n",
    "\n",
    "    # linear warmup followed by cosine decay\n",
    "    if epoch < TRAINING_LR_INIT_EPOCHS:\n",
    "        lr = (TRAINING_LR_MAX - TRAINING_LR_INIT)*(float(epoch)/TRAINING_LR_INIT_EPOCHS) + TRAINING_LR_INIT\n",
    "    else:\n",
    "        lr = (TRAINING_LR_MAX - TRAINING_LR_FINAL)*max(0.0, math.cos(((float(epoch) - TRAINING_LR_INIT_EPOCHS)/(TRAINING_LR_FINAL_EPOCHS - 1.0))*(math.pi/2.0))) + TRAINING_LR_FINAL\n",
    "    return lr\n",
    "\n",
    "plt.plot(range(1,TRAINING_NUM_EPOCHS+1), [lr_schedule(i) for i in np.arange(TRAINING_NUM_EPOCHS)+1])\n",
    "plt.title(\"Learning Rate Schedule\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"l.r.\")\n",
    "plt.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-25T05:10:35.533280Z",
     "start_time": "2020-02-25T05:10:35.523304Z"
    }
   },
   "outputs": [],
   "source": [
    "# plot training accuracy and loss curves\n",
    "def plot_training_curves(history):\n",
    "\n",
    "    # training and validation data accuracy\n",
    "    acc     = history.history['accuracy']\n",
    "    val_acc = history.history['val_accuracy']\n",
    "\n",
    "    # training and validation data loss\n",
    "    loss     = history.history['loss']\n",
    "    val_loss = history.history['val_loss']\n",
    "\n",
    "    # plot accuracy\n",
    "    plt.figure(figsize=(8, 8))\n",
    "    plt.subplot(2, 1, 1)\n",
    "    plt.plot(acc, label='Training Accuracy')\n",
    "    plt.plot(val_acc, label='Validation Accuracy')\n",
    "    plt.legend(loc='lower right')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.ylim([min(plt.ylim()), 1])\n",
    "    plt.title('Training and Validation Accuracy')\n",
    "\n",
    "    # plot loss\n",
    "    plt.subplot(2, 1, 2)\n",
    "    plt.plot(loss, label='Training Loss')\n",
    "    plt.plot(val_loss, label='Validation Loss')\n",
    "    plt.legend(loc='upper right')\n",
    "    plt.ylabel('Cross Entropy')\n",
    "    plt.ylim([0, 2.0])\n",
    "    plt.title('Training and Validation Loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-25T06:47:03.152036Z",
     "start_time": "2020-02-25T05:10:35.677408Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train for 3333 steps, validate for 333 steps\n",
      "Epoch 1/30\n",
      "3332/3333 [============================>.] - ETA: 0s - loss: 5.2676 - acc: 0.0213\n",
      "Epoch 00001: val_loss improved from inf to 4.96955, saving model to F:\\Models\\ImageNet_64\\1\n",
      "WARNING:tensorflow:From C:\\Users\\harri\\Anaconda3\\envs\\deep-learning\\lib\\site-packages\\tensorflow_core\\python\\ops\\resource_variable_ops.py:1786: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "INFO:tensorflow:Assets written to: F:\\Models\\ImageNet_64\\1\\assets\n",
      "3333/3333 [==============================] - 265s 80ms/step - loss: 5.2676 - acc: 0.0213 - val_loss: 4.9696 - val_acc: 0.0411\n",
      "Epoch 2/30\n",
      "3332/3333 [============================>.] - ETA: 0s - loss: 4.4421 - acc: 0.0841\n",
      "Epoch 00002: val_loss improved from 4.96955 to 4.58747, saving model to F:\\Models\\ImageNet_64\\1\n",
      "INFO:tensorflow:Assets written to: F:\\Models\\ImageNet_64\\1\\assets\n",
      "3333/3333 [==============================] - 212s 64ms/step - loss: 4.4419 - acc: 0.0841 - val_loss: 4.5875 - val_acc: 0.0787\n",
      "Epoch 3/30\n",
      "3332/3333 [============================>.] - ETA: 0s - loss: 3.7590 - acc: 0.1679\n",
      "Epoch 00003: val_loss improved from 4.58747 to 3.99326, saving model to F:\\Models\\ImageNet_64\\1\n",
      "INFO:tensorflow:Assets written to: F:\\Models\\ImageNet_64\\1\\assets\n",
      "3333/3333 [==============================] - 215s 64ms/step - loss: 3.7589 - acc: 0.1679 - val_loss: 3.9933 - val_acc: 0.1475\n",
      "Epoch 4/30\n",
      "3332/3333 [============================>.] - ETA: 0s - loss: 3.4004 - acc: 0.2255\n",
      "Epoch 00004: val_loss improved from 3.99326 to 3.53790, saving model to F:\\Models\\ImageNet_64\\1\n",
      "INFO:tensorflow:Assets written to: F:\\Models\\ImageNet_64\\1\\assets\n",
      "3333/3333 [==============================] - 213s 64ms/step - loss: 3.4004 - acc: 0.2255 - val_loss: 3.5379 - val_acc: 0.2121\n",
      "Epoch 5/30\n",
      "3332/3333 [============================>.] - ETA: 0s - loss: 3.1520 - acc: 0.2719\n",
      "Epoch 00005: val_loss did not improve from 3.53790\n",
      "3333/3333 [==============================] - 198s 59ms/step - loss: 3.1521 - acc: 0.2719 - val_loss: 3.9109 - val_acc: 0.1845\n",
      "Epoch 6/30\n",
      "3332/3333 [============================>.] - ETA: 0s - loss: 2.9501 - acc: 0.3076\n",
      "Epoch 00006: val_loss did not improve from 3.53790\n",
      "3333/3333 [==============================] - 197s 59ms/step - loss: 2.9501 - acc: 0.3075 - val_loss: 3.5945 - val_acc: 0.2316\n",
      "Epoch 7/30\n",
      "3332/3333 [============================>.] - ETA: 0s - loss: 2.7030 - acc: 0.3552- ETA: 1s - lo\n",
      "Epoch 00007: val_loss improved from 3.53790 to 3.19018, saving model to F:\\Models\\ImageNet_64\\1\n",
      "INFO:tensorflow:Assets written to: F:\\Models\\ImageNet_64\\1\\assets\n",
      "3333/3333 [==============================] - 214s 64ms/step - loss: 2.7030 - acc: 0.3552 - val_loss: 3.1902 - val_acc: 0.2824\n",
      "Epoch 8/30\n",
      "3332/3333 [============================>.] - ETA: 0s - loss: 2.4826 - acc: 0.3991- ETA: 3s - l\n",
      "Epoch 00008: val_loss did not improve from 3.19018\n",
      "3333/3333 [==============================] - 203s 61ms/step - loss: 2.4827 - acc: 0.3991 - val_loss: 3.3543 - val_acc: 0.2633\n",
      "Epoch 9/30\n",
      "3332/3333 [============================>.] - ETA: 0s - loss: 2.2943 - acc: 0.4369- ETA: 0s - loss: 2.2948 -  - ETA: 0s - loss: 2.2945 - acc: 0.4369\n",
      "Epoch 00009: val_loss improved from 3.19018 to 2.81472, saving model to F:\\Models\\ImageNet_64\\1\n",
      "INFO:tensorflow:Assets written to: F:\\Models\\ImageNet_64\\1\\assets\n",
      "3333/3333 [==============================] - 222s 67ms/step - loss: 2.2942 - acc: 0.4370 - val_loss: 2.8147 - val_acc: 0.3508\n",
      "Epoch 10/30\n",
      "3332/3333 [============================>.] - ETA: 0s - loss: 2.1131 - acc: 0.4732- ETA: 2s - loss: 2.1142 - acc: 0.473 - ETA: 2s - loss: 2.1142  - ETA: 1s - loss: 2.1134  - ETA: 0s - loss: 2.1130 - ac\n",
      "Epoch 00010: val_loss did not improve from 2.81472\n",
      "3333/3333 [==============================] - 204s 61ms/step - loss: 2.1130 - acc: 0.4732 - val_loss: 3.0097 - val_acc: 0.3295\n",
      "Epoch 11/30\n",
      "3332/3333 [============================>.] - ETA: 0s - loss: 1.9453 - acc: 0.5089- E\n",
      "Epoch 00011: val_loss did not improve from 2.81472\n",
      "3333/3333 [==============================] - 204s 61ms/step - loss: 1.9454 - acc: 0.5088 - val_loss: 2.9549 - val_acc: 0.3552\n",
      "Epoch 12/30\n",
      "3332/3333 [============================>.] - ETA: 0s - loss: 1.7843 - acc: 0.5419- - ETA: 3s - loss:\n",
      "Epoch 00012: val_loss did not improve from 2.81472\n",
      "3333/3333 [==============================] - 204s 61ms/step - loss: 1.7842 - acc: 0.5419 - val_loss: 2.9389 - val_acc: 0.3609\n",
      "Epoch 13/30\n",
      "3332/3333 [============================>.] - ETA: 0s - loss: 1.6204 - acc: 0.5773\n",
      "Epoch 00013: val_loss improved from 2.81472 to 2.69689, saving model to F:\\Models\\ImageNet_64\\1\n",
      "INFO:tensorflow:Assets written to: F:\\Models\\ImageNet_64\\1\\assets\n",
      "3333/3333 [==============================] - 223s 67ms/step - loss: 1.6204 - acc: 0.5773 - val_loss: 2.6969 - val_acc: 0.4145\n",
      "Epoch 14/30\n",
      "3332/3333 [============================>.] - ETA: 0s - loss: 1.4565 - acc: 0.6128\n",
      "Epoch 00014: val_loss did not improve from 2.69689\n",
      "3333/3333 [==============================] - 192s 58ms/step - loss: 1.4565 - acc: 0.6128 - val_loss: 2.8123 - val_acc: 0.3898\n",
      "Epoch 15/30\n",
      "3332/3333 [============================>.] - ETA: 0s - loss: 1.2890 - acc: 0.6521\n",
      "Epoch 00015: val_loss did not improve from 2.69689\n",
      "3333/3333 [==============================] - 176s 53ms/step - loss: 1.2890 - acc: 0.6521 - val_loss: 3.0485 - val_acc: 0.3833\n",
      "Epoch 16/30\n",
      "3332/3333 [============================>.] - ETA: 0s - loss: 1.1434 - acc: 0.6855\n",
      "Epoch 00016: val_loss did not improve from 2.69689\n",
      "3333/3333 [==============================] - 176s 53ms/step - loss: 1.1433 - acc: 0.6855 - val_loss: 2.8970 - val_acc: 0.4185\n",
      "Epoch 17/30\n",
      "3332/3333 [============================>.] - ETA: 0s - loss: 0.9838 - acc: 0.7252\n",
      "Epoch 00017: val_loss did not improve from 2.69689\n",
      "3333/3333 [==============================] - 176s 53ms/step - loss: 0.9839 - acc: 0.7252 - val_loss: 2.9006 - val_acc: 0.4219\n",
      "Epoch 18/30\n",
      "3332/3333 [============================>.] - ETA: 0s - loss: 0.8399 - acc: 0.7600\n",
      "Epoch 00018: val_loss did not improve from 2.69689\n",
      "3333/3333 [==============================] - 176s 53ms/step - loss: 0.8400 - acc: 0.7600 - val_loss: 3.4069 - val_acc: 0.3735\n",
      "Epoch 19/30\n",
      "3331/3333 [============================>.] - ETA: 0s - loss: 0.7001 - acc: 0.7957\n",
      "Epoch 00019: val_loss did not improve from 2.69689\n",
      "3333/3333 [==============================] - 176s 53ms/step - loss: 0.6999 - acc: 0.7958 - val_loss: 3.2918 - val_acc: 0.3960\n",
      "Epoch 20/30\n",
      "3332/3333 [============================>.] - ETA: 0s - loss: 0.5773 - acc: 0.8303\n",
      "Epoch 00020: val_loss did not improve from 2.69689\n",
      "3333/3333 [==============================] - 176s 53ms/step - loss: 0.5772 - acc: 0.8303 - val_loss: 3.1989 - val_acc: 0.4102\n",
      "Epoch 21/30\n",
      "3331/3333 [============================>.] - ETA: 0s - loss: 0.4632 - acc: 0.8615\n",
      "Epoch 00021: val_loss did not improve from 2.69689\n",
      "3333/3333 [==============================] - 176s 53ms/step - loss: 0.4632 - acc: 0.8615 - val_loss: 3.3688 - val_acc: 0.4020\n",
      "Epoch 22/30\n",
      "3332/3333 [============================>.] - ETA: 0s - loss: 0.3635 - acc: 0.8923\n",
      "Epoch 00022: val_loss did not improve from 2.69689\n",
      "3333/3333 [==============================] - 177s 53ms/step - loss: 0.3635 - acc: 0.8923 - val_loss: 3.6743 - val_acc: 0.3992\n",
      "Epoch 23/30\n",
      "3332/3333 [============================>.] - ETA: 0s - loss: 0.2831 - acc: 0.9170\n",
      "Epoch 00023: val_loss did not improve from 2.69689\n",
      "3333/3333 [==============================] - 176s 53ms/step - loss: 0.2831 - acc: 0.9170 - val_loss: 3.5634 - val_acc: 0.4223\n",
      "Epoch 24/30\n",
      "3332/3333 [============================>.] - ETA: 0s - loss: 0.2155 - acc: 0.9381\n",
      "Epoch 00024: val_loss did not improve from 2.69689\n",
      "3333/3333 [==============================] - 176s 53ms/step - loss: 0.2155 - acc: 0.9381 - val_loss: 3.7998 - val_acc: 0.4108\n",
      "Epoch 25/30\n",
      "3332/3333 [============================>.] - ETA: 0s - loss: 0.1568 - acc: 0.9570- ETA:\n",
      "Epoch 00025: val_loss did not improve from 2.69689\n",
      "3333/3333 [==============================] - 176s 53ms/step - loss: 0.1568 - acc: 0.9570 - val_loss: 3.7619 - val_acc: 0.4227\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26/30\n",
      "3332/3333 [============================>.] - ETA: 0s - loss: 0.1119 - acc: 0.9719\n",
      "Epoch 00026: val_loss did not improve from 2.69689\n",
      "3333/3333 [==============================] - 176s 53ms/step - loss: 0.1119 - acc: 0.9719 - val_loss: 3.7851 - val_acc: 0.4223\n",
      "Epoch 27/30\n",
      "3332/3333 [============================>.] - ETA: 0s - loss: 0.0750 - acc: 0.9834\n",
      "Epoch 00027: val_loss did not improve from 2.69689\n",
      "3333/3333 [==============================] - 176s 53ms/step - loss: 0.0750 - acc: 0.9834 - val_loss: 3.8124 - val_acc: 0.4261\n",
      "Epoch 28/30\n",
      "3332/3333 [============================>.] - ETA: 0s - loss: 0.0522 - acc: 0.9902\n",
      "Epoch 00028: val_loss did not improve from 2.69689\n",
      "3333/3333 [==============================] - 176s 53ms/step - loss: 0.0522 - acc: 0.9902 - val_loss: 3.7269 - val_acc: 0.4337\n",
      "Epoch 29/30\n",
      "3332/3333 [============================>.] - ETA: 0s - loss: 0.0355 - acc: 0.9946\n",
      "Epoch 00029: val_loss did not improve from 2.69689\n",
      "3333/3333 [==============================] - 177s 53ms/step - loss: 0.0355 - acc: 0.9946 - val_loss: 3.7248 - val_acc: 0.4422\n",
      "Epoch 30/30\n",
      "3332/3333 [============================>.] - ETA: 0s - loss: 0.0271 - acc: 0.9968\n",
      "Epoch 00030: val_loss did not improve from 2.69689\n",
      "3333/3333 [==============================] - 176s 53ms/step - loss: 0.0271 - acc: 0.9968 - val_loss: 3.6735 - val_acc: 0.4434\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'accuracy'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-62711cedcc0f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[1;31m# plot accuracy and loss curves\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 24\u001b[1;33m \u001b[0mplot_training_curves\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-7-fcc6d00279ac>\u001b[0m in \u001b[0;36mplot_training_curves\u001b[1;34m(history)\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[1;31m# training and validation data accuracy\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m     \u001b[0macc\u001b[0m     \u001b[1;33m=\u001b[0m \u001b[0mhistory\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'accuracy'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m     \u001b[0mval_acc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhistory\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'val_accuracy'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'accuracy'"
     ]
    }
   ],
   "source": [
    "training_step_size = 100000//TRAINING_NUM_EPOCHS\n",
    "val_step_size = 10000//TRAINING_NUM_EPOCHS\n",
    "\n",
    "# callbacks (learning rate schedule, model checkpointing during training)\n",
    "callbacks = [tf.keras.callbacks.LearningRateScheduler(lr_schedule),\n",
    "             tf.keras.callbacks.ModelCheckpoint(filepath=str(SAVE_MODEL_PATH), \n",
    "                                                save_best_only=True, \n",
    "                                                monitor='val_loss', \n",
    "                                                verbose=1)]\n",
    "\n",
    "# training\n",
    "initial_epoch_num = 0\n",
    "history           = model.fit(x=train_ds_cachefile, \n",
    "                              epochs=TRAINING_NUM_EPOCHS, \n",
    "                              verbose=1, \n",
    "                              steps_per_epoch=training_step_size,\n",
    "                              callbacks=callbacks, \n",
    "                              validation_data=valid_ds_cachefile,                               \n",
    "                              validation_steps=val_step_size,\n",
    "                              initial_epoch=initial_epoch_num)\n",
    "\n",
    "\n",
    "# plot accuracy and loss curves\n",
    "plot_training_curves(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-25T16:29:13.286596Z",
     "start_time": "2020-02-25T16:29:02.841250Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "333/333 [==============================] - 10s 31ms/step - loss: 0.9630 - acc: 0.1566\n",
      "Test loss:      0.9630128982597151\n",
      "Test accuracy:  0.15662538\n"
     ]
    }
   ],
   "source": [
    "# test\n",
    "test_loss, test_accuracy = model.evaluate(x=test_ds_nocache, steps=val_step_size)\n",
    "print('Test loss:     ', test_loss)\n",
    "print('Test accuracy: ', test_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:deep-learning]",
   "language": "python",
   "name": "conda-env-deep-learning-py"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
