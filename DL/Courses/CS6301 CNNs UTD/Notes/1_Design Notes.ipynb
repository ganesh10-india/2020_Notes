{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Design\" data-toc-modified-id=\"Design-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Design</a></span><ul class=\"toc-item\"><li><span><a href=\"#Keep-your-goals-Simple\" data-toc-modified-id=\"Keep-your-goals-Simple-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>Keep your goals Simple</a></span></li><li><span><a href=\"#CNN-Design-Overview\" data-toc-modified-id=\"CNN-Design-Overview-1.2\"><span class=\"toc-item-num\">1.2&nbsp;&nbsp;</span>CNN Design Overview</a></span><ul class=\"toc-item\"><li><span><a href=\"#CNN-Architecture-(Image-Classification)\" data-toc-modified-id=\"CNN-Architecture-(Image-Classification)-1.2.1\"><span class=\"toc-item-num\">1.2.1&nbsp;&nbsp;</span>CNN Architecture (Image Classification)</a></span><ul class=\"toc-item\"><li><span><a href=\"#Head\" data-toc-modified-id=\"Head-1.2.1.1\"><span class=\"toc-item-num\">1.2.1.1&nbsp;&nbsp;</span>Head</a></span></li><li><span><a href=\"#Tail-and-Body\" data-toc-modified-id=\"Tail-and-Body-1.2.1.2\"><span class=\"toc-item-num\">1.2.1.2&nbsp;&nbsp;</span>Tail and Body</a></span></li><li><span><a href=\"#Receptive-field-size-at-the-head\" data-toc-modified-id=\"Receptive-field-size-at-the-head-1.2.1.3\"><span class=\"toc-item-num\">1.2.1.3&nbsp;&nbsp;</span>Receptive field size at the head</a></span></li></ul></li></ul></li></ul></li><li><span><a href=\"#CNN-Architectures\" data-toc-modified-id=\"CNN-Architectures-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>CNN Architectures</a></span><ul class=\"toc-item\"><li><span><a href=\"#Serial-(LeNet,-AlexNet,-VGG,-MobileNet-V1)\" data-toc-modified-id=\"Serial-(LeNet,-AlexNet,-VGG,-MobileNet-V1)-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>Serial (LeNet, AlexNet, VGG, MobileNet V1)</a></span></li><li><span><a href=\"#Parallel-(GoogLeNet,-Inceptions,-SqueezeNet)\" data-toc-modified-id=\"Parallel-(GoogLeNet,-Inceptions,-SqueezeNet)-2.2\"><span class=\"toc-item-num\">2.2&nbsp;&nbsp;</span>Parallel (GoogLeNet, Inceptions, SqueezeNet)</a></span></li><li><span><a href=\"#Dense-(DenseNet)\" data-toc-modified-id=\"Dense-(DenseNet)-2.3\"><span class=\"toc-item-num\">2.3&nbsp;&nbsp;</span>Dense (DenseNet)</a></span></li><li><span><a href=\"#Residual\" data-toc-modified-id=\"Residual-2.4\"><span class=\"toc-item-num\">2.4&nbsp;&nbsp;</span>Residual</a></span></li><li><span><a href=\"#Neural-Architecture-Search\" data-toc-modified-id=\"Neural-Architecture-Search-2.5\"><span class=\"toc-item-num\">2.5&nbsp;&nbsp;</span>Neural Architecture Search</a></span><ul class=\"toc-item\"><li><span><a href=\"#Notes-for-Vision-Optimized-CNNs\" data-toc-modified-id=\"Notes-for-Vision-Optimized-CNNs-2.5.1\"><span class=\"toc-item-num\">2.5.1&nbsp;&nbsp;</span>Notes for Vision Optimized CNNs</a></span></li></ul></li></ul></li><li><span><a href=\"#RNNs\" data-toc-modified-id=\"RNNs-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>RNNs</a></span><ul class=\"toc-item\"><li><span><a href=\"#RNN\" data-toc-modified-id=\"RNN-3.1\"><span class=\"toc-item-num\">3.1&nbsp;&nbsp;</span>RNN</a></span></li><li><span><a href=\"#GRU\" data-toc-modified-id=\"GRU-3.2\"><span class=\"toc-item-num\">3.2&nbsp;&nbsp;</span>GRU</a></span></li><li><span><a href=\"#LSTM\" data-toc-modified-id=\"LSTM-3.3\"><span class=\"toc-item-num\">3.3&nbsp;&nbsp;</span>LSTM</a></span></li></ul></li><li><span><a href=\"#Attention\" data-toc-modified-id=\"Attention-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Attention</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-02T16:00:24.128154Z",
     "start_time": "2020-04-02T16:00:18.448338Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.1.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import *\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Design\n",
    "\n",
    "[Slides](https://github.com/arthurredfern/UT-Dallas-CS-6301-CNNs/blob/master/Lectures/xNNs_050_Design.pdf)\n",
    "\n",
    "[On disk](file:///F:/Data/xNNs_050_Design.pdf)\n",
    "\n",
    "## Keep your goals Simple\n",
    "- The simpler the task, the more training data there is. The more data, the better a NN will perform\n",
    "- Handle complex problems with a shallow combination of simple problems.\n",
    "\n",
    "## CNN Design Overview\n",
    "- CNN exploits spatial structure in data. correlations between loaclized regions of pixels.\n",
    "- __Encoder__ (Tail & Body for feature extraction) + __decoder__ (head for prediction)\n",
    "    - Commonly: Serial, Parallel, Dense, or Residual\n",
    "    \n",
    "- __Tail__- few specialized layers with weaker features and better localization.\n",
    "- __Body__ - layers that define the network. strong features with worse localization.\n",
    "- __Head__ - task specific layers that produce the output of the network\n",
    "\n",
    "\n",
    "### CNN Architecture (Image Classification)\n",
    "\n",
    "Classification network inputs an image and ouputs a vector, whose largest elements are the predicted class.\n",
    "\n",
    "$$TAIL\\rightarrow BODY \\rightarrow HEAD$$\n",
    "\n",
    "$$ENCODER\\rightarrow DECODER$$\n",
    "\n",
    "#### Head\n",
    "\n",
    "The __head__ estimates the dominant object in a specific region by using the feature vector in the corresponding region of the feature maps at the body ouput. \n",
    "- __Global average pooling__ will extract information from each feature map (presence of feature across all areas) and the head will use a linear combination of these global features to predict the dominant object of the image. Global average Pooling or vectorization can be used to convert the output of the body to a 1xN vector for input into the Linear layers of the head.\n",
    "\n",
    "GAP is more popular since it allows arbitrary sized input images and also reduces the computational complexity of the decoding phase.\n",
    "\n",
    "\n",
    "Different head designs can be created to accomplish different goals, and more than one head can be attached to the end of a tail and body.\n",
    "\n",
    "#### Tail and Body\n",
    "\n",
    "Tail and body transform entire regions of the input image to feature vectures across the ouput feature maps. The __receptive field__ is the area of pixels from the input that can be mapped to a feature vector output.\n",
    "\n",
    "Each activation in the output of the body relates the confidence of the presence of a class in a certain area (size receptive field). Averaging over every region leads to the final prediction in the head of the dominant class in the image.\n",
    "\n",
    "\n",
    "##### Tail\n",
    "- High compute, high feature map memory low parameter memory, lots of spatial redundancy.\n",
    "- Aggressive down sampling and aggressive increase in number of channels.\n",
    "\n",
    "Common architectures:\n",
    "- conv (7,7) stride 2, 64 channels -> max pool (3x3) stride 2\n",
    "- conv(3,3) stride 2, 32 channels -> conv (3,3) s=1, c=64 -> conv(3,3) s=2, c=64\n",
    "\n",
    "\n",
    "##### Body\n",
    "Blocks followed by down-sampling. Gradual reduction in memory required for feature maps. Leads to a loss in spatial resolution (pooling throws out pixels) Increase in number of channels\n",
    "\n",
    "Common design practices:\n",
    "- reduce cols and rows by 1/2 and double the number of channels\n",
    "    - data volume shrinks by a factor of 2 (reduces compute by factor of 2)\n",
    "  \n",
    "#### Receptive field size at the head\n",
    "Shows the area of information that the classifier has to work with at inference. May affect images with higher resolution.\n",
    "\n",
    "__Calculation of Receptive Field__\n",
    "1. Start at the output of the body and set r.f.=1\n",
    "2. Move backwards in network.\n",
    "3. Everytime you reach a filter, increase the receptive field size by F-1. (F is length of filter eg 3x3)\n",
    "4. Everytime you reach a down sample multiply the receptive field by S, then subtract by S -1 (S the down sampling factor eg 2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN Architectures\n",
    "\n",
    "## Serial (LeNet, AlexNet, VGG, MobileNet V1)\n",
    "\n",
    "__VGGNet__ introduced stacked 3x3 conv filters. Used vectorization at the head\n",
    "   - Stacking 3x3 convolutions reduces compute while expanding receptive field.\n",
    "    \n",
    "\n",
    "__[MobileNet V1](https://arxiv.org/pdf/1704.04861.pdf)__ uses cascade of 3x3 spatial and 1x1 channel convs. Utilized global avg pool at head.\n",
    "   - __Depthwise-seperable convolutions__. Depthwise convolution has a nxnx1 kernel for each channel of the input. Pointwise convolution has 1x1xc kernels for c channels in input. [depthwise conv explained](https://towardsdatascience.com/a-basic-introduction-to-separable-convolutions-b99ec3102728)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MobileNetV1(inputs, num_filters, strides=(1,1)):\n",
    "    \"\"\"\n",
    "    Conv Block from the MobileNetV1 architecture.\n",
    "    \n",
    "    inputs: Tensor- input to the first layer\n",
    "    num_filters: int - desired number of channels in output\n",
    "    strides: tuple-strides for the first convolution\n",
    "    \"\"\"\n",
    "    x = DepthwiseConv2D((3,3), strides=strides)(inputs)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = ReLU()(x)\n",
    "    \n",
    "    x = Conv2D(num_filters, (1, 1))(x) #pointwise conv\n",
    "    x = BatchNormalization()(x) \n",
    "    x = ReLU()(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parallel (GoogLeNet, Inceptions, SqueezeNet)\n",
    "Input split $\\rightarrow$ parallel ops $\\rightarrow$ output combine. \n",
    "\n",
    "[Inception overview](https://towardsdatascience.com/a-simple-guide-to-the-versions-of-the-inception-network-7fc52b863202)\n",
    "\n",
    "__[Inception V1](https://arxiv.org/pdf/1409.4842v1.pdf)__ - Size of the object may differ across images So build parallel convolution pathways with different receptive fields.\n",
    "\n",
    "Authors limited the compute by adding 1x1 pontwise convolutions (reducing input channels) before each of the conv branches. References the *Network in Network* paper as the source of pointwise conv.\n",
    "\n",
    "Authors also include a Dropout layer after GAP with dropout prob of 0.4.\n",
    "\n",
    "Authors use axillary classifiers, heads inside of the network that predict the output. This attempts to allow clean gradient flow to earler parts of the network.\n",
    "\n",
    "<img src=\"../img/lenet.PNG\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_block(inputs, filters, kernel_size=(3,3), strides=(1,1)):\n",
    "    \"\"\"Generic Conv -> BN -> ReLU abstraction\"\"\"\n",
    "    x = Conv2D(filters, kernel_size, strides=strides)(inputs)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = ReLU()(x)\n",
    "    return x  \n",
    "\n",
    "def InceptionV1(inputs, squeeze_dims, out_dims, strides=(1,1)):\n",
    "    \"\"\"\n",
    "    Inception V1 Conv Block (GoogLeNet).\n",
    "    \"\"\"\n",
    "    x1 = conv_block(inputs, out_dims, kernel_size=(1,1))\n",
    "    \n",
    "    x2 = conv_block(inputs, squeeze_dims, kernel_size=(1,1))\n",
    "    x2 = conv_block(x2, out_dims, kernel_size=(3,3))\n",
    "    \n",
    "    x3 = conv_block(inputs, squeeze_dims, kernel_size=(1,1))\n",
    "    x3 = conv_block(x3, out_dims, kernel_size=(5,5))\n",
    "    \n",
    "    x4 = MaxPool2D(pool_size=(3,3), strides=(1,1))(inputs)\n",
    "    x4 = conv_block(inputs, out_dims, kernel_size=(1,1))\n",
    "    \n",
    "    concat = tf.concat([x1,x2,x3,x4],3)\n",
    "    return concat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__[Inception V2 and V3](https://arxiv.org/pdf/1512.00567.pdf)__\n",
    "\n",
    "__Inception V2__ focused on: \n",
    "- avoiding representational bottlenecks with extreme compression. \n",
    "- converting the 5x5 convolution to two stacked 3x3 convolutions. \n",
    "- Also focuses on reducing compute by factorizing an nxn convolution to an nx1 and 1xn convolutions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def InceptionV2_fig5(inputs, squeeze_dims, out_dims, strides=(1,1)):\n",
    "    \"\"\"\n",
    "    Inception V2 Figure 5\n",
    "    \"\"\"\n",
    "    x1 = conv_block(inputs, squeeze_dims, kernel_size=(1,1))\n",
    "    \n",
    "    x2 = conv_block(inputs, squeeze_dims, kernel_size=(1,1))\n",
    "    x2 = conv_block(x2, out_dims, kernel_size=(3,3))\n",
    "    \n",
    "    x3 = conv_block(inputs, squeeze_dims, kernel_size=(1,1))\n",
    "    x3 = conv_block(x3, out_dims, kernel_size=(3,3))\n",
    "    x3 = conv_block(x3, out_dims, kernel_size=(3,3))\n",
    "    \n",
    "    x4 = MaxPool2D(pool_size=(3,3), strides=(1,1))(inputs)\n",
    "    x4 = conv_block(inputs, squeeze_dims, kernel_size=(1,1))\n",
    "    \n",
    "    concat = tf.concat([x1,x2,x3,x4],3)\n",
    "    return concat\n",
    "\n",
    "def InceptionV2_fig6(inputs, squeeze_dims, out_dims, strides=(1,1)):\n",
    "    \"\"\"\n",
    "    Inception V2 figure 6\n",
    "    \"\"\"\n",
    "    x1 = conv_block(inputs, squeeze_dims, kernel_size=(1,1))\n",
    "    \n",
    "    x2 = conv_block(inputs, squeeze_dims, kernel_size=(1,1))\n",
    "    x2 = conv_block(x2, out_dims, kernel_size=(3,1))\n",
    "    x2 = conv_block(x2, out_dims, kernel_size=(1,3))\n",
    "    \n",
    "    x3 = conv_block(inputs, squeeze_dims, kernel_size=(1,1))\n",
    "    x3 = conv_block(x3, out_dims, kernel_size=(3,1))\n",
    "    x3 = conv_block(x3, out_dims, kernel_size=(1,3))\n",
    "    x3 = conv_block(x3, out_dims, kernel_size=(3,1))\n",
    "    x3 = conv_block(x3, out_dims, kernel_size=(1,3))\n",
    "    \n",
    "    x4 = MaxPool2D(pool_size=(3,3), strides=(1,1))(inputs)\n",
    "    x4 = conv_block(inputs, squeeze_dims, kernel_size=(1,1))\n",
    "    \n",
    "    concat = tf.concat([x1,x2,x3,x4],3)\n",
    "    return concat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Inception V3__ \n",
    "- adds RMSProp optimizer\n",
    "- factorized 7x7 convolutions\n",
    "- Batch Norm in auxillary classifiers\n",
    "- label smoothing (regularization)\n",
    "\n",
    "__[Inception V4](https://arxiv.org/pdf/1602.07261.pdf)__\n",
    "\n",
    "Changes the tail design and introduces 5 new inception blocks. Also introduces an Inception-Resnet V1 and V2 architecture that make use of skip connections.\n",
    "\n",
    "__[SqueezeNet](https://arxiv.org/abs/1602.07360)__\n",
    "\n",
    "Parallel block design. A  1x1 convolutions that squeezes the channel dimension. Followed by parallel 1x1 and 3x3 convolutions that expand the channel dim.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dense (DenseNet)\n",
    "Input split into a trasformation path and an identity path. Results are concatenated (Adds width to the netwprk w/ features at different depths) \n",
    "\n",
    "## Residual\n",
    "Instead of concat as in DenseNet perform an add on the two splits.\n",
    "\n",
    "__[ResNetV1](https://arxiv.org/pdf/1512.03385.pdf)__ Add the input of a convolutional block to the results of the block's convolutions. These \"skip connections\" allow networks to train faster by allowing the clean flow of gradients through the network.\n",
    "\n",
    "Do the ReLU after the Add so the feature maps are compatable (both have positive and negative values)\n",
    "\n",
    "<img src=\"../img/resnet.png\">\n",
    "\n",
    "[This paper](https://arxiv.org/pdf/1603.05027.pdf) describes the importance of \"pre-activation\" for the clean flow of gradients through the network.\n",
    "<img src=\"../img/preactivation.png\" height=200>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ResNetV1(inputs, channel_dims, strides=(1,1)):\n",
    "    \"\"\"ResNet block w preactivation\"\"\"\n",
    "    #RESIDUAL PATH\n",
    "    resid = BatchNormalization()(inputs)\n",
    "    resid = ReLU()(resid)\n",
    "    resid = conv_block(inputs, channel_dims, kernel_size=(3,3), strides=strides)\n",
    "    resid = Conv2D(channel_dims, (3,3), strides=(1,1))(resid)\n",
    "    #IDENTITY PATH\n",
    "    if strides==(2,2):\n",
    "        inputs = Conv2D(channel_dims, (1,1), strides=strides)(inputs)\n",
    "    #COMBINE\n",
    "    combined = Add()([inputs, resid])\n",
    "    return combined"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__[ResNeXT](https://arxiv.org/abs/1611.05431)__\n",
    "\n",
    "Replace 3x3 convolutions in the\n",
    "residual bottleneck with highly\n",
    "grouped 3x3 convolutions\n",
    "\n",
    "<img src=\"../img/resnext.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__[MobileNet V2](https://arxiv.org/abs/1801.04381)__\n",
    "\n",
    "- Uses an expansion 1x1 conv\n",
    "- Utilizes depthwise seperable convolutions\n",
    "- Utilizes ReLU6\n",
    "\n",
    "The inverted residual ends the block with a compression 1x1 conv that shrinks the number of channel dims."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-27T02:55:28.430847Z",
     "start_time": "2020-02-27T02:55:28.417881Z"
    }
   },
   "outputs": [],
   "source": [
    "def inverted_residual(inputs, expand_channels, squeeze_channels, strides=(1,1)):\n",
    "    \"\"\"\n",
    "    Inverted residual a la MobileNet V2 note the channel dimension will\n",
    "     be expanded by pointwise conv, processed with depthwise conv, then \n",
    "     compressed by a linear bottleneck\n",
    "    \"\"\"\n",
    "    x = Conv2D(expand_channels, (1, 1), strides=strides, padding='same')(inputs)\n",
    "    x = BatchNormalization(**bn_params)(x)\n",
    "    x = ReLU(max_value=6)(x)  # the paper uses a thresholded ReLU (3-bit output)\n",
    "    \n",
    "    x = DepthwiseConv2D((3,3), strides=(1,1), **conv_params)(x)\n",
    "    x = BatchNormalization(**bn_params)(x)\n",
    "    x = ReLU(max_value=6)(x)\n",
    "    \n",
    "    x = Conv2D(squeeze_channels, (1, 1), strides=(1, 1), padding='same')(x)\n",
    "    x = BatchNormalization(**bn_params)(x) #No activation here (Linear BottleNeck)\n",
    "    \n",
    "    if strides==(2,2): # maintain dimensions during downsampling\n",
    "        inputs = Conv2D(squeeze_channels, (1, 1), strides=strides, padding='same')(inputs)\n",
    "    \n",
    "    return Add()([x, inputs])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__[ShuffleNet](https://arxiv.org/pdf/1707.01083.pdf)__\n",
    "\n",
    "__[ShiftNet](https://arxiv.org/abs/1711.08141)__\n",
    "\n",
    "__[Residual Squeeze and Excite](https://arxiv.org/abs/1709.01507)__\n",
    "\n",
    "__[Residual Selective Kernel Methods](https://arxiv.org/abs/1903.06586)__\n",
    "\n",
    "__[ShuffleNet V2](https://arxiv.org/abs/1807.11164)__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Architecture Search\n",
    "\n",
    "[1](https://www.fast.ai/2018/07/23/auto-ml-3)\n",
    "\n",
    "[survey](https://arxiv.org/abs/1808.05377)\n",
    "\n",
    "[survey](https://www.ml4aad.org/automl/literature-on-neural-architecture-search/)\n",
    "\n",
    "__[MnasNet](https://ai.googleblog.com/2018/08/mnasnet-towards-automating-design-of.html)__\n",
    "\n",
    "__[EfficientNet](https://arxiv.org/abs/1905.11946)__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notes for Vision Optimized CNNs\n",
    "- Target a feature size of 6-8x6-8 before GAP\n",
    "- Early body blocks will have a high compute cost, while late blocks will require more memory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNNs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exploit sequential structure in data\n",
    "\n",
    "## RNN\n",
    "$$y_t = f(x_t^TH+y_{t-1}^TG+v^T)$$\n",
    "\n",
    "## GRU\n",
    "Update Gate:\n",
    "$$u_t^T=\\sigma(x_t^TH_U+y_{t-1}^TG_U+v_U^T)$$\n",
    "\n",
    "Reset Gate:\n",
    "$$r_t^T=\\sigma(x_t^TH_r+y_{t-1}^TG_r+v_r^T)$$\n",
    "\n",
    "Output:\n",
    "$$y_t^T=[u_t\\circ y_{t-1}]+[(1-u_t)\\circ \\sigma(x_t^TH_y+(r_t\\circ y_{t-1})G_y+v_y)$$\n",
    "\n",
    "$\\circ$ is Hadamard product (pointwse multiplication)\n",
    "\n",
    "## LSTM\n",
    "\n",
    "[Understanding LSTMS](https://colah.github.io/posts/2015-08-Understanding-LSTMs/)\n",
    "\n",
    "<img src=\"../img/lstm.png\">\n",
    "\n",
    "The __cell state__ is persistent memory that can be updated or modfied at each time step.\n",
    "\n",
    "The __forget gate__ is a sigmoid operation wrapped arounf a dense unit. The output will be hadamard multiplied with the cell state and \"remove information\"\n",
    "$$f_t=\\sigma(W_f\\cdot concat(h_{t-1},x_t)+b_f)$$\n",
    "\n",
    "__Input Gate__ decides which values of the cell state wil be updated. \n",
    "$$i_t=\\sigma(W_i\\cdot concat(h_{t-1},x_t)+b_i)$$\n",
    "\n",
    "This is Hadamard multiplied with __candidate values__ created by a tanh dense operation (outputs bounded (-1,1))\n",
    "$$\\tilde{C_t} = tanh(W_c\\cdot concat(h_{t-1},x_t)+b_c)$$\n",
    "\n",
    "The new cell state is then modified\n",
    "$$C_t = f_t*C_{t-1}+i_t*\\tilde{C_t}$$\n",
    "\n",
    "\n",
    "Lastly the output will be a filtered version of the cell state. The __output gate__ is another sigmoid-dense operation that will be multiplied with the cell state to produce the __hidden state__ $h_t$\n",
    "\n",
    "$$o_t=\\sigma(W_o\\cdot concat(h_{t-1},x_t)+b_o)$$\n",
    "\n",
    "$$h_t=o_t*tanh(C_t)$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TensorFlow APIs\n",
    "tf.keras.layers.RNN()\n",
    "tf.keras.layers.GRU()\n",
    "tf.keras.layers.LSTM()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Attention](https://distill.pub/2016/augmented-rnns/)\n",
    "[The Illustrated Transformer](http://jalammar.github.io/illustrated-transformer/)\n",
    "[Annotated](http://nlp.seas.harvard.edu/2018/04/03/attention.html)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:deep-learning]",
   "language": "python",
   "name": "conda-env-deep-learning-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
