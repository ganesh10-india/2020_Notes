{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[slides](https://github.com/arthurredfern/UT-Dallas-CS-6301-CNNs/blob/master/Lectures/xNNs_070_Implementation.pdf)\n",
    "\n",
    "[disk](file:///F:/Data/xNNs_070_Implementation.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementation Notes\n",
    "\n",
    "## Hardware and Software\n",
    "\n",
    "Software can be more general (hardware agnostic) that leans on runtime intelligent hardware to optimize the perforance of the program duting execution. \n",
    "\n",
    "Software can rely on compile time intelligent hardware. \n",
    "\n",
    "### __Graph Specification__ \n",
    "Is a representation of a high-level hardware agnostic computer algorithm. nodes are operators. Implicit data and instruction movement. Edges prepresent dependenies (memory)\n",
    "\n",
    "### __Graph Lowering__ \n",
    "Map a high level program to a low-level, hardware specific graph. Includes compute and communication nodes, as well as data and instruction edges. Nodes map 1 to 1 to hardware.\n",
    "\n",
    "This is an iterative optimization process that can be improved with knowledge of domain and hardware.\n",
    "\n",
    "### __Graph Execution__ \n",
    "Software runtime runs on a control processor and cycles through nodes on the low level graph. Hardware execures the nodes with computation and communication primitives as well as general compute for everything else."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimized XNN Design\n",
    "\n",
    "Some hardware aware design considerations:\n",
    "- Operator selection: quantization, sparisification, and compression\n",
    "- Network sizing: depth, width, input size\n",
    "\n",
    "### __Quantization__ \n",
    "__Quantization__ reduces the number of bits required to represent feature maps and parameters. \n",
    "\n",
    "Quantizing deep convolutional networks for efficient inference: A whitepaper\n",
    "https://arxiv.org/abs/1806.08342\n",
    "\n",
    "*__Per-channel quantization of weights and per-layer quantization of activations to 8-bits of precision__ post-training produces classification accuracies within 2% of floating point networks for a wide variety of CNN architectures. __Model sizes can be reduced by a factor of 4 by quantizing weights to 8-bits__, even when 8-bit arithmetic is not supported...Observe a speedup of 2x-3x for quantized implementations compared to floating point on CPUs*\n",
    "\n",
    "\n",
    "Reduces memory and communication requirements of the network. Multiplication complexity scales to the square of the number of bits, while addition scales linearly with the number of bits.\n",
    "\n",
    "32 bit float, 16 bit float, 8 bit fixed (common for CNN inference), and 4 bit fixed\n",
    "\n",
    "### __Sparsification__\n",
    "\n",
    "__Sparsifiaton__ Reduces memory and the required number of computations. Random sparsity can be encouraged with L1 regularization as well as thresholding dense coefficient values.\n",
    "\n",
    "Network pruning methods exist. Using second oreder derivatives of parameters wrt error function\n",
    "\n",
    "https://openreview.net/pdf?id=Sy1iIDkPM\n",
    "\n",
    "### Appropriate Sizing\n",
    "Match the network size to the available memory on device.\n",
    "\n",
    "### Train Test Constraints\n",
    "\n",
    "__Train__\n",
    "- Can batch inputs\n",
    "- Need extra batch norm computations (moving average compute)\n",
    "- Need error calculation and extra memory for backprop\n",
    "- Need higher precision floating point\n",
    "\n",
    "__Test__\n",
    "- Absorb batch norm op into convolution\n",
    "- Need network output\n",
    "- Less memory space required since no backprop\n",
    "- OK to use lower precision in fixed points"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XNN Software\n",
    "\n",
    "Software maps the XNN design to hardware. Does graph specification, lowering and execution. User specifies the network graph design and software automatically creates a reverse graph to propagate errors to weights.\n",
    "\n",
    "### __CNN Graph Lowering__\n",
    "\n",
    "• Domain agnostic hardware agnostic optimization\n",
    "    - Remove unneeded edges and nodes required for specified input output\n",
    "    - Constant folding and constant propagation\n",
    "    \n",
    "• Domain specific hardware agnostic optimization\n",
    "    - xNNs: remove dropout and scale associated weight layers\n",
    "    - xNNs: absorb batch norm into convolution and create a bias term\n",
    "\n",
    "• Domain specific hardware aware optimization\n",
    "    - xNNs: transform data layouts (tensor ordering)\n",
    "    - xNNs: node fusion, tiling and grouping\n",
    "    - xNNs: post training quantization\n",
    "\n",
    "• Domain agnostic hardware specific code generation\n",
    "    - Memory planning for all tensors\n",
    "    - Data movement and compute strategy selection for each node\n",
    "    - Code generation for selected strategy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### __Graph Execution__\n",
    "role of the software runtime. Ties addresses for dynamic tensors into the graph.\n",
    "\n",
    "During execution cycle through the nodes, sing the appropriate communication and computation primitives."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XNN Hardware\n",
    "\n",
    "__Dennard Scaling__ Transistor power is no longer proportional to area. More transistors increases power consumption increases heat...Now we need good architecture designs to advance performance,\n",
    "\n",
    "__Dark Silicon__ only a fraction of the transistors on a chip can be activated at one time (or they will melt) -> design accelerators to be really efficient.\n",
    "\n",
    "__Dark Memory__ only a fraction of DRAM can be active at one time -> maximize data locality to minimize data movement and boost performance\n",
    "\n",
    "\n",
    "\n",
    "What consumes power (most to least)\n",
    "- Physical movement (motors)\n",
    "- Long distance communication\n",
    "- off device comm.\n",
    "- on device comm.\n",
    "- computation\n",
    "\n",
    "__How to design optimal hardware:__ \n",
    "- minimize off dev comm. -> inlude a lot of on dev memory\n",
    "- minimize on dev movememnt -> data locality\n",
    "- optimize comptute -> match accelerators to algorithm\n",
    "\n",
    "### Domain Specific Architecture (DSA)\n",
    " \n",
    "DSAs are optimized for a specific application. Includes memory, control, communication and compute. Can vastly improve compute performance.\n",
    "\n",
    "__DSAs are different than CPUs__ \n",
    "\n",
    "They are designed to optimize throughput, Domain optimized communication, comput, and mem.\n",
    "\n",
    "__Primitive defined domains__ \n",
    "\n",
    "Define the domian in terms of fundamental math, not an application\n",
    "\n",
    "#### __Memory__ \n",
    "Need enough on device memory such that either all feature maps or all coefficients fit on device.\n",
    "\n",
    " If all feature maps can be loaded to on device memory you can perform the matrix multiply by grabbing the coefficients by rows and performin the op. (likewise for if coefficients can be stored on-device)\n",
    " \n",
    " If neither fit entirely in memory, than the chip has to load both in groups (row/col) creating a small section of the output at one time. Will also require one of either coeefs or filters to be loaded on to-device more than once. (double loop situation)\n",
    " \n",
    "__Design the memory to have enough to store feature maps fully on device so only weights need to be moved.__ As you progress through the network, grab weights for each layer.\n",
    "\n",
    "<img src=\"../img/on-dev-mem.PNG\">\n",
    "\n",
    "\n",
    "#### Control\n",
    "\n",
    "Control cycles through the nodes on the low-level graph. Gives instructions to the appropriate primitive. Typucally a general compute resource that executes all non primitive supported nodes\n",
    "\n",
    "#### Communication Strategy\n",
    "\n",
    "Ping-pong buffers in DRAM and local mem to allow concurrent compute and communication. \n",
    "\n",
    "Performs the following in parallel:\n",
    "- external <-> local\n",
    "- local <-> computer\n",
    "- Computation\n",
    "\n",
    "__External bandwidth is much less than internal bandwidth__ -> need high data reuse or keep some data on device at all times.\n",
    "\n",
    "<img src=\"../img/communication.PNG\" width=\"300\">\n",
    "\n",
    "\n",
    "__Communication Primitive__\n",
    "\n",
    "A transform primitive with a communication framework. \n",
    "- Gather: vector read on-dev/DRAM\n",
    "- Transform: compress/decompress\n",
    "- Transform: encrypt/decrypt\n",
    "- Scatter: vector write on-mem/DRAM\n",
    "\n",
    "#### Computation Strategy\n",
    "\n",
    "Create reaaly efficient primitives for operations that are done alot (max, min, sort, rank, pool, find, median) Everything else performed on a general compute. This allows new  operations.\n",
    "\n",
    "Use a matrix comp primitice for (conv, fft)\n",
    "\n",
    "Reads and writes are adress aligned to maximize bandwidth efficiency. Pre/post-processing formats data to to transform a comatibale problem to matrix matrix multiplication. Ping-pong registers based on matrix mult method\n",
    "\n",
    "\n",
    "__Multiply Matrices w < $N^3$ MACs__\n",
    "\n",
    "Multiply costs more than addition.-> create intermediary addition terms that substtute multiplication\n",
    "\n",
    "__Strassen's Algorithm__\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matr_A = [[1,2],[3,4]]\n",
    "matr_B = [[5,6],[7,8]]\n",
    "\n",
    "def matmul(matrix_A, matrix_B):\n",
    "    assert(len(matrix_A[0])==len(matrix_B))\n",
    "    out_height = len(matrix_A)\n",
    "    interm_dim = len(matrix_A[0])\n",
    "    out_width = len(matrix_B[0])\n",
    "    \n",
    "    matrix_C = [[0 for j in range(out_width)] for i in range(out_height)]\n",
    "    for h in range(out_height):\n",
    "        for w in range(out_width):\n",
    "            c = 0\n",
    "            for k from range(interm_dim):\n",
    "                c += matrix_A[h][k]*matrix_B[k][w]\n",
    "            C[h][w] = c\n",
    "    return C\n",
    "            \n",
    "\n",
    "def strassens_matmul(matrix_A, matrix_B):\n",
    "    assert(len(matrix_A[0])==len(matrix_B))\n",
    "    out_height = len(matrix_A)\n",
    "    interm_dim = len(matrix_A[0])\n",
    "    out_width = len(matrix_B[0])\n",
    "    \n",
    "    matrix_C = [[0 for j in range(out_width)] for i in range(out_height)]\n",
    "    \n",
    "    S_1 = "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XNN Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:deep-learning]",
   "language": "python",
   "name": "conda-env-deep-learning-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
