{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Sources\" data-toc-modified-id=\"Sources-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Sources</a></span></li><li><span><a href=\"#Speech-and-Audio\" data-toc-modified-id=\"Speech-and-Audio-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Speech and Audio</a></span><ul class=\"toc-item\"><li><span><a href=\"#Pre-Processing\" data-toc-modified-id=\"Pre-Processing-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>Pre Processing</a></span><ul class=\"toc-item\"><li><span><a href=\"#Time-Domain-Waveform\" data-toc-modified-id=\"Time-Domain-Waveform-2.1.1\"><span class=\"toc-item-num\">2.1.1&nbsp;&nbsp;</span>Time Domain Waveform</a></span></li><li><span><a href=\"#Windowing-and-Spectrogram\" data-toc-modified-id=\"Windowing-and-Spectrogram-2.1.2\"><span class=\"toc-item-num\">2.1.2&nbsp;&nbsp;</span>Windowing and Spectrogram</a></span></li><li><span><a href=\"#MFCC\" data-toc-modified-id=\"MFCC-2.1.3\"><span class=\"toc-item-num\">2.1.3&nbsp;&nbsp;</span>MFCC</a></span></li></ul></li><li><span><a href=\"#Network-Structures\" data-toc-modified-id=\"Network-Structures-2.2\"><span class=\"toc-item-num\">2.2&nbsp;&nbsp;</span>Network Structures</a></span></li></ul></li><li><span><a href=\"#Speaker-Identification\" data-toc-modified-id=\"Speaker-Identification-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Speaker Identification</a></span><ul class=\"toc-item\"><li><span><a href=\"#Imprinting-Strategy\" data-toc-modified-id=\"Imprinting-Strategy-3.1\"><span class=\"toc-item-num\">3.1&nbsp;&nbsp;</span>Imprinting Strategy</a></span></li></ul></li><li><span><a href=\"#Keyword-Spotting\" data-toc-modified-id=\"Keyword-Spotting-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Keyword Spotting</a></span><ul class=\"toc-item\"><li><span><a href=\"#Wake-Up\" data-toc-modified-id=\"Wake-Up-4.1\"><span class=\"toc-item-num\">4.1&nbsp;&nbsp;</span>Wake Up</a></span></li><li><span><a href=\"#Strategy\" data-toc-modified-id=\"Strategy-4.2\"><span class=\"toc-item-num\">4.2&nbsp;&nbsp;</span>Strategy</a></span></li></ul></li><li><span><a href=\"#Conditional-Modeling\" data-toc-modified-id=\"Conditional-Modeling-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>Conditional Modeling</a></span><ul class=\"toc-item\"><li><span><a href=\"#For-Language-Translation\" data-toc-modified-id=\"For-Language-Translation-5.1\"><span class=\"toc-item-num\">5.1&nbsp;&nbsp;</span>For Language Translation</a></span></li><li><span><a href=\"#For-Speech-to-Text\" data-toc-modified-id=\"For-Speech-to-Text-5.2\"><span class=\"toc-item-num\">5.2&nbsp;&nbsp;</span>For Speech to Text</a></span></li><li><span><a href=\"#For-Text-to-Audio\" data-toc-modified-id=\"For-Text-to-Audio-5.3\"><span class=\"toc-item-num\">5.3&nbsp;&nbsp;</span>For Text to Audio</a></span></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sources\n",
    "\n",
    "- [slides](file:///C:/Users/harri/Desktop/CNN%20Slides/xNNs_100_Speech.pdf)\n",
    "- [A Comparison of Sequence-to-Sequence Models for Speech Recognition](https://www.isca-speech.org/archive/Interspeech_2017/pdfs/0233.PDF)\n",
    "- [First-Pass Large Vocabulary Continuous Speech Recognition](https://arxiv.org/abs/1408.2873)\n",
    "- [An All-Neural On-Device Speech Recognizer](https://ai.googleblog.com/2019/03/an-all-neural-on-device-speech.html)\n",
    "- [Streaming End-to-end Speech Recognition For Mobile Devices](https://arxiv.org/abs/1811.06621)\n",
    "- [The fall of RNN / LSTM](https://towardsdatascience.com/the-fall-of-rnn-lstm-2d1594c74ce0)\n",
    "- [When Recurrent Models Don't Need to be Recurrent](https://bair.berkeley.edu/blog/2018/08/06/recurrent/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Speech and Audio\n",
    "\n",
    "\n",
    "## Pre Processing\n",
    "\n",
    "### Time Domain Waveform\n",
    "\n",
    "Microphone converts soundwaves to continuouse time voltages. Sampled by an ADC to create discrete time-domain samples. (Produces bits per sample at a particular rate.)\n",
    "\n",
    "Need to sample a twice the rate of the input frequency for no loss of information.\n",
    "\n",
    "__Result is a waveform with time and Amplitude dimension.__\n",
    "\n",
    "### Windowing and Spectrogram\n",
    "\n",
    "Convert audio waveform to the frequency domain. __Discrete Fourier Transform (DFT)__ would lose temporal information, so input is blocked into 20-25 ms intervals w 10ms overlap. DFT of each frame is taken, magnitude is used to create spectrogram.\n",
    "\n",
    "### MFCC\n",
    "\n",
    "Places emphasis on lower range of the frequency domain."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network Structures\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Speaker Identification\n",
    "\n",
    "Identify the speaker from an audio clip at a given time (where multiple speakers are contained within the clip) Has variance in the method of recording, background noise, compression,...\n",
    "\n",
    "## Imprinting Strategy\n",
    "\n",
    "Using large database, use NN to map from speech recording to spearker (CNN,rnn,attention)\n",
    "\n",
    "Add a new voice, using pretrained model map training speech to final feature vec. Average all feature representations together to get an embedding for the new user's voice. Add vector to the linear mapping to create a new speaker class.\n",
    "\n",
    "Similar to strategy in FaceNet paper. Want voice embeddings to be distant from each other, so new data can be easily distinguished. \n",
    "\n",
    "When new data comes in for inference, use NN to get output feature embedding, then use cosine similarity to compare embedding with embeddings of stored users. Find the most similar user, and suggest that user.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keyword Spotting\n",
    "## Wake Up\n",
    "\n",
    "Wakeup process saves power for low resource settings. Start with a simple task to determine whether more complex evaluation is needed (was Alexa uttered $\\rightarrow$ higher functions) \n",
    "\n",
    "- Sound Detection\n",
    "- Keyword Detection\n",
    "- Speech recognition\n",
    "\n",
    "All yes/no classification problems\n",
    "\n",
    "## Strategy\n",
    "\n",
    "Build a XNN classifier w raw audio inputs. Map to [keyword, not keyword] or [vocab_1, vocab_2,..., vocab_n]\n",
    "\n",
    "How to combine input data scross time and frequency, imbalanced training data, low latency for user experience, low complexity for resource usage.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conditional Modeling\n",
    "\n",
    "__Model__ Assign probabilities to a sequence of elements $y_i$. The key is next element prediction given previous elements.\n",
    "\n",
    "$$P(y_n|y_{n-1},...,y_1)$$\n",
    "\n",
    "Create a model to learn the conditional probability distribution.\n",
    "\n",
    "Conditioning on input data will make output predictions more spikey. Reduces the entropy of the conditional PMF, and requires labelled input/output pairs for training.\n",
    "\n",
    "$$P(y_n|y_{n-1},...,y_1, x_n)$$\n",
    "\n",
    "During testing previous true outputs are estimated with model predictions. Use beam search to reduce error feedback.\n",
    "\n",
    "$$P(y_n|\\hat{y_{n-1}},...,\\hat{y_1}, x_n)$$\n",
    "\n",
    "## For Language Translation\n",
    "\n",
    "Encoder-decoder LSTM architectures. Train a model 2 to predict next word in language 2. Retrain model 2 to condition on features generaed from language 1\n",
    "\n",
    "## For Speech to Text\n",
    "\n",
    "Learn model for text that can predict next phoneme/grapheme/word peice/ word given previous. Then bias predictions by conditioning on feature vectors from speech network.\n",
    "\n",
    "## For Text to Audio\n",
    "\n",
    "Learn model to predict next audio sample from previous audio samples. Then bias w features from speech.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "center"
   },
   "source": [
    "# Speech to Text\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:deep-learning]",
   "language": "python",
   "name": "conda-env-deep-learning-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
