{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Implementation-Notes\" data-toc-modified-id=\"Implementation-Notes-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Implementation Notes</a></span><ul class=\"toc-item\"><li><span><a href=\"#Hardware-and-Software\" data-toc-modified-id=\"Hardware-and-Software-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>Hardware and Software</a></span><ul class=\"toc-item\"><li><span><a href=\"#Graph-Specification\" data-toc-modified-id=\"Graph-Specification-1.1.1\"><span class=\"toc-item-num\">1.1.1&nbsp;&nbsp;</span><strong>Graph Specification</strong></a></span></li><li><span><a href=\"#Graph-Lowering\" data-toc-modified-id=\"Graph-Lowering-1.1.2\"><span class=\"toc-item-num\">1.1.2&nbsp;&nbsp;</span><strong>Graph Lowering</strong></a></span></li><li><span><a href=\"#Graph-Execution\" data-toc-modified-id=\"Graph-Execution-1.1.3\"><span class=\"toc-item-num\">1.1.3&nbsp;&nbsp;</span><strong>Graph Execution</strong></a></span></li></ul></li><li><span><a href=\"#Optimized-XNN-Design\" data-toc-modified-id=\"Optimized-XNN-Design-1.2\"><span class=\"toc-item-num\">1.2&nbsp;&nbsp;</span>Optimized XNN Design</a></span><ul class=\"toc-item\"><li><span><a href=\"#Quantization\" data-toc-modified-id=\"Quantization-1.2.1\"><span class=\"toc-item-num\">1.2.1&nbsp;&nbsp;</span><strong>Quantization</strong></a></span></li><li><span><a href=\"#Sparsification\" data-toc-modified-id=\"Sparsification-1.2.2\"><span class=\"toc-item-num\">1.2.2&nbsp;&nbsp;</span><strong>Sparsification</strong></a></span></li><li><span><a href=\"#Appropriate-Sizing\" data-toc-modified-id=\"Appropriate-Sizing-1.2.3\"><span class=\"toc-item-num\">1.2.3&nbsp;&nbsp;</span>Appropriate Sizing</a></span></li><li><span><a href=\"#Train-Test-Constraints\" data-toc-modified-id=\"Train-Test-Constraints-1.2.4\"><span class=\"toc-item-num\">1.2.4&nbsp;&nbsp;</span>Train Test Constraints</a></span></li></ul></li><li><span><a href=\"#XNN-Software\" data-toc-modified-id=\"XNN-Software-1.3\"><span class=\"toc-item-num\">1.3&nbsp;&nbsp;</span>XNN Software</a></span><ul class=\"toc-item\"><li><span><a href=\"#CNN-Graph-Lowering\" data-toc-modified-id=\"CNN-Graph-Lowering-1.3.1\"><span class=\"toc-item-num\">1.3.1&nbsp;&nbsp;</span><strong>CNN Graph Lowering</strong></a></span></li><li><span><a href=\"#Graph-Execution\" data-toc-modified-id=\"Graph-Execution-1.3.2\"><span class=\"toc-item-num\">1.3.2&nbsp;&nbsp;</span><strong>Graph Execution</strong></a></span></li></ul></li><li><span><a href=\"#XNN-Hardware\" data-toc-modified-id=\"XNN-Hardware-1.4\"><span class=\"toc-item-num\">1.4&nbsp;&nbsp;</span>XNN Hardware</a></span><ul class=\"toc-item\"><li><span><a href=\"#Domain-Specific-Architecture-(DSA)\" data-toc-modified-id=\"Domain-Specific-Architecture-(DSA)-1.4.1\"><span class=\"toc-item-num\">1.4.1&nbsp;&nbsp;</span>Domain Specific Architecture (DSA)</a></span><ul class=\"toc-item\"><li><span><a href=\"#Memory\" data-toc-modified-id=\"Memory-1.4.1.1\"><span class=\"toc-item-num\">1.4.1.1&nbsp;&nbsp;</span><strong>Memory</strong></a></span></li><li><span><a href=\"#Control\" data-toc-modified-id=\"Control-1.4.1.2\"><span class=\"toc-item-num\">1.4.1.2&nbsp;&nbsp;</span>Control</a></span></li><li><span><a href=\"#Communication-Strategy\" data-toc-modified-id=\"Communication-Strategy-1.4.1.3\"><span class=\"toc-item-num\">1.4.1.3&nbsp;&nbsp;</span>Communication Strategy</a></span></li><li><span><a href=\"#Computation-Strategy\" data-toc-modified-id=\"Computation-Strategy-1.4.1.4\"><span class=\"toc-item-num\">1.4.1.4&nbsp;&nbsp;</span>Computation Strategy</a></span></li></ul></li></ul></li><li><span><a href=\"#XNN-Performance\" data-toc-modified-id=\"XNN-Performance-1.5\"><span class=\"toc-item-num\">1.5&nbsp;&nbsp;</span>XNN Performance</a></span><ul class=\"toc-item\"><li><span><a href=\"#Backup-General\" data-toc-modified-id=\"Backup-General-1.5.1\"><span class=\"toc-item-num\">1.5.1&nbsp;&nbsp;</span>Backup-General</a></span></li><li><span><a href=\"#Items-that-can-be-quantized\" data-toc-modified-id=\"Items-that-can-be-quantized-1.5.2\"><span class=\"toc-item-num\">1.5.2&nbsp;&nbsp;</span>Items that can be quantized</a></span></li><li><span><a href=\"#Qunitzation-Ops-and-Process\" data-toc-modified-id=\"Qunitzation-Ops-and-Process-1.5.3\"><span class=\"toc-item-num\">1.5.3&nbsp;&nbsp;</span>Qunitzation Ops and Process</a></span></li></ul></li><li><span><a href=\"#XNN-Graph-Specification\" data-toc-modified-id=\"XNN-Graph-Specification-1.6\"><span class=\"toc-item-num\">1.6&nbsp;&nbsp;</span>XNN Graph Specification</a></span></li><li><span><a href=\"#DSA-Components\" data-toc-modified-id=\"DSA-Components-1.7\"><span class=\"toc-item-num\">1.7&nbsp;&nbsp;</span>DSA Components</a></span><ul class=\"toc-item\"><li><span><a href=\"#CPU\" data-toc-modified-id=\"CPU-1.7.1\"><span class=\"toc-item-num\">1.7.1&nbsp;&nbsp;</span>CPU</a></span></li><li><span><a href=\"#GPU\" data-toc-modified-id=\"GPU-1.7.2\"><span class=\"toc-item-num\">1.7.2&nbsp;&nbsp;</span>GPU</a></span></li><li><span><a href=\"#External-Memory\" data-toc-modified-id=\"External-Memory-1.7.3\"><span class=\"toc-item-num\">1.7.3&nbsp;&nbsp;</span>External Memory</a></span></li><li><span><a href=\"#Internal-Memory\" data-toc-modified-id=\"Internal-Memory-1.7.4\"><span class=\"toc-item-num\">1.7.4&nbsp;&nbsp;</span>Internal Memory</a></span></li></ul></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-06T17:13:18.204626Z",
     "start_time": "2020-04-06T17:13:13.073207Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.1.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow_datasets as tfds\n",
    "import tensorflow as     tf\n",
    "import math\n",
    "import numpy             as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.layers import *\n",
    "import pathlib\n",
    "\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[slides](https://github.com/arthurredfern/UT-Dallas-CS-6301-CNNs/blob/master/Lectures/xNNs_070_Implementation.pdf)\n",
    "\n",
    "[disk](file:///F:/Data/xNNs_070_Implementation.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementation Notes\n",
    "\n",
    "## Hardware and Software\n",
    "\n",
    "Software can be more general (hardware agnostic) that leans on runtime intelligent hardware to optimize the perforance of the program duting execution. \n",
    "\n",
    "Software can rely on compile time intelligent hardware. \n",
    "\n",
    "### __Graph Specification__ \n",
    "Is a representation of a high-level hardware agnostic computer algorithm. nodes are operators. Implicit data and instruction movement. Edges prepresent dependenies (memory)\n",
    "\n",
    "### __Graph Lowering__ \n",
    "Map a high level program to a low-level, hardware specific graph. Includes compute and communication nodes, as well as data and instruction edges. Nodes map 1 to 1 to hardware.\n",
    "\n",
    "This is an iterative optimization process that can be improved with knowledge of domain and hardware.\n",
    "\n",
    "### __Graph Execution__ \n",
    "Software runtime runs on a control processor and cycles through nodes on the low level graph. Hardware execures the nodes with computation and communication primitives as well as general compute for everything else."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimized XNN Design\n",
    "\n",
    "Some hardware aware design considerations:\n",
    "- Operator selection: quantization, sparisification, and compression\n",
    "- Network sizing: depth, width, input size\n",
    "\n",
    "### __Quantization__ \n",
    "__Quantization__ reduces the number of bits required to represent feature maps and parameters. \n",
    "\n",
    "Quantizing deep convolutional networks for efficient inference: A whitepaper\n",
    "https://arxiv.org/abs/1806.08342\n",
    "\n",
    "*__Per-channel quantization of weights and per-layer quantization of activations to 8-bits of precision__ post-training produces classification accuracies within 2% of floating point networks for a wide variety of CNN architectures. __Model sizes can be reduced by a factor of 4 by quantizing weights to 8-bits__, even when 8-bit arithmetic is not supported...Observe a speedup of 2x-3x for quantized implementations compared to floating point on CPUs*\n",
    "\n",
    "\n",
    "Reduces memory and communication requirements of the network. Multiplication complexity scales to the square of the number of bits, while addition scales linearly with the number of bits.\n",
    "\n",
    "32 bit float, 16 bit float, 8 bit fixed (common for CNN inference), and 4 bit fixed\n",
    "\n",
    "### __Sparsification__\n",
    "\n",
    "__Sparsifiaton__ Reduces memory and the required number of computations. Random sparsity can be encouraged with L1 regularization as well as thresholding dense coefficient values.\n",
    "\n",
    "Network pruning methods exist. Using second oreder derivatives of parameters wrt error function\n",
    "\n",
    "https://openreview.net/pdf?id=Sy1iIDkPM\n",
    "\n",
    "### Appropriate Sizing\n",
    "Match the network size to the available memory on device.\n",
    "\n",
    "### Train Test Constraints\n",
    "\n",
    "__Train__\n",
    "- Can batch inputs\n",
    "- Need extra batch norm computations (moving average compute)\n",
    "- Need error calculation and extra memory for backprop\n",
    "- Need higher precision floating point\n",
    "\n",
    "__Test__\n",
    "- Absorb batch norm op into convolution\n",
    "- Need network output\n",
    "- Less memory space required since no backprop\n",
    "- OK to use lower precision in fixed points"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XNN Software\n",
    "\n",
    "Software maps the XNN design to hardware. Does graph specification, lowering and execution. User specifies the network graph design and software automatically creates a reverse graph to propagate errors to weights.\n",
    "\n",
    "### __CNN Graph Lowering__\n",
    "\n",
    "• Domain agnostic hardware agnostic optimization\n",
    "    - Remove unneeded edges and nodes required for specified input output\n",
    "    - Constant folding and constant propagation\n",
    "    \n",
    "• Domain specific hardware agnostic optimization\n",
    "    - xNNs: remove dropout and scale associated weight layers\n",
    "    - xNNs: absorb batch norm into convolution and create a bias term\n",
    "\n",
    "• Domain specific hardware aware optimization\n",
    "    - xNNs: transform data layouts (tensor ordering)\n",
    "    - xNNs: node fusion, tiling and grouping\n",
    "    - xNNs: post training quantization\n",
    "\n",
    "• Domain agnostic hardware specific code generation\n",
    "    - Memory planning for all tensors\n",
    "    - Data movement and compute strategy selection for each node\n",
    "    - Code generation for selected strategy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### __Graph Execution__\n",
    "role of the software runtime. Ties addresses for dynamic tensors into the graph.\n",
    "\n",
    "During execution cycle through the nodes, sing the appropriate communication and computation primitives."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XNN Hardware\n",
    "\n",
    "__Dennard Scaling__ Transistor power is no longer proportional to area. More transistors increases power consumption increases heat...Now we need good architecture designs to advance performance,\n",
    "\n",
    "__Dark Silicon__ only a fraction of the transistors on a chip can be activated at one time (or they will melt) -> design accelerators to be really efficient.\n",
    "\n",
    "__Dark Memory__ only a fraction of DRAM can be active at one time -> maximize data locality to minimize data movement and boost performance\n",
    "\n",
    "\n",
    "\n",
    "What consumes power (most to least)\n",
    "- Physical movement (motors)\n",
    "- Long distance communication\n",
    "- off device comm.\n",
    "- on device comm.\n",
    "- computation\n",
    "\n",
    "__How to design optimal hardware:__ \n",
    "- minimize off dev comm. -> inlude a lot of on dev memory\n",
    "- minimize on dev movememnt -> data locality\n",
    "- optimize comptute -> match accelerators to algorithm\n",
    "\n",
    "### Domain Specific Architecture (DSA)\n",
    " \n",
    "DSAs are optimized for a specific application. Includes memory, control, communication and compute. Can vastly improve compute performance.\n",
    "\n",
    "__DSAs are different than CPUs__ \n",
    "\n",
    "They are designed to optimize throughput, Domain optimized communication, comput, and mem.\n",
    "\n",
    "__Primitive defined domains__ \n",
    "\n",
    "Define the domian in terms of fundamental math, not an application\n",
    "\n",
    "#### __Memory__ \n",
    "Need enough on device memory such that either all feature maps or all coefficients fit on device.\n",
    "\n",
    " If all feature maps can be loaded to on device memory you can perform the matrix multiply by grabbing the coefficients by rows and performin the op. (likewise for if coefficients can be stored on-device)\n",
    " \n",
    " If neither fit entirely in memory, than the chip has to load both in groups (row/col) creating a small section of the output at one time. Will also require one of either coeefs or filters to be loaded on to-device more than once. (double loop situation)\n",
    " \n",
    "__Design the memory to have enough to store feature maps fully on device so only weights need to be moved.__ As you progress through the network, grab weights for each layer.\n",
    "\n",
    "<img src=\"../img/on-dev-mem.PNG\">\n",
    "\n",
    "\n",
    "#### Control\n",
    "\n",
    "Control cycles through the nodes on the low-level graph. Gives instructions to the appropriate primitive. Typucally a general compute resource that executes all non primitive supported nodes\n",
    "\n",
    "#### Communication Strategy\n",
    "\n",
    "Ping-pong buffers in DRAM and local mem to allow concurrent compute and communication. \n",
    "\n",
    "Performs the following in parallel:\n",
    "- external <-> local\n",
    "- local <-> computer\n",
    "- Computation\n",
    "\n",
    "__External bandwidth is much less than internal bandwidth__ -> need high data reuse or keep some data on device at all times.\n",
    "\n",
    "<img src=\"../img/communication.PNG\" width=\"300\">\n",
    "\n",
    "\n",
    "__Communication Primitive__\n",
    "\n",
    "A transform primitive with a communication framework. \n",
    "- Gather: vector read on-dev/DRAM\n",
    "- Transform: compress/decompress\n",
    "- Transform: encrypt/decrypt\n",
    "- Scatter: vector write on-mem/DRAM\n",
    "\n",
    "#### Computation Strategy\n",
    "\n",
    "Create reaaly efficient primitives for operations that are done alot (max, min, sort, rank, pool, find, median) Everything else performed on a general compute. This allows new  operations.\n",
    "\n",
    "Use a matrix comp primitice for (conv, fft)\n",
    "\n",
    "Reads and writes are adress aligned to maximize bandwidth efficiency. Pre/post-processing formats data to to transform a comatibale problem to matrix matrix multiplication. Ping-pong registers based on matrix mult method\n",
    "\n",
    "\n",
    "__Multiply Matrices w < $N^3$ MACs__\n",
    "\n",
    "Multiply costs more than addition.-> create intermediary addition terms that substtute multiplication\n",
    "\n",
    "__Strassen's Algorithm__\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See Homework 7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XNN Performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Performance Prediction__ is estimating software/ hardware implementation performance.\n",
    "\n",
    "1. Choose memory locations (local/exterrnal mem) \n",
    "2. calculate communication time (coeffs, feat maps, out maps)\n",
    "3. Calculate compute time\n",
    "4. Bound total time per layer (comm+compute or max of both if parallel)\n",
    "5. Bound total network time (sum of layer times)\n",
    "\n",
    "\n",
    "\n",
    "### Backup-General\n",
    "\n",
    "__IEEE 754 32-bit float__ - 1 sign bit, 8 bit exponent, 23 bit significand\n",
    "\n",
    "__IEEE 754 16-bit float__ - 1 sign bit, 5 bit exponent, 10 bit significand"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Items that can be quantized\n",
    "\n",
    "All memory elements.\n",
    "- feature maps\n",
    "- filter coefficients\n",
    "- gradients\n",
    "\n",
    "A network might be able to reduce precision, a linear layer is drawing decision boundary in a high-dim space. loss of precision implies a noisy/fuzzy boundary.\n",
    "\n",
    "### Qunitzation Ops and Process\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## XNN Graph Specification\n",
    "\n",
    "\n",
    "## DSA Components\n",
    "\n",
    "### CPU\n",
    "General compute that can do anything. Not great for optimization, but can allow DSA to cope with new methods.\n",
    "\n",
    "### GPU\n",
    "Less optimal for large matrix-matrx operations with static compile time graphs . Convenient for training CNNs.\n",
    "\n",
    "### External Memory\n",
    "__DRAM__\n",
    "off device volatile storage\n",
    "\n",
    "    __Memory Cell__ data is stored as a charge on a capacitor representing a bit (1 transistor per bit)\n",
    "    \n",
    "• Organization\n",
    "• (Banks * rows * columns) x bits\n",
    "• Commonly most efficient with ~ 64 B alignment and\n",
    "multiple of 64 B accesses; specific alignment and access\n",
    "size is a function of the specific memory\n",
    "• This affects data arrangement and memory accesses\n",
    "\n",
    "\n",
    " Occasionally uses of external memory for xNNs\n",
    "• A buffer for intermediate feature maps when they’re too\n",
    "big to fit on device\n",
    "\n",
    "\n",
    "### Internal Memory\n",
    "\n",
    "__SRAM__\n",
    "On-devie vlatile storage data is stored as a flipflop representing a but (4-6 transistors per bit)\n",
    "\n",
    "Organization\n",
    "• Divided into multiple banks where each bank can be\n",
    "thought of as a 2D array of bits / bytes\n",
    "• Access are most efficient that read a row at a time\n",
    "• Applications spread data across multiple banks for\n",
    "multiple simultaneous read / write operations\n",
    "• Either use bank randomization or coordinated\n",
    "memory arrangements to minimize delays caused by\n",
    "multiple simultaneous read / write operations to the\n",
    "same bank\n",
    "\n",
    "Typical uses of internal memory for xNNs\n",
    "• Finite (though frequently occupying a large fraction of\n",
    "an optimal device)\n",
    "• Input and output feature maps for internal graph edges\n",
    "• Filter coefficients for the current layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:deep-learning]",
   "language": "python",
   "name": "conda-env-deep-learning-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
