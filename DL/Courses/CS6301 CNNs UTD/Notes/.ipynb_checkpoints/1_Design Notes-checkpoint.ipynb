{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-01T18:59:07.761090Z",
     "start_time": "2020-04-01T18:59:03.245142Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.1.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import *\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Design\n",
    "\n",
    "[Slides](https://github.com/arthurredfern/UT-Dallas-CS-6301-CNNs/blob/master/Lectures/xNNs_050_Design.pdf)\n",
    "\n",
    "\n",
    "## Keep your goals Simple\n",
    "- The simpler the task, the more training data there is. The more data, the better a NN will perform\n",
    "- Handle complex problems with a shallow combination of simple problems.\n",
    "\n",
    "## CNN Design Overview\n",
    "- CNN exploits spatial structure in data. correlations between loaclized regions of pixels.\n",
    "- __Encoder__ (Tail & Body for feature extraction) + __decoder__ (head for prediction)\n",
    "    - Commonly: Serial, Parallel, Dense, or Residual\n",
    "    \n",
    "- __Tail__- few specialized layers with weaker features and better localization.\n",
    "- __Body__ - layers that define the network. strong features with worse localization.\n",
    "- __Head__ - task specific layers that produce the output of the network\n",
    "\n",
    "\n",
    "### CNN Architecture (Image Classification)\n",
    "\n",
    "Classification network inputs an image and ouputs a vector, whose largest elements are the predicted class.\n",
    "\n",
    "$$TAIL\\rightarrow BODY \\rightarrow HEAD$$\n",
    "\n",
    "$$ENCODER\\rightarrow DECODER$$\n",
    "\n",
    "#### Head\n",
    "\n",
    "The __head__ estimates the dominant object in a specific region by using the feature vector in the corresponding region of the feature maps at the body ouput. \n",
    "- __Global average pooling__ will extract information from each feature map (presence of feature across all areas) and the head will use a linear combination of these global features to predict the dominant object of the image. Global average Pooling or vectorization can be used to convert the output of the body to a 1xN vector for input into the Linear layers of the head.\n",
    "\n",
    "GAP is more popular since it allows arbitrary sized input images and also reduces the computational complexity of the decoding phase.\n",
    "\n",
    "\n",
    "Different head designs can be created to accomplish different goals, and more than one head can be attached to the end of a tail and body.\n",
    "\n",
    "#### Tail and Body\n",
    "\n",
    "Tail and body transform entire regions of the input image to feature vectures across the ouput feature maps. The __receptive field__ is the area of pixels from the input that can be mapped to a feature vector output.\n",
    "\n",
    "Each activation in the output of the body relates the confidence of the presence of a class in a certain area (size receptive field). Averaging over every region leads to the final prediction in the head of the dominant class in the image.\n",
    "\n",
    "\n",
    "##### Tail\n",
    "- High compute, high feature map memory low parameter memory, lots of spatial redundancy.\n",
    "- Aggressive down sampling and aggressive increase in number of channels.\n",
    "\n",
    "Common architectures:\n",
    "- conv (7,7) stride 2, 64 channels -> max pool (3x3) stride 2\n",
    "- conv(3,3) stride 2, 32 channels -> conv (3,3) s=1, c=64 -> conv(3,3) s=2, c=64\n",
    "\n",
    "\n",
    "##### Body\n",
    "Blocks followed by down-sampling. Gradual reduction in memory required for feature maps. Leads to a loss in spatial resolution (pooling throws out pixels) Increase in number of channels\n",
    "\n",
    "Common design practices:\n",
    "- reduce cols and rows by 1/2 and double the number of channels\n",
    "    - data volume shrinks by a factor of 2 (reduces compute by factor of 2)\n",
    "  \n",
    "#### Receptive field size at the head\n",
    "Shows the area of information that the classifier has to work with at inference. May affect images with higher resolution.\n",
    "\n",
    "__Calculation of Receptive Field__\n",
    "1. Start at the output of the body and set r.f.=1\n",
    "2. Move backwards in network.\n",
    "3. Everytime you reach a filter, increase the receptive field size by F-1. (F is length of filter eg 3x3)\n",
    "4. Everytime you reach a down sample multiply the receptive field by S, then subtract by S -1 (S the down sampling factor eg 2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN Architectures\n",
    "\n",
    "## Serial (LeNet, AlexNet, VGG, MobileNet V1)\n",
    "\n",
    "__VGGNet__ introduced stacked 3x3 conv filters. Used vectorization at the head\n",
    "   - Stacking 3x3 convolutions reduces compute while expanding receptive field.\n",
    "    \n",
    "\n",
    "__[MobileNet V1](https://arxiv.org/pdf/1704.04861.pdf)__ uses cascade of 3x3 spatial and 1x1 channel convs. Utilized global avg pool at head.\n",
    "   - __Depthwise-seperable convolutions__. Depthwise convolution has a nxnx1 kernel for each channel of the input. Pointwise convolution has 1x1xc kernels for c channels in input. [depthwise conv explained](https://towardsdatascience.com/a-basic-introduction-to-separable-convolutions-b99ec3102728)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MobileNetV1(inputs, num_filters, strides=(1,1)):\n",
    "    \"\"\"\n",
    "    Conv Block from the MobileNetV1 architecture.\n",
    "    \n",
    "    inputs: Tensor- input to the first layer\n",
    "    num_filters: int - desired number of channels in output\n",
    "    strides: tuple-strides for the first convolution\n",
    "    \"\"\"\n",
    "    x = DepthwiseConv2D((3,3), strides=strides)(inputs)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = ReLU()(x)\n",
    "    \n",
    "    x = Conv2D(num_filters, (1, 1))(x) #pointwise conv\n",
    "    x = BatchNormalization()(x) \n",
    "    x = ReLU()(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parallel (GoogLeNet, Inceptions, SqueezeNet)\n",
    "Input split $\\rightarrow$ parallel ops $\\rightarrow$ output combine. \n",
    "\n",
    "[Inception overview](https://towardsdatascience.com/a-simple-guide-to-the-versions-of-the-inception-network-7fc52b863202)\n",
    "\n",
    "__[Inception V1](https://arxiv.org/pdf/1409.4842v1.pdf)__ - Size of the object may differ across images So build parallel convolution pathways with different receptive fields.\n",
    "\n",
    "Authors limited the compute by adding 1x1 pontwise convolutions (reducing input channels) before each of the conv branches. References the *Network in Network* paper as the source of pointwise conv.\n",
    "\n",
    "Authors also include a Dropout layer after GAP with dropout prob of 0.4.\n",
    "\n",
    "Authors use axillary classifiers, heads inside of the network that predict the output. This attempts to allow clean gradient flow to earler parts of the network.\n",
    "\n",
    "<img src=\"../img/lenet.PNG\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_block(inputs, filters, kernel_size=(3,3), strides=(1,1)):\n",
    "    \"\"\"Generic Conv -> BN -> ReLU abstraction\"\"\"\n",
    "    x = Conv2D(filters, kernel_size, strides=strides)(inputs)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = ReLU()(x)\n",
    "    return x  \n",
    "\n",
    "def InceptionV1(inputs, squeeze_dims, out_dims, strides=(1,1)):\n",
    "    \"\"\"\n",
    "    Inception V1 Conv Block (GoogLeNet).\n",
    "    \"\"\"\n",
    "    x1 = conv_block(inputs, out_dims, kernel_size=(1,1))\n",
    "    \n",
    "    x2 = conv_block(inputs, squeeze_dims, kernel_size=(1,1))\n",
    "    x2 = conv_block(x2, out_dims, kernel_size=(3,3))\n",
    "    \n",
    "    x3 = conv_block(inputs, squeeze_dims, kernel_size=(1,1))\n",
    "    x3 = conv_block(x3, out_dims, kernel_size=(5,5))\n",
    "    \n",
    "    x4 = MaxPool2D(pool_size=(3,3), strides=(1,1))(inputs)\n",
    "    x4 = conv_block(inputs, out_dims, kernel_size=(1,1))\n",
    "    \n",
    "    concat = tf.concat([x1,x2,x3,x4],3)\n",
    "    return concat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__[Inception V2 and V3](https://arxiv.org/pdf/1512.00567.pdf)__\n",
    "\n",
    "__Inception V2__ focused on: \n",
    "- avoiding representational bottlenecks with extreme compression. \n",
    "- converting the 5x5 convolution to two stacked 3x3 convolutions. \n",
    "- Also focuses on reducing compute by factorizing an nxn convolution to an nx1 and 1xn convolutions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def InceptionV2_fig5(inputs, squeeze_dims, out_dims, strides=(1,1)):\n",
    "    \"\"\"\n",
    "    Inception V2 Figure 5\n",
    "    \"\"\"\n",
    "    x1 = conv_block(inputs, squeeze_dims, kernel_size=(1,1))\n",
    "    \n",
    "    x2 = conv_block(inputs, squeeze_dims, kernel_size=(1,1))\n",
    "    x2 = conv_block(x2, out_dims, kernel_size=(3,3))\n",
    "    \n",
    "    x3 = conv_block(inputs, squeeze_dims, kernel_size=(1,1))\n",
    "    x3 = conv_block(x3, out_dims, kernel_size=(3,3))\n",
    "    x3 = conv_block(x3, out_dims, kernel_size=(3,3))\n",
    "    \n",
    "    x4 = MaxPool2D(pool_size=(3,3), strides=(1,1))(inputs)\n",
    "    x4 = conv_block(inputs, squeeze_dims, kernel_size=(1,1))\n",
    "    \n",
    "    concat = tf.concat([x1,x2,x3,x4],3)\n",
    "    return concat\n",
    "\n",
    "def InceptionV2_fig6(inputs, squeeze_dims, out_dims, strides=(1,1)):\n",
    "    \"\"\"\n",
    "    Inception V2 figure 6\n",
    "    \"\"\"\n",
    "    x1 = conv_block(inputs, squeeze_dims, kernel_size=(1,1))\n",
    "    \n",
    "    x2 = conv_block(inputs, squeeze_dims, kernel_size=(1,1))\n",
    "    x2 = conv_block(x2, out_dims, kernel_size=(3,1))\n",
    "    x2 = conv_block(x2, out_dims, kernel_size=(1,3))\n",
    "    \n",
    "    x3 = conv_block(inputs, squeeze_dims, kernel_size=(1,1))\n",
    "    x3 = conv_block(x3, out_dims, kernel_size=(3,1))\n",
    "    x3 = conv_block(x3, out_dims, kernel_size=(1,3))\n",
    "    x3 = conv_block(x3, out_dims, kernel_size=(3,1))\n",
    "    x3 = conv_block(x3, out_dims, kernel_size=(1,3))\n",
    "    \n",
    "    x4 = MaxPool2D(pool_size=(3,3), strides=(1,1))(inputs)\n",
    "    x4 = conv_block(inputs, squeeze_dims, kernel_size=(1,1))\n",
    "    \n",
    "    concat = tf.concat([x1,x2,x3,x4],3)\n",
    "    return concat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Inception V3__ adds RMSProp optimizer, factorized 7x7 convolutions, Batch Norm in auxillary classifiers, label smoothing,\n",
    "\n",
    "__[Inception V4](https://arxiv.org/pdf/1602.07261.pdf)__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dense (DenseNet)\n",
    "Input split into a trasformation path and an identity path. Results are concatenated (Adds width to the netwprk w/ features at different depths) \n",
    "\n",
    "## Residual\n",
    "Instead of concat as in DenseNet perform an add on the two splits.\n",
    "\n",
    "__[ResNetV1](https://arxiv.org/pdf/1512.03385.pdf)__ Add the input of a convolutional block to the results of the blocks convolutions. These \"skip connections\" allow networks to train faster by allowing the clean flow of gradients through the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def ResNetV1(inputs, squeez_dims, expand_dims, strides=(1,1)):\n",
    "    #RESIDUAL PATH\n",
    "    resid = conv_block(resid, squeeze_dims, kernel_size=(3,3), strides=strides)\n",
    "    #IDENTITY PATH\n",
    "    if strides==(2,2):\n",
    "        inputs = Conv2D(expand_dims, (1,1), strides=strides)(inputs)\n",
    "    #COMBINE\n",
    "    return Add()([inputs, resid])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notes for Vision Optimized CNNs\n",
    "- Target a feature size of 6-8x6-8 before GAP\n",
    "- Early body blocks will have a high compute cost, while late blocks will require more memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ResNet Architecture\n",
    "\n",
    "### Sources \n",
    "- [Aggregated Residual Transformations for Deep Neural Networks](https://arxiv.org/pdf/1611.05431.pdf)\n",
    "- [Deep Residual Learning for Image Recognition](https://arxiv.org/pdf/1512.03385.pdf)\n",
    "- [Identity Mappings in Deep Residual Networks](https://arxiv.org/pdf/1603.05027.pdf)\n",
    "\n",
    "### Notes\n",
    "[ResNet V1 Paper](https://arxiv.org/pdf/1512.03385.pdf)\n",
    "\n",
    "__Intuition behind Residual Connenction__\n",
    "\n",
    "*In this paper, we address the degradation problem by introducing a deep residual learning framework.... we let the stacked nonlinear layers fit another mapping of F(x) := H(x)−x. The original mapping is recast into F(x)+x. We hypothesize that it is easier to optimize the residual mapping than to optimize the original, unreferenced mapping. To the extreme, if an identity mapping were optimal, it would be easier to push the residual to zero than to fit an identity mapping by a stack of nonlinear layers.*\n",
    "\n",
    "*With the residual learning reformulation, if identity mappings are optimal, the solvers may simply drive the weights of the multiple nonlinear layers toward zero to approach identity mappings.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__What to do during down sampling__\n",
    "\n",
    "*The identity shortcuts (Eqn.(1)) can be directly used when the input and utput are of the same dimensions (solid line shortcuts in ig. 3). When the dimensions increase (dotted line shortcuts in Fig. 3), we consider two options: (A) The shortcut still performs identity mapping, with extra zero entries padded for increasing dimensions. This option introduces no extra parameter; (B) __The projection shortcut in Eqn.(2) is used to match dimensions (done by 1×1 convolutions)__. For both options, when the shortcuts go across feature maps of two sizes, they are performed with a stride of 2.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*__Deeper Bottleneck Architectures__. Next we describe our\n",
    "deeper nets for ImageNet. Because of concerns on the training time that we can afford, we modify the building block\n",
    "as a bottleneck design4\n",
    ". For each residual function F, we\n",
    "use a stack of 3 layers instead of 2 (Fig. 5). The three layers\n",
    "are 1×1, 3×3, and 1×1 convolutions, where the 1×1 layers\n",
    "are responsible for reducing and then increasing (restoring)\n",
    "dimensions, leaving the 3×3 layer a bottleneck with smaller\n",
    "input/output dimensions. Fig. 5 shows an example, where\n",
    "both designs have similar time complexity.*\n",
    "\n",
    "[Optimizing Residual Block Architecture](https://arxiv.org/pdf/1603.05027.pdf)\n",
    "\n",
    "*(i) The feature xL of any deeper unit L can be represented as the\n",
    "feature xl of any shallower unit l plus a residual function in a form of $\\sum F,$*\n",
    "\n",
    "*The additive term of ∂E∂xL ensures that information isdirectly propagated back to any shallower unit l. Eqn.(5) also suggests that it is unlikely for the gradient ∂E ∂xl to be canceled out for a mini-batch, because in\n",
    "general the term ∂ ∂xl PL−1i=l F cannot be always -1 for all samples in a mini-batch. This implies that the gradient of a layer does not vanish even when the weights are arbitrarily small.*\n",
    "\n",
    "__TLDR__ Signals in both forward and backward pass can be passed directly from any layer to another (in the direction of the process)\n",
    "\n",
    "*We want to make f an identity mapping, which is done by re-arranging\n",
    "the activation functions (ReLU and/or BN). The original Residual Unit in [1]\n",
    "has a shape in Fig. 4(a) — BN is used after each weight layer, and ReLU is\n",
    "adopted after BN except that the last ReLU in a Residual Unit is after elementwise addition (f = ReLU). Fig. 4(b-e) show the alternatives we investigated,\n",
    "explained as following.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ResNext Architecture\n",
    "\n",
    "### Sources\n",
    "- [1](https://arxiv.org/pdf/1611.05431.pdf)\n",
    "\n",
    "\n",
    "*. Our network is constructed\n",
    "by repeating a building block that aggregates a set of transformations with the same topology. Our simple design results in a homogeneous, multi-branch architecture that has\n",
    "only a few hyper-parameters to set. This strategy exposes a\n",
    "new dimension, which we call “cardinality” (the size of the\n",
    "set of transformations), as an essential factor in addition to\n",
    "the dimensions of depth and width*\n",
    "\n",
    "*In this paper, we present a simple architecture which\n",
    "adopts VGG/ResNets’ strategy of repeating layers, while\n",
    "exploiting the split-transform-merge strategy in an easy, extensible way. A module in our network performs a set\n",
    "of transformations, each on a low-dimensional embedding,\n",
    "whose outputs are aggregated by summation. We pursuit a\n",
    "simple realization of this idea — the transformations to be\n",
    "aggregated are all of the same topology (e.g., Fig. 1 (right)).\n",
    "This design allows us to extend to any large number of\n",
    "transformations without specialized designs*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MobileNet V2 Architecture\n",
    "\n",
    "- [1](https://arxiv.org/pdf/1801.04381.pdf)\n",
    "\n",
    "- [2](https://towardsdatascience.com/mobilenetv2-inverted-residuals-and-linear-bottlenecks-8a4362f4ffd5)\n",
    "\n",
    "__Main Contributions__\n",
    "- Depthwise Seperable Convolutions - reduce the computational complexity of a standard 2D Conv by replacing this layer with two layers. The first, a __3x3 depthwise convolution__ applying K kernels with each channel (where K is the number of input channels). Second, a __1x1 pointwise convolution__ to produce linear combinations of channel pixels.\n",
    "\n",
    "- Linear Bottlenecks - TLDR when you are compressing your representation with a convolutional operation, do not use a nonlinearity after the compression. (especially ReLU which throws out negative activations) Instead, make bottleneck layers linear operations.\n",
    "\n",
    "- Inverted Residuals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-27T02:55:28.430847Z",
     "start_time": "2020-02-27T02:55:28.417881Z"
    }
   },
   "outputs": [],
   "source": [
    "def inverted_residual(inputs, expand_channels, squeeze_channels, strides=(1,1)):\n",
    "    \"\"\"\n",
    "    inputs: Tensor- input to the first layer\n",
    "    expand_channels: int - depth of the channel dimension after expansion\n",
    "    squeeze_channels: int-depth of channel dimension after linear bottleneck\n",
    "    strides: tuple-strides for the first convolution\n",
    "    \n",
    "    Inverted residual a la MobileNet V2 note the channel dimension will\n",
    "     be expanded by pointwise conv, processed with depthwise conv, then \n",
    "     compressed by a linear bottleneck\n",
    "    \"\"\"\n",
    "    x = Conv2D(expand_channels, (1, 1), strides=strides, padding='same')(inputs)\n",
    "    x = BatchNormalization(**bn_params)(x)\n",
    "    x = ReLU(max_value=6)(x)  # the paper uses a thresholded ReLU (3-bit output)\n",
    "    \n",
    "    x = DepthwiseConv2D((3,3), strides=(1,1), **conv_params)(x)\n",
    "    x = BatchNormalization(**bn_params)(x)\n",
    "    x = ReLU(max_value=6)(x)\n",
    "    \n",
    "    x = Conv2D(squeeze_channels, (1, 1), strides=(1, 1), padding='same')(x)\n",
    "    x = BatchNormalization(**bn_params)(x) #No activation here (Linear BottleNeck)\n",
    "    \n",
    "    if strides==(2,2): # maintain dimensions during downsampling\n",
    "        inputs = Conv2D(squeeze_channels, (1, 1), strides=strides, padding='same')(inputs)\n",
    "    \n",
    "    return Add()([x, inputs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:deep-learning]",
   "language": "python",
   "name": "conda-env-deep-learning-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
