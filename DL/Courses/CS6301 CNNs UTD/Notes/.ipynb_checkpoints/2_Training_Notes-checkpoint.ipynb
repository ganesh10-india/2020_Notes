{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-10T19:47:54.366766Z",
     "start_time": "2020-03-10T19:47:54.361753Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.1.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "[slides](https://github.com/arthurredfern/UT-Dallas-CS-6301-CNNs/blob/master/Lectures/xNNs_060_Training.pdf)\n",
    "\n",
    "[disk](file:///F:/Data/xNNs_060_Training.pdf)\n",
    "\n",
    "# Forward Pass\n",
    "\n",
    "Modify the forward pass to impove convergence and generalization on inference data.\n",
    "\n",
    "## Convergence\n",
    "\n",
    "### Batch Normalization\n",
    "Normalization done on a per channel basis. During training compute mean and variance for batch.\n",
    "\n",
    "Input:X(n,c,h,w) \n",
    "\n",
    "Output: X(n,c,h,w)\n",
    "\n",
    "<img src=../img/bn.PNG>\n",
    "\n",
    "__During Training__\n",
    "\n",
    "$$\\mu_{c,b}=\\frac{1}{(NHW)}\\sum_{n,h,w}X(n,c,h,w)$$\n",
    "$$\\sigma_{c,b}^2=\\frac{1}{(NHW)}\\sum_{n,h,w}(X(n,c,h,w)-\\mu_{c,b})^2$$\n",
    "\n",
    "Transform data per batch per channel mean and variance and per trainable scale $\\gamma_c$ and trainable bias $\\beta_c$\n",
    "\n",
    "$$Y[:,c:,:]=\\gamma_c \\frac{(X[:,c,:,:]-\\mu_{c,b})}{ \\sigma_{c,b}}+\\beta _c$$\n",
    "\n",
    "__During Inference__ Track running average of mean and variance across batches for use during inference, $\\alpha \\approx 0.99$\n",
    "\n",
    "$$\\mu_c = \\alpha \\mu_c+(1-\\alpha)\\mu_{c,b}$$\n",
    "$$\\sigma_c^2 = \\alpha \\sigma_c^2+(1-\\alpha)\\sigma_{c,b}^2$$\n",
    "\n",
    "During inference, fix the running average parameters and either normalize as in the above formula, or absorb the numbers into convolution paramenters.\n",
    "\n",
    "$$Y[:,c:,:] = \\frac{\\gamma_c }{\\sigma_{c,b}}X[:,c,:,:]+(\\beta_c-\\frac{\\gamma_c\\mu_c }{\\sigma_{c,b}})$$\n",
    "\n",
    "Here multiply the convolution parameters by a scalar $\\frac{\\gamma_c }{\\sigma_{c,b}}$ and add a bias term $\\beta_c-\\frac{\\gamma_c\\mu_c }{\\sigma_{c,b}}$\n",
    "\n",
    "__Why BN Works__ [paper](https://arxiv.org/pdf/1805.11604.pdf)\n",
    "\n",
    "Batch normalization seeks to stabilize the distribution of inputs to a given network layer during training.The batch norm layer sets the first two moments of the distribution of each activation to zero and one respectively, (mean and variance) Then BN inputs are scaled by trainable parameters. BN is applied pre activation.\n",
    "\n",
    "Internal covariate shift - distribution of input to a layer changes due to an update of a parameter of an earlier layer. This change is believed to constantly change the training problem as parameters are updated. Networks with BN have been tested and do not significantly reduce ICS compared to networks wout BN.\n",
    "\n",
    "Batch Norm smooths the loss function. The loss changes at a smaller rate with smaller magnitude of gradients as well. With a smoother loss surface, larger step size can be used wihich leads to faster convergence. \n",
    "\n",
    "\n",
    "\n",
    "__Tips__\n",
    "- Batch normalization doesn't work as well when the batch size is small. This can become an issue when processing high resolution images that stretch your training system's memory limit\n",
    "- the link between batch norm and reducing internal covariate shift is [tenuous](https://arxiv.org/pdf/1805.11604.pdf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Batch_Normalization():\n",
    "    \"\"\"\n",
    "    Implementation of a BatchNormalization for a computational graph.\n",
    "    Input: tensor with shape (B,H,W,C)\n",
    "    OutputL tensor with shape (B,H,W,C) Normalized by channel\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.gamma = None\n",
    "        self.beta = None\n",
    "        self.mu = None\n",
    "        self.sigma = None\n",
    "        self.mu_ra = None\n",
    "        self.sigma_ra = None\n",
    "        self.alpha = 0.99\n",
    "        self.fitted = False\n",
    "        \n",
    "    def forward(self, inputs, training=False):\n",
    "        \"\"\"Note that each input is a batch, and all statistics are\n",
    "        calculated at the channel level.\"\"\"\n",
    "        if not self.fitted:\n",
    "            self.gamma =tf.random.normal(inputs.shape[-1])\n",
    "            self.beta = tf.random.normal(inputs.shape[-1])\n",
    "            self.mu_ra = tf.random.normal(inputs.shape[-1])\n",
    "            self.sigma_ra = tf.random.normal(inputs.shape[-1])\n",
    "            \n",
    "        if training:\n",
    "            #batch statistics\n",
    "            self.mu = tf.reduce_mean(inputs, axis=-1)\n",
    "            self.sigma = tf.reduce_variance(inputs, axis=-1)\n",
    "            #exp moving average\n",
    "            self.mu_ra = self.alpha*self.mu_ra+(1-self.alpha)*self.mu\n",
    "            self.sigma_ra = self.alpha*self.sigma_ra+(1-self.alpha)*self.sigma\n",
    "            \n",
    "            self.scaled = (inputs-self.mu)/tf.math.sqrt(self.sigma+ 1e-5)\n",
    "            result = self.gamma*self.scaled + self.beta\n",
    "            return result\n",
    "        \n",
    "        else: #uses exp. average\n",
    "            result = self.gamma*(inputs-self.mu_ra)/tf.math.sqrt(self.sigma_ra + 1e-5) + self.beta\n",
    "            return result\n",
    "\n",
    "    def backward(self, dz, lr):\n",
    "        #update this with correct mat_mul code\n",
    "        self.gamma += -lr*dz*self.scaled \n",
    "        self.beta += -lr*dz\n",
    "        dz_dx = dz*self.gamma/tf.math.sqrt(self.sigma_ra)\n",
    "        return dz_dx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch Renormalization\n",
    "\n",
    "Starts with batch norm, but gradually transition from using sample mean and variance, to the running avg and variance statistic.\n",
    "\n",
    "- Can help in situations with small batch sizes that might lead to large sample statistics.\n",
    "- Integrates the exp. average during training which could lead to better performance\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Group Normalization\n",
    "\n",
    "Divide channels into groups, compute mean and variance based on groups and channels for each instance.\n",
    "\n",
    "Group normalization creates a new dimension by splitting the channel dimension. This is the axis along which mean and variance are computed (not batch dimension). because the statistics are at the instance level, there is no difference betweeen execution of training and inference.\n",
    "\n",
    "__Instance Norm (C//C)__\n",
    "\n",
    "Normalize each channel of each image. \n",
    "<img src=../img/in.PNG>\n",
    "\n",
    "__Layer Norm (C//1)__\n",
    "Normalize across all channels in an image (every pixel mean/variance)\n",
    "<img src=../img/ln.PNG>\n",
    "\n",
    "__Group Norm(C//variable)__\n",
    "Groups a subset of the channels together for normalization.\n",
    "<img src=../img/gn.PNG>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Group_Normalization():\n",
    "    \"\"\"\n",
    "    Implementation of a Group Normalization for a computational graph.\n",
    "    Input: tensor with shape (B,H,W,C)\n",
    "    OutputL tensor with shape (B,H,W,C) Normalized by channel\n",
    "    \"\"\"\n",
    "    def __init__(self, num_groups):\n",
    "        self.num_groups = num_groups\n",
    "        self.gamma = None\n",
    "        self.beta = None\n",
    "        self.mu = None\n",
    "        self.sigma = None\n",
    "        self.fitted = False\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        if not self.fitted:\n",
    "            self.gamma =tf.random.normal(inputs.shape[-1])\n",
    "            self.beta = tf.random.normal(inputs.shape[-1])\n",
    "\n",
    "        B,H,W,C = inputs.shape\n",
    "        inputs = tf.reshape(inputs, [B, H, W, self.num_groups, C//self.num_groups])\n",
    "        #group statistics\n",
    "        self.mu, self.sigma = tf.nn.moments(inputs, [1,2,4])\n",
    "        #Normalize (add small pos num to prevent div by zero)\n",
    "        self.scaled = (inputs-self.mu)/tf.math.sqrt(self.sigma + 1e-5)\n",
    "        result = self.gamma*tf.reshape(self.scaled,[B,H,W,C]) + self.beta\n",
    "        return result\n",
    "\n",
    "    def backward(self, dz, lr):\n",
    "        #TO DO\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Layer Normalization\n",
    "\n",
    "Normalization without batched, Use the mean and variance statistics as an estimate of the pop mean and variance for the full dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regularization (Generalization)\n",
    "\n",
    "\n",
    "### Stochastic Width\n",
    " Useful in wider networks. \n",
    " \n",
    " __Dropout zeros out a random set of layer outputs per batch.__ Forces multiple groups of output features to be able to estimate a class. (Similar to ensembling, multiple pathways for class prediction within a network) \n",
    "\n",
    "__Dropconnect zeros out a random set of layer weights per batch.__ Forces multiple groups of input features to be able to generate an output feature.\n",
    "\n",
    "### Stochastic Depth (layer skipping)\n",
    "\n",
    "Used in very deep networks. Randomly skips layers where the probability of being skipped increases the deeper a layer is into the network.\n",
    "\n",
    "### Stochastic Branching\n",
    "\n",
    "__ShakeShape and ShakeDrop__ use do regularize residual networks (originally ResNeXt) the idea being to skip a branch in ResNet style block.\n",
    "\n",
    "### Noise Addition\n",
    "\n",
    "Add noise to the network $\\rightarrow$ noisy activation  functions dataset augmentation..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Error Calculation\n",
    "\n",
    "### Classification Loss Function\n",
    "__Softmax- cross entropy__ - uses KL divergence to compare the true probability mass function with the estimated probability mass function (output of the softmax). KL Divergence = cross entropy when  applied to one hot vectors.\n",
    "\n",
    "Other options \n",
    "- KL divergence with label smoothing\n",
    "- noise or overconfidence penalization\n",
    "- optimal transport\n",
    "\n",
    "\n",
    "### Regression Loss Function\n",
    "Generally $l_p$ norms (p=1,2). Another might be __Huber loss__ which is curved like L2 for [-1,1], then straightens out to L1 for larger/smaller values. This circumvents issues with L2's large errors that might cause overcorrection during backprop.\n",
    "\n",
    "\n",
    "### Unequal Class Weightings\n",
    "Can rebalance the importance of specific classes by messing with the error function. Used in cases of class imbalance, or when a certain type of instance is particularly tricky to classify.\n",
    "\n",
    "### Auxillary Network Heads\n",
    "\n",
    "Create extra heads hearlier on in the network that predict the output. Allows gradient to propagate directly to the body of the network. Also encourages earlier features to be stronger\n",
    "\n",
    "### Weight decay (L1/L2 Loss Penalty Term)\n",
    "\n",
    "Add a regularizing term to the error function. For a more detailed explanation [here](https://github.com/harrisonjansma/2020_Notes/blob/master/DL/Courses/CS231n%20Conv%20Nets%20Stanford/1_Neural%20Networks%20Parts%201-2-3.ipynb)\n",
    "\n",
    "[sprecral norm regularization](https://arxiv.org/abs/1705.10941)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Backward Pass\n",
    "\n",
    "## Memory Maintenance\n",
    "\n",
    "### Checkpointing\n",
    "Strategy to address running out of memory. Save every Nth computation result, then during backpropagation, recompute the necessary intermediate values from the saved checkpoints. \n",
    "\n",
    "More compute but allows less memory utilization.\n",
    "\n",
    "[1](https://www-sop.inria.fr/tropics/papers/DauvergneHascoet06.pdf)\n",
    "\n",
    "[2](https://arxiv.org/pdf/1604.06174.pdf)\n",
    "\n",
    "[3](https://arxiv.org/abs/1606.03401)\n",
    "\n",
    "[4](https://github.com/cybertronai/gradient-checkpointing)\n",
    "\n",
    "### In Place Activated Batch Norm\n",
    "\n",
    "Memory optimized BN that utilizes leaky ReLU\n",
    "https://arxiv.org/pdf/1712.02616.pdf\n",
    "\n",
    "### Reversible Architectures\n",
    "\n",
    "Uses large reversible differentiable functions so backpropagation does not need activations to be stored. The network function is \"reversed\" (inverse operations) and gradients are computed with inverse operations.\n",
    "https://arxiv.org/pdf/1707.04585.pdf\n",
    "\n",
    "\n",
    "\n",
    "### Evolving BackPropagation\n",
    "\n",
    "https://arxiv.org/pdf/1804.00746.pdf\n",
    "Augmentation of the Back Prop algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Weight Update\n",
    "\n",
    "## Batch Size\n",
    "\n",
    "Effect of batch size:\n",
    "- smaller batch size $\\rightarrow$ less memory, more gradient noise, less effective batch norm, better result, slower onvergence\n",
    "\n",
    "Good choice for batch size on a single machine is 32.\n",
    "\n",
    "## Weight Update Considerations\n",
    "\n",
    "- Weights: $w$\n",
    "- Error: $e(w)$\n",
    "- Gradient: $g=\\delta e/\\delta w$\n",
    "- Hessian: $H=\\delta^2e/\\delta w$\n",
    "\n",
    "__2nd order approximation of error around a point $w_0$__\n",
    "$neighbor error = error+ 1st order approximate change + 2nd order correction$\n",
    "$e(w) \\approx e(w_0)+(w-w_0)^Tg+0.5(w-w_0)^TH(w-w_0)$\n",
    "\n",
    "__Newton's Method__\n",
    "\n",
    " If $H>0$ (positive definite)\n",
    "- $w \\leftarrow w-H^{-1}g$\n",
    "\n",
    "Note that this formula replaces the scalar learning rate $\\alpha$ with a matrix that determies the appropriate lr for each coefficient.\n",
    "\n",
    "Newtons method is attracted to critical points, and in error spaces with many many dimensions, critical points are most likely to be saddle points. Saddle points tend to have high error, this is one reason why Newton's method has not been commonly used in DL training.\n",
    "\n",
    "\n",
    "__Repeated Transformations in Networks__\n",
    "Transformtions (scalar ops and mms) scale gradients. In systems like RNNs or seqential CNNs, where a transformation is repeatedly applied, the gradient will be repeatedly scaled. (leads to gradient explosion or vanishing gradient) __This is why the scale of transformations should hover around 1__ \n",
    "\n",
    "## Optimizers\n",
    "__SGD__\n",
    "\n",
    "\n",
    "Applied as either true SGD (bath size=1) or mini-batch SGD (btch-size=n). Neurons that fire together are updated together, whle neurons that do not activate strongly on a certain group of instances are not updated.\n",
    "$$g \\leftarrow \\frac{1}{n}\\nabla_{\\theta}\\sum_i L(x_i, y_i,\\theta)$$\n",
    "\n",
    "$$\\theta \\leftarrow \\theta - \\alpha g$$\n",
    "\n",
    "__Nesterov SGD__\n",
    "\n",
    "\n",
    "Adds momentum to the update equation. Implemented with an exponential moving average for gradients. Nesterov momentum applies interim update first, computes gradients at interim points, calculates velocity update, then reverts and applies new gradient update\n",
    "\n",
    "$$\\tilde{\\theta} = \\theta +\\alpha V $$\n",
    "\n",
    "$$g \\leftarrow \\frac{1}{n}\\nabla_{\\tilde{\\theta}}\\sum_i L(x_i, y_i,\\tilde{\\theta})$$\n",
    "$$V\\leftarrow \\alpha V - \\epsilon g$$\n",
    "\n",
    "$$\\theta \\leftarrow \\theta + V$$\n",
    "\n",
    "__AdaGrad__ [paper](http://www.jmlr.org/papers/volume12/duchi11a/duchi11a.pdf)\n",
    "\n",
    "\n",
    "\n",
    "__RMSProp__ [slides](http://www.cs.toronto.edu/~tijmen/csc321/slides/lecture_slides_lec6.pdf)\n",
    "\n",
    "__Adam__ [paper](https://arxiv.org/abs/1412.6980)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Rate Update Schedules\n",
    "\n",
    "Cyclical learning rates and super convergence appl\n",
    "\n",
    "https://arxiv.org/abs/1506.01186\n",
    "\n",
    "https://arxiv.org/abs/1708.07120\n",
    "\n",
    "https://www.fast.ai/2018/04/30/dawnbench-fastai/\n",
    "\n",
    "## Regularization in Optimization \n",
    "\n",
    "__Gradient Noise__\n",
    "\n",
    "Add noise to the weight update to regularize model learning.\n",
    "\n",
    "https://arxiv.org/abs/1511.06485\n",
    "\n",
    "https://arxiv.org/abs/1511.06807\n",
    "\n",
    "\n",
    "__Gradient Clipping__ \n",
    "\n",
    "Curtail exploding/vanishing gradients with gradient clipping. Set a max/min threshold for a gradient value.\n",
    "\n",
    "https://arxiv.org/abs/1211.5063"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parallelization\n",
    "\n",
    "__Synchronous parallelization__\n",
    "\n",
    "Give a part of a lartch batch to each of multiple workers\n",
    "\n",
    "https://arxiv.org/abs/1706.02677\n",
    "\n",
    "https://arxiv.org/abs/1708.03888\n",
    "\n",
    "https://arxiv.org/abs/1709.05011\n",
    "\n",
    "https://arxiv.org/abs/1711.04325\n",
    "\n",
    "https://arxiv.org/abs/1904.00962\n",
    "\n",
    "__Ideal Bath Size for Parallel Training__ \n",
    "\n",
    "Want a huge batch size. Essentially choose a number that gives each of your worker nodes an optimal batch size for a single macchine (~32)\n",
    "\n",
    "__Asynchronous Parallelization__\n",
    "\n",
    "Allow asynchronous updates of gradients to a common parameter server\n",
    "\n",
    "https://storage.googleapis.com/pub-tools-public-publication-data/pdf/40565.pdf\n",
    "\n",
    "https://arxiv.org/abs/1609.08326\n",
    "\n",
    "https://arxiv.org/abs/1711.00489\n",
    "\n",
    "https://arxiv.org/abs/1811.03600"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation\n",
    "\n",
    "__Early Stopping__\n",
    "End training when validation error does no improve for subsequent epochs.\n",
    "https://page.mi.fu-berlin.de/prechelt/Biblio/stop_tricks1997.pdf\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Additional Links\n",
    "\n",
    "• Bag of tricks for image classification with convolutional neural networks\n",
    "• https://arxiv.org/abs/1812.01187\n",
    "• Training tips for the transformer model\n",
    "• https://arxiv.org/abs/1804.00247\n",
    "• Factorization tricks for LSTM networks\n",
    "• https://arxiv.org/abs/1703.10722\n",
    "\n",
    "• AdamW and super-convergence is now the fastest way to train neural nets\n",
    "• https://www.fast.ai/2018/07/02/adam-weight-decay/\n",
    "• Training ImageNet in 3 hours for $25; and CIFAR10 for $0.26\n",
    "• https://www.fast.ai/2018/04/30/dawnbench-fastai/\n",
    "\n",
    "https://arxiv.org/abs/1807.11205\n",
    "\n",
    "https://arxiv.org/abs/1811.06992\n",
    "\n",
    "https://arxiv.org/abs/1805.09501v1\n",
    "\n",
    "https://arxiv.org/abs/1906.11052\n",
    "\n",
    "https://arxiv.org/pdf/1706.06083.pdf\n",
    "\n",
    "• Curriculum learning\n",
    "• https://ronan.collobert.com/pub/matos/2009_curriculum_icml.pdf\n",
    "• Group normalization\n",
    "• https://arxiv.org/abs/1803.08494\n",
    "• Improving neural networks by preventing co-adaptation of feature detectors\n",
    "• https://arxiv.org/abs/1207.0580\n",
    "\n",
    "• Large-margin softmax loss for convolutional neural networks\n",
    "• https://arxiv.org/abs/1612.02295\n",
    "• Distilling the knowledge in a neural network\n",
    "• https://arxiv.org/abs/1503.02531\n",
    "• Effect of depth and width on local minima in deep learning\n",
    "• https://arxiv.org/abs/1811.08150\n",
    "• When does label smoothing help?\n",
    "• https://arxiv.org/abs/1906.02629\n",
    "• The loss surfaces of multilayer networks\n",
    "• https://arxiv.org/abs/1412.0233\n",
    "• Identifying and attacking the saddle point problem in high-dimensional non-convex optimization\n",
    "• https://arxiv.org/abs/1406.2572\n",
    "• Hierarchical loss for classification\n",
    "• https://arxiv.org/abs/1709.01062\n",
    "\n",
    "Automatic differentiation\n",
    "• http://www.robots.ox.ac.uk/~tvg/publications/talks/autodiff.pdf\n",
    "• Reverse-mode automatic differentiation: a tutorial\n",
    "• https://rufflewind.com/2016-12-30/reverse-mode-automatic-differentiation\n",
    "• Automatic differentiation of algorithms\n",
    "• https://www.sciencedirect.com/science/article/pii/S0377042700004222?via%3Dihub\n",
    "• Automatic reverse-mode differentiation: lecture notes\n",
    "• http://www.cs.cmu.edu/~wcohen/10-605/notes/autodiff.pdf\n",
    "• Training neural networks using features replay\n",
    "• https://arxiv.org/abs/1807.04511\n",
    "• Automatic differentiation in ML: Where we are and where we should be going\n",
    "• https://arxiv.org/abs/1810.11530\n",
    "• Why momentum really works\n",
    "• https://distill.pub/2017/momentum/\n",
    "• On the importance of initialization and momentum in deep learning\n",
    "• http://www.cs.toronto.edu/~fritz/absps/momentum.pdf\n",
    "• An overview of gradient descent optimization algorithms\n",
    "• https://arxiv.org/abs/1609.04747\n",
    "• Online learning rate adaptation with hypergradient descent\n",
    "• https://arxiv.org/abs/1703.04782\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:deep-learning]",
   "language": "python",
   "name": "conda-env-deep-learning-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
