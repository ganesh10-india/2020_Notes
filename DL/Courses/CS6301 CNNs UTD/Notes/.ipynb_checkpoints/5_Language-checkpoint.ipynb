{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Language\" data-toc-modified-id=\"Language-1\">Language</a></span><ul class=\"toc-item\"><li><span><a href=\"#Character-and-Word-Embeddings\" data-toc-modified-id=\"Character-and-Word-Embeddings-1.1\">Character and Word Embeddings</a></span><ul class=\"toc-item\"><li><span><a href=\"#Methods-of-Word-Embedding\" data-toc-modified-id=\"Methods-of-Word-Embedding-1.1.1\">Methods of Word Embedding</a></span></li></ul></li><li><span><a href=\"#Language-Models\" data-toc-modified-id=\"Language-Models-1.2\">Language Models</a></span><ul class=\"toc-item\"><li><span><a href=\"#Neural-Language-Models\" data-toc-modified-id=\"Neural-Language-Models-1.2.1\">Neural Language Models</a></span></li></ul></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Language\n",
    "\n",
    "## Character and Word Embeddings\n",
    "\n",
    "__Character embeddings__ assign a vector for each character in a written language.\n",
    "\n",
    "__Word Embeddings__ assign a vector for each word. use dense vectors instead of One Hot Encoding (100-1000 dimension vector) Useful if vector assignment captures semantic meaning of words.\n",
    "\n",
    "__Distributional Hypothesis__ Words that appear in similar contexts have similar meaning. (Judege a word by the company it keeps) Every method that embeds words into a dense representation uses this assumption.\n",
    "\n",
    "### Methods of Word Embedding\n",
    "\n",
    "__SVD Based Word Embeddings__ - Use the contextual neighbors of a word to build a matrix representation of the language.(requires lots of data) Context can be a window of variable size (two neighbors or full document)\n",
    "\n",
    " Forms a matrix $X$ w/ unique words as rows and unique contexts as columns. Each entry is the probability or counts (sometimes smoothed) of a word being in a given context.\n",
    " \n",
    " Next $X$ is decomposed with SVD. \n",
    " $$X = U \\cdot S \\cdot V^T$$\n",
    " \n",
    "Decide the number of rows in $U$ and $V^T$, Each row of $U\\cdot S$ is a word embedding.\n",
    "\n",
    "__Word2Vec__  creates a dense vector representation by predicting context igven word (skip-gram) or word given context (CBOW). Uses a two-layer neural network to learn the representation (two dense laters w/ hierarchical softmax)\n",
    "\n",
    "CBOW (word given context)\n",
    "- input is OHE word vector for each word in context\n",
    "- Embedding (Layer 1) is $NUM_WORDS\\times REPR_LENGTH$ representing the word embeddings\n",
    "- embedding (layer 2) is $REPR_LENGTH \\times NUM_WORDS$ context embeddings\n",
    "\n",
    "Skip Gram (context given word) - Does a better job with infrequent words,\n",
    "slightly worse with small data sets, slightly\n",
    "better with large data sets\n",
    "- input is OHE vector with word\n",
    "\n",
    "Opimization methods: \n",
    "- Hierarchical softmax. (quicker computation for a softmax output with huge numbe of dimensions)\n",
    "- Negative sampling - only update a subset of all words for each sample (Predict is this in context YES/NO)\n",
    "- Sample inversely based on word frequency. (so rarer words can be processed more often)\n",
    "\n",
    "\n",
    "__Glove__  Purpose is to combine statistical benefits of global matrix factorization methods with analogy benefits of context window based methods.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Language Models\n",
    "\n",
    "Predict the Nth word in a N word sequence given the first N-1 words. Estimate the conditional probabilities $P(W_N|W_{N-1},...W_1)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Language Models\n",
    "\n",
    "Start with an embedding layer that seeks to encode each of the first N-1 words. Concatenate the word vectors, and process through multiple intermediate layers, softmax classifier head to predict PMF of Nth word\n",
    "\n",
    "__Standard Neural Netword (Dense layers)__ is akin to N-grams since there is a required, fixed-size input. Must make the context window a parameter in architecture creation.\n",
    "\n",
    "__Recurrent Neural Networks__ process each of the N-1 words, retaining a hidden state vector at each time step. The Final timestep at N-1 is processed with a softmax head to predict the output word"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:deep-learning]",
   "language": "python",
   "name": "conda-env-deep-learning-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
