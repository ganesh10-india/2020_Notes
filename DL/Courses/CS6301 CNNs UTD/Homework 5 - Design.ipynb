{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 5 - Design\n",
    "\n",
    "\n",
    "## Theory\n",
    "\n",
    "1. Using pencil and paper, compute the receptive field size at the input to the global average pooling layer for ResNet 50.\n",
    "\n",
    "__Why use paper when you can use `Python`__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-25T05:33:27.631938Z",
     "start_time": "2020-02-25T05:33:27.615863Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ResNet Receptive Field (wOut bottle neck): 951\n"
     ]
    }
   ],
   "source": [
    "\"\"\" This calculation is for Resnet without bottle neck\n",
    "Each block is 3x3 -> 3x3\n",
    "\"\"\"\n",
    "\n",
    "def add_filter(rf, f):\n",
    "    return rf + f-1\n",
    "\n",
    "def add_pool(rf, S):\n",
    "    return (rf * S) - (S-1)\n",
    "\n",
    "rf = 1\n",
    "for i in range(3):\n",
    "    rf = add_filter(rf,3)\n",
    "    rf = add_filter(rf,3)\n",
    "rf = add_pool(rf, 2)\n",
    "\n",
    "for i in range(6):\n",
    "    rf = add_filter(rf,3)\n",
    "    rf = add_filter(rf,3)\n",
    "rf = add_pool(rf, 2)\n",
    "\n",
    "for i in range(4):\n",
    "    rf = add_filter(rf,3)\n",
    "    rf = add_filter(rf,3)\n",
    "rf = add_pool(rf, 2)\n",
    "\n",
    "for i in range(3):\n",
    "    rf = add_filter(rf,3)\n",
    "    rf = add_filter(rf,3)\n",
    "\n",
    "#TAIL\n",
    "rf = add_pool(rf, 2)\n",
    "rf = add_pool(rf, 2)\n",
    "rf = add_filter(rf,7)\n",
    "print(\"ResNet Receptive Field (wOut bottle neck): {}\".format(rf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-25T05:33:28.049246Z",
     "start_time": "2020-02-25T05:33:28.038348Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ResNet Receptive Field (w bottle neck): 479\n"
     ]
    }
   ],
   "source": [
    "\"\"\" This calculation is for Resnet with bottle neck\n",
    "Each block is 1x1 -> 3x3 -> 1x1\"\"\"\n",
    "\n",
    "rf = 1\n",
    "for i in range(3):\n",
    "    rf = add_filter(rf,3)\n",
    "rf = add_pool(rf, 2)\n",
    "\n",
    "for i in range(6):\n",
    "    rf = add_filter(rf,3)\n",
    "rf = add_pool(rf, 2)\n",
    "\n",
    "for i in range(4):\n",
    "    rf = add_filter(rf,3)\n",
    "rf = add_pool(rf, 2)\n",
    "\n",
    "for i in range(3):\n",
    "    rf = add_filter(rf,3)\n",
    "\n",
    "#TAIL\n",
    "rf = add_pool(rf, 2)\n",
    "rf = add_pool(rf, 2)\n",
    "rf = add_filter(rf,7)\n",
    "print(\"ResNet Receptive Field (w bottle neck): {}\".format(rf))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Practice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data\n",
    "DATA_NUM_CLASSES        = 200\n",
    "DATA_CHANNELS           = 3\n",
    "DATA_ROWS               = 64\n",
    "DATA_COLS               = 64\n",
    "#DATA_CROP_ROWS          = 28\n",
    "#DATA_CROP_COLS          = 28\n",
    "\n",
    "# model\n",
    "MODEL_LEVEL_0_REPEATS   = 3\n",
    "MODEL_LEVEL_1_REPEATS   = 3\n",
    "MODEL_LEVEL_2_REPEATS   = 3\n",
    "\n",
    "# training\n",
    "TRAINING_BATCH_SIZE      = 32\n",
    "TRAINING_SHUFFLE_BUFFER  = 5000\n",
    "TRAINING_LR_MAX          = 0.001\n",
    "# TRAINING_LR_SCALE        = 0.1\n",
    "# TRAINING_LR_EPOCHS       = 2\n",
    "TRAINING_LR_INIT_SCALE   = 0.01\n",
    "TRAINING_LR_INIT_EPOCHS  = 5\n",
    "TRAINING_LR_FINAL_SCALE  = 0.01\n",
    "TRAINING_LR_FINAL_EPOCHS = 25\n",
    "\n",
    "# training (derived)\n",
    "TRAINING_NUM_EPOCHS = TRAINING_LR_INIT_EPOCHS + TRAINING_LR_FINAL_EPOCHS\n",
    "TRAINING_LR_INIT    = TRAINING_LR_MAX*TRAINING_LR_INIT_SCALE\n",
    "TRAINING_LR_FINAL   = TRAINING_LR_MAX*TRAINING_LR_FINAL_SCALE\n",
    "\n",
    "DATA_PATH = 'F://Data/ImageNet/tiny-imagenet-200/'\n",
    "SAVE_MODEL_PATH = pathlib.Path('F://Models/ImageNet_64/1/')\n",
    "#!mkdir -p \"$SAVE_MODEL_PATH\"\n",
    "\n",
    "AUTOTUNE = tf.data.experimental.AUTOTUNE\n",
    "\n",
    "def get_label(file_path):\n",
    "  # convert the path to a list of path components\n",
    "  parts = tf.strings.split(file_path, os.path.sep)\n",
    "  # The second to last is the class-directory\n",
    "  return parts[-2] == CLASSES\n",
    "\n",
    "def decode_image(img):\n",
    "    img = tf.image.decode_image(img, channels=3)\n",
    "    img = tf.image.convert_image_dtype(img, tf.float32)\n",
    "    img = tf.image.random_flip_left_right(img)\n",
    "    #img = tf.image.random_crop(img, size=[DATA_CROP_ROWS, DATA_CROP_COLS, 3])\n",
    "    return img\n",
    "\n",
    "def process_path(path):\n",
    "    \"\"\"\n",
    "    Input: file_path of a sample image\n",
    "    Output: image in 3x64x64 float32 Tensor and one hot tensor\n",
    "    \"\"\"\n",
    "    label = get_label(path)\n",
    "    image = tf.io.read_file(path)\n",
    "    image = decode_image(image)\n",
    "    return image, label\n",
    "\n",
    "\n",
    "def prepare_dataset(data_path, cache=False, shuffle_buffer_size=TRAINING_SHUFFLE_BUFFER):\n",
    "    list_files = tf.data.Dataset.list_files(str(data_path/'*/*'))\n",
    "    #map the above function to file_name dataset\n",
    "    ds = list_files.map(process_path, num_parallel_calls=AUTOTUNE) \n",
    "    if cache:\n",
    "        if isinstance(cache, str):\n",
    "            ds = ds.cache(cache)\n",
    "        else:\n",
    "            ds = ds.cache()\n",
    "    ds = ds.shuffle(buffer_size=shuffle_buffer_size)\n",
    "    ds = ds.repeat()\n",
    "    ds = ds.batch(TRAINING_BATCH_SIZE)\n",
    "    ds = ds.prefetch(buffer_size=AUTOTUNE)\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_root = pathlib.Path(DATA_PATH)\n",
    "CLASSES = os.listdir(data_root/'train')\n",
    "train_ds_cachefile = prepare_dataset(data_root/'train', \n",
    "                                          cache=str(SAVE_MODEL_PATH.parent/\"cache.tfcache\"), \n",
    "                                          shuffle_buffer_size=TRAINING_SHUFFLE_BUFFER)\n",
    "\n",
    "valid_ds_cachefile = prepare_dataset(data_root/'val', \n",
    "                                          cache=str(SAVE_MODEL_PATH.parent/\"valid_cache.tfcache\"), \n",
    "                                          shuffle_buffer_size=TRAINING_SHUFFLE_BUFFER)\n",
    "\n",
    "test_ds_nocache = prepare_dataset(data_root/'test', \n",
    "                                          cache=False, \n",
    "                                          shuffle_buffer_size=TRAINING_SHUFFLE_BUFFER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNet_Block(tf.keras.Model):\n",
    "    def __init__(self, filters):\n",
    "        super(ResNet_Block, self).__init__()\n",
    "        \n",
    "        self.conv1 = tf.keras.layers.Conv2D(filters, (1,1))\n",
    "        self.bn1 = tf.keras.layers.BatchNormalization()\n",
    "        \n",
    "        self.conv2 = tf.keras.layers.Conv2D(filters,(3,3), padding='same')\n",
    "        self.bn2 = tf.keras.layers.BatchNormalization()\n",
    "        \n",
    "        self.conv3 = tf.keras.layers.Conv2D(filters, (1,1))\n",
    "        self.bn3 = tf.keras.layers.BatchNormalization()\n",
    "    \n",
    "    def call(self, x, training=True):\n",
    "        f_x = self.bn1(self.conv1(x), training=training)\n",
    "        f_x = tf.nn.relu(f_x)\n",
    "        f_x = self.bn2(self.conv2(f_x), training=training)\n",
    "        f_x = tf.nn.relu(f_x)\n",
    "        f_x = self.bn3(self.conv3(f_x), training=training)\n",
    "        f_x = tf.nn.relu(f_x)\n",
    "        return x + f_x\n",
    "\n",
    "\n",
    "def create_model(conv_block, level_repeats=[3,3,3], initial_filter_size=32):\n",
    "    # encoder - input\n",
    "    model_input = tf.keras.Input(shape=(DATA_ROWS, DATA_COLS, DATA_CHANNELS), name='input_image')\n",
    "    x           = model_input\n",
    "    \n",
    "    x = tf.keras.layers.Conv2D(initial_filter_size, \n",
    "                               (7,7), \n",
    "                               strides=(1,1),\n",
    "                               activation='relu',\n",
    "                               padding='same')(x)\n",
    "    \n",
    "    \n",
    "    filter_size = initial_filter_size\n",
    "    for i in range(len(level_repeats)):\n",
    "        x = tf.keras.layers.Conv2D(filter_size, (3,3), strides=(2,2), activation='relu', padding='same')(x)\n",
    "        for n0 in range(level_repeats[i]):\n",
    "            x = conv_block(filter_size)(x)    \n",
    "        filter_size *= 2\n",
    "        \n",
    "        \n",
    "    encoder_output = x\n",
    "    # decoder\n",
    "    y              = tf.keras.layers.GlobalAveragePooling2D()(encoder_output)\n",
    "    decoder_output = tf.keras.layers.Dense(DATA_NUM_CLASSES, activation='softmax')(y)\n",
    "    \n",
    "    # forward path\n",
    "    model = tf.keras.Model(inputs=model_input, outputs=decoder_output, name='Mymodel')\n",
    "    # loss, backward path (implicit) and weight update\n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(TRAINING_LR_MAX), loss='categorical_crossentropy', metrics=['acc'])\n",
    "    # return model\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create and compile model\n",
    "model = create_model(ResNet_Block, level_repeats=[3,3,3], initial_filter_size=64)\n",
    "# model description and figure\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# learning rate schedule\n",
    "def lr_schedule(epoch):\n",
    "\n",
    "    # staircase\n",
    "    # lr = TRAINING_LR_MAX*math.pow(TRAINING_LR_SCALE, math.floor(epoch/TRAINING_LR_EPOCHS))\n",
    "\n",
    "    # linear warmup followed by cosine decay\n",
    "    if epoch < TRAINING_LR_INIT_EPOCHS:\n",
    "        lr = (TRAINING_LR_MAX - TRAINING_LR_INIT)*(float(epoch)/TRAINING_LR_INIT_EPOCHS) + TRAINING_LR_INIT\n",
    "    else:\n",
    "        lr = (TRAINING_LR_MAX - TRAINING_LR_FINAL)*max(0.0, math.cos(((float(epoch) - TRAINING_LR_INIT_EPOCHS)/(TRAINING_LR_FINAL_EPOCHS - 1.0))*(math.pi/2.0))) + TRAINING_LR_FINAL\n",
    "    return lr\n",
    "\n",
    "plt.plot(range(1,TRAINING_NUM_EPOCHS+1), [lr_schedule(i) for i in np.arange(TRAINING_NUM_EPOCHS)+1])\n",
    "plt.title(\"Learning Rate Schedule\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"l.r.\")\n",
    "plt.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot training accuracy and loss curves\n",
    "def plot_training_curves(history):\n",
    "\n",
    "    # training and validation data accuracy\n",
    "    acc     = history.history['accuracy']\n",
    "    val_acc = history.history['val_accuracy']\n",
    "\n",
    "    # training and validation data loss\n",
    "    loss     = history.history['loss']\n",
    "    val_loss = history.history['val_loss']\n",
    "\n",
    "    # plot accuracy\n",
    "    plt.figure(figsize=(8, 8))\n",
    "    plt.subplot(2, 1, 1)\n",
    "    plt.plot(acc, label='Training Accuracy')\n",
    "    plt.plot(val_acc, label='Validation Accuracy')\n",
    "    plt.legend(loc='lower right')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.ylim([min(plt.ylim()), 1])\n",
    "    plt.title('Training and Validation Accuracy')\n",
    "\n",
    "    # plot loss\n",
    "    plt.subplot(2, 1, 2)\n",
    "    plt.plot(loss, label='Training Loss')\n",
    "    plt.plot(val_loss, label='Validation Loss')\n",
    "    plt.legend(loc='upper right')\n",
    "    plt.ylabel('Cross Entropy')\n",
    "    plt.ylim([0, 2.0])\n",
    "    plt.title('Training and Validation Loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_step_size = 100000//TRAINING_NUM_EPOCHS\n",
    "val_step_size = 10000//TRAINING_NUM_EPOCHS\n",
    "\n",
    "# callbacks (learning rate schedule, model checkpointing during training)\n",
    "callbacks = [tf.keras.callbacks.LearningRateScheduler(lr_schedule),\n",
    "             tf.keras.callbacks.ModelCheckpoint(filepath=str(SAVE_MODEL_PATH), \n",
    "                                                save_best_only=True, \n",
    "                                                monitor='val_loss', \n",
    "                                                verbose=1)]\n",
    "\n",
    "# training\n",
    "initial_epoch_num = 0\n",
    "history           = model.fit(x=train_ds_cachefile, \n",
    "                              epochs=TRAINING_NUM_EPOCHS, \n",
    "                              verbose=1, \n",
    "                              steps_per_epoch=training_step_size,\n",
    "                              callbacks=callbacks, \n",
    "                              validation_data=valid_ds_cachefile,                               \n",
    "                              validation_steps=val_step_size,\n",
    "                              initial_epoch=initial_epoch_num)\n",
    "\n",
    "\n",
    "# plot accuracy and loss curves\n",
    "plot_training_curves(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test\n",
    "test_loss, test_accuracy = model.evaluate(x=test_ds_nocache)\n",
    "print('Test loss:     ', test_loss)\n",
    "print('Test accuracy: ', test_accuracy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:deep-learning]",
   "language": "python",
   "name": "conda-env-deep-learning-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
