{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-31T17:35:56.977130Z",
     "start_time": "2020-03-31T17:35:50.874338Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.1.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[course_syllabus](http://cs231n.stanford.edu/syllabus.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimization\n",
    "\n",
    "## Loss Functions and Optimization (Lecture 3)\n",
    "\n",
    "### Sources\n",
    "- [linear](http://cs231n.github.io/linear-classify/)\n",
    "- [opt-1](http://cs231n.github.io/optimization-1/)\n",
    "- [lecture video](https://www.youtube.com/watch?v=h7iBpEHGVNc&list=PL3FW7Lu3i5JvHM8ljYj-zLfQRF3EO8sYv&index=4&t=0s)\n",
    "- [lecture3 Slides](http://cs231n.stanford.edu/slides/2019/cs231n_2019_lecture03.pdf)\n",
    "\n",
    "__Linear classification intuition__ \n",
    "\n",
    "Each row in the matrix of weights represents a template for each class. Each weight represents the amount that each pixel contributes to the class prediction. Can also be seen as a high dimensional linear decision boundary seperating the vectors where values are pixel intensities.\n",
    "\n",
    "$$f(x,W,b)=Wx+b$$\n",
    "\n",
    "In the case of multiclass classification, W represents a matrix with a row for each potential target class. By multiplying this tensor with the input data x, the model evaluates the score for each class concurrently.\n",
    "\n",
    "Because of their limitations, linear classifications can only pick up on very simple features (color and location).\n",
    "\n",
    "__Loss Functions__ Maps a prediction and a true output label to a scalar that increases as the model performs worse. __Soft constraints__ on the model can be applied by adding regularization terms to the loss function. (L1, L2, ...) \n",
    "\n",
    "### __SVM Classifiers__\n",
    "\n",
    "$$Hinge Loss_i = \\sum max(0,s_j-s_{y_i}+\\Delta)+\\lambda R(W)$$\n",
    "\n",
    "Here $y_i$ represents the index of the correct class. $s_j$ represents the class scores determined by the linear classifier. The linear model predicts the highest class, so the above function penalizes any prediction that scores higher than the true class label. Any negative score receives a loss of zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-04T20:55:25.723701Z",
     "start_time": "2020-03-04T20:55:25.717717Z"
    }
   },
   "outputs": [],
   "source": [
    "def multiclass_svm_loss(x,y, W,delta=1, squared=False, l2_penalty=0.0):\n",
    "    \"\"\"\n",
    "    x: a tensor shape (1,feat_dims)\n",
    "    W: a tensor shape (feat_dims, num_classes)\n",
    "    y: a vector with the index of correct class \n",
    "    delta: goal margin between prediction class and other classes\n",
    "    squared: whether to use squared hinge\n",
    "    l2_penalty: regularization strength for L2 penalty\n",
    "    \"\"\"\n",
    "    scores = tf.linalg.matmul(x, W)\n",
    "    margins = tf.maximum(0, scores-scores[y]+delta)\n",
    "    margins[y]=0\n",
    "    \n",
    "    if squared:\n",
    "        margins = tf.math.square(margins)\n",
    "        \n",
    "    #Add L2 penalty    \n",
    "    margins+=l2_penalty*tf.reduce_sum(tf.math.square(W),axis=1, keep_dims=True)\n",
    "    \n",
    "    loss_i = tf.reduce_sum(margins)\n",
    "    return loss_i"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### __SoftMax Classifier__\n",
    "Alternative to an SVM classifier. The output of a softmax classifier is a probability distribution over the potential classes. With a softmax classifier, we interpret $f(x)=W\\cdot x$ as __unnormalized log probabilities__ for each class.\n",
    "\n",
    "__Cross-Entropy Loss__\n",
    "\n",
    "$$Cross Entropy Loss = -log(\\frac{e^{f_{y_i}}}{\\sum_je^{f_j}})$$\n",
    "\n",
    "\n",
    "__Cross Entropy__\n",
    "$$Cross Entropy = -\\sum p(x)log(q(x))$$\n",
    "\n",
    "__SoftMax Function__\n",
    "\n",
    "$$f(score) - \\frac{e^{score_{j}}}{\\sum_ke^{score_k}}$$\n",
    "\n",
    "Softmax classifiers minimizes the crossentropy between the true distribution and the predicted distribution.\n",
    "\n",
    "*Moreover, since the cross-entropy can be written in terms of entropy and the Kullback-Leibler divergence as $H(p,q)=H(p)+D_{KL}(p||q)$, and the entropy of the delta function p is zero, this is also equivalent to minimizing the KL divergence between the two distributions (a measure of distance). In other words, the cross-entropy objective wants the predicted distribution to have all of its mass on the correct answer.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(logits, axis=-1):\n",
    "    \"\"\"Naive Implementation\"\"\"\n",
    "    exps = tf.math.exp(logits)\n",
    "    return exps/tf.reduce_sum(exps, axis)\n",
    "\n",
    "#tf.nn.softmax()\n",
    "\n",
    "\n",
    "def crossentropy_loss(x, y, W, stable=True):\n",
    "    \"\"\"\n",
    "    x: a tensor shape (1,feat_dims)\n",
    "    W: a tensor shape (feat_dims, num_classes)\n",
    "    y: a vector with the index of correct class\n",
    "    stable: whether to scale scores for numeric stability\n",
    "    \"\"\"\n",
    "    score = tf.linalg.matmul(x,W)\n",
    "    if stable: #scale by max value\n",
    "        score-=tf.reduce_max(score)\n",
    "    probs = softmax(score)\n",
    "    return -tf.math.log(scores)[y]\n",
    "\n",
    "#tf.keras.losses.sparse_categorical_crossentropy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regularization (weight decay)\n",
    "\n",
    "Generally takes the form of\n",
    "$$L(W)=\\frac{1}{N}\\sum_i L_i(f(x_i,W),y_i)+\\lambda R(W)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reg(loss_func, x, y, W, l1=0.0, l2=0.0):\n",
    "    \"\"\"\n",
    "    Computes Elastic Net regularization for an arbitrary loss function of\n",
    "    form L(x,y,W)\n",
    "    \"\"\"\n",
    "    return loss_func(x,y,W) + l2*tf.reduce_sum(tf.math.square(W)) + l1*tf.reduce_sum(tf.math.abs(W))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### __Gradient Computation__ \n",
    "\n",
    "One can compute the gradient __numerically__ by evaluating the function with small perterbations along each dimension, or __analytically__ by deriving the gradient function with calculus. Numerical gradient computations can often be used as a sanity check of analytical methods.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-06T03:06:31.951081Z",
     "start_time": "2020-03-06T03:06:31.943103Z"
    }
   },
   "outputs": [],
   "source": [
    "def mult_n_add(x, w):\n",
    "    return tf.linalg.matmul(x, w) + 4\n",
    "\n",
    "def numerical_gradient(func, x, w):\n",
    "    \"\"\"\n",
    "    func: function to approximate gradient\n",
    "    x: a tensor shape (1,feat_dims)\n",
    "    \"\"\"\n",
    "    fx = func(x,w)\n",
    "    h=1e-3\n",
    "    #create a diagonal to add delta\n",
    "    diff_matrix = (tf.eye(x.shape[-1]) * h) + w\n",
    "    #wanted to use tf.map_fn here, but couldnt with the x input\n",
    "    #unstack -> recover dim -> apply function subtract by unperterbed value\n",
    "    grads = [func(x, tf.expand_dims(g,-1)) - fx for g in tf.unstack(diff_matrix, axis=-1)]\n",
    "    #stack and throw out extra dim\n",
    "    return tf.squeeze(tf.stack(grads), axis=-1)/h\n",
    "    \n",
    "#tf.test.compute_gradient()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-06T03:06:32.275055Z",
     "start_time": "2020-03-06T03:06:32.264083Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Auto gradient:  tf.Tensor(\n",
      "[[-0.9707055]\n",
      " [ 0.9031635]\n",
      " [-1.1831928]], shape=(3, 1), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "x = tf.random.normal((1,3))\n",
    "w = tf.random.normal((3,1))\n",
    "\n",
    "with tf.GradientTape() as g:\n",
    "    g.watch(w)\n",
    "    y = mult_n_add(x,w)\n",
    "dy_dw=g.gradient(y,w)\n",
    "    \n",
    "print(\"Auto gradient: \",dy_dw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-06T03:06:32.833038Z",
     "start_time": "2020-03-06T03:06:32.823061Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(3, 1), dtype=float32, numpy=\n",
       "array([[-0.9708404],\n",
       "       [ 0.9031295],\n",
       "       [-1.1835098]], dtype=float32)>"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numerical_gradient(mult_n_add, x, w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to Neural Networks (Lecture 4)\n",
    "\n",
    "### Sources\n",
    "- [optimization-2](http://cs231n.github.io/optimization-2/)\n",
    "- [deriv-paper](http://cs231n.stanford.edu/handouts/derivatives.pdf)\n",
    "- [lecun paper](http://yann.lecun.com/exdb/publis/pdf/lecun-98b.pdf)\n",
    "- [lecture4slides](http://cs231n.stanford.edu/slides/2019/cs231n_2019_lecture04.pdf)\n",
    "- [lecture video](https://www.youtube.com/watch?v=d14TUNcbn1k&list=PL3FW7Lu3i5JvHM8ljYj-zLfQRF3EO8sYv&index=4)\n",
    "- [auto-diff-survey](https://arxiv.org/pdf/1502.05767.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computational Graphs ([video](https://www.youtube.com/watch?v=d14TUNcbn1k&list=PL3FW7Lu3i5JvHM8ljYj-zLfQRF3EO8sYv&index=4))\n",
    "__Nodes__ are the computations and __edges__ are tensors. Backpropagation uses the chain rule to express the gradient wrt to every variable in the computational graph.\n",
    "\n",
    "Start at the computed loss at the end of the computational graph. $\\frac{\\delta Loss}{\\delta z}$. Then use the chain rule\n",
    "$$\\frac{\\delta f}{\\delta y} = \\frac{\\delta f}{\\delta q}\\frac{\\delta q}{\\delta y}$$\n",
    "\n",
    "For an arbitrary function $q(x,w)$ we can insert the function into the computational graph and apply backprop as long as we have the __local gradient__ $\\frac{\\delta q}{\\delta x}$ and $\\frac{\\delta q}{\\delta w}$\n",
    "\n",
    "You can make nodes in the graph have any granularity. They can be distinct additions, multiplications, or the grouping of multiple operations into a single node. ex make a sigmoid node. Grouping operations can make you comp graph smaller."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-08T22:00:51.584762Z",
     "start_time": "2020-03-08T22:00:51.573799Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([0.24751654], shape=(1,), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "#SIGMOID AS A SINGLE NODE\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + tf.exp(-x))\n",
    "\n",
    "z=tf.constant([0.2])\n",
    "with tf.GradientTape() as tape:\n",
    "    tape.watch(z)\n",
    "    sig = sigmoid(z)\n",
    "dsig_dz = tape.gradient(sig,z)\n",
    "\n",
    "#see below for a plot of sigmoid gradient\n",
    "print(dsig_dz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-08T22:00:52.225070Z",
     "start_time": "2020-03-08T22:00:52.217549Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([1. 1.], shape=(2,), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "#ADD GATE PASSES BACK GRADIENT TO BOTH BRANCHES\n",
    "def add_gate(x):\n",
    "    return tf.reduce_sum(x)\n",
    "\n",
    "z=tf.constant([0.2, 3.])\n",
    "with tf.GradientTape() as tape:\n",
    "    tape.watch(z)\n",
    "    m = add_gate(z)\n",
    "dm_dz = tape.gradient(m,z)\n",
    "\n",
    "print(dm_dz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-08T22:00:52.894864Z",
     "start_time": "2020-03-08T22:00:52.886856Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([0. 1.], shape=(2,), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "#MAX GATE ROUTES GRADIENT TO MAX ELEMENT   \n",
    "def max_gate(x):\n",
    "    return tf.reduce_max(x)\n",
    "\n",
    "z=tf.constant([.2, 3.])\n",
    "with tf.GradientTape() as tape:\n",
    "    tape.watch(z)\n",
    "    m = max_gate(z)\n",
    "dm_dz = tape.gradient(m,z)\n",
    "\n",
    "print(dm_dz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-08T22:00:53.679423Z",
     "start_time": "2020-03-08T22:00:53.669772Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([3.  0.2], shape=(2,), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "#MULTIPLICATION GATE - LOCAL GRADIENT IS THE VALUE OF THE OTHER BRANCH\n",
    "def mult_gate(x):\n",
    "    return tf.reduce_prod(x)\n",
    "\n",
    "z=tf.constant([0.2, 3.])\n",
    "with tf.GradientTape() as tape:\n",
    "    tape.watch(z)\n",
    "    m = mult_gate(z)\n",
    "dm_dz = tape.gradient(m,z)\n",
    "\n",
    "print(dm_dz)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__At branches__ in the computational graph, the gradients sum together during back propagation.\n",
    "\n",
    "$$Vec(1,4000)\\rightarrow f(x)=max(0,x) \\rightarrow Vec(1,4000)$$\n",
    "\n",
    "$$Jacobian \\in \\mathbb{R} (4000,4000)$$\n",
    "\n",
    "__Jacobian__ each row is partial derivative of each dim of output wrt to each dim of input. In practice dont need to compute a huge Jacobian $\\rightarrow$ Jacobian is going to be a diagonal matrix for elementwise functions. \n",
    "\n",
    "__The gradient wrt a vector is always going to be the same size of the original vector__. Each element in the gradient shows how much each corresponding weight/input effects the ouput of the computational graph. \n",
    "\n",
    "\n",
    "#### Implementing in Code\n",
    "During forward pass __compute nodes in topologically sorted order__ so every input is ready when it is needed. __Cache the values of the forward pass for use in backwards pass.__\n",
    "\n",
    "Implement the graph with node classes with `.forward()` and `.backward()` API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class sigmoid_node():\n",
    "    \"\"\"Sigmoid operation\"\"\"\n",
    "    def __init__(self, axis=-1):\n",
    "        self.z=None\n",
    "    def forward(self, x):\n",
    "        self.z = 1 / (1 + tf.exp(-x))\n",
    "        return self.z\n",
    "    def backward(self, dz):\n",
    "        return dz*(1-self.z)*self.z\n",
    "\n",
    "class add_node():\n",
    "    \"\"\"Elementwise addition\"\"\"\n",
    "    def __init__(self,):\n",
    "    def forward(self,x,y):\n",
    "        return tf.add(x,y)\n",
    "    def backward(self,dz):\n",
    "        return [dz,dz]\n",
    "\n",
    "class max_node():\n",
    "    def __init__(self, axis=-1):\n",
    "        self.max_val=None\n",
    "        self.mat_shape=None\n",
    "        self.axis=axis\n",
    "    def forward(self,x):\n",
    "        self.max_val=tf.argmax(x, axis=self.axis)\n",
    "        self.mat_shape=x.shape\n",
    "        return tf.reduce_max(x, axis=self.axis)\n",
    "    \n",
    "    def backward(self,dz):\n",
    "        g = tf.zeros(self.mat_shape)\n",
    "        g[self.max_val] = dz #this wont work, but im not sure how to set indexed values\n",
    "        return g\n",
    "    \n",
    "class mult_node():\n",
    "    def __init__(self,):\n",
    "        self.x=None\n",
    "        self.y=None\n",
    "    def forward(self,x, y):\n",
    "        self.x=x\n",
    "        self.y=y\n",
    "        return x*y\n",
    "    def backward(self,dz):\n",
    "        dy=dz*self.x\n",
    "        dx=dz*self.y\n",
    "        return [dx,dy]\n",
    "\n",
    "class mat_mul_node():\n",
    "    def __init__(self,):\n",
    "        self.x=None\n",
    "        self.y=None\n",
    "    def forward(self,x, y):\n",
    "        self.x=x\n",
    "        self.y=y\n",
    "        z = tf.matmul(x,y)\n",
    "        return z\n",
    "    def backward(self,dz):\n",
    "        dy=tf.matmul(dz, self.x, transpose_b=True)\n",
    "        dx=tf.matmul(dz, self.y, transpose_b=True)\n",
    "        return [dx,dy]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class computational_graph():\n",
    "    def __init__(self):\n",
    "        #CONSTRUCT GRAPH HERE\n",
    "        return None\n",
    "    def call(self, inputs):\n",
    "        #ITERATE THROUGH GRAPH WITH .forward() METHOD\n",
    "        return None\n",
    "    def backward(self):\n",
    "        #ITERATE THROUGH GRAPH WITH .backward() METHOD\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Stacking nonlinear functions__ allows stacking simpler functions together to compute complex nonlinear functions. $W_1$ is like a template for each predicted class. $W_1$ is a weighting of these templates that allows combinations of features. \n",
    "$$f=W_2max(0,W_1\\cdot x)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimization ([notes](http://cs231n.github.io/optimization-2/))\n",
    "\n",
    "__Backpropagation__ is a recursive application of the chain rule to a set of  nodes in a computational graph. The objective of backprop is to compute the gradient at x, $\\nabla Loss_{W}(x)$.\n",
    "\n",
    "*The derivative on each variable tells you the sensitivity of the whole expression on its value.*\n",
    "\n",
    "*Notice that if one of the inputs to the multiply gate is very small and the other is very big, then the multiply gate will do something slightly unintuitive: it will assign a relatively huge gradient to the small input and a tiny gradient to the large input. Note that in linear classifiers where the weights are dot producted wTxi (multiplied) with the inputs, __this implies that the scale of the data has an effect on the magnitude of the gradient for the weights.__ For example, if you multiplied all input data examples xi by 1000 during preprocessing, then the gradient on the weights will be 1000 times larger, and you’d have to lower the learning rate by that factor to compensate*\n",
    "\n",
    "__Tips:__\n",
    "- Normalize your data\n",
    "- Use dimension analysis - The gradient for W must have the same shape as W\n",
    "- Work small and generalize.\n",
    "- __Local gradient is on the right!__\n",
    "$\\frac{\\delta f}{\\delta y}= Upstream Gradient * Local Gradient$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Derivatives ([Paper](http://cs231n.stanford.edu/handouts/derivatives.pdf))\n",
    "\n",
    "__Gradient__ the derivative of a vector to scalar function. The gradient is a vector of the same size as the input vector.The gradient is a vector of partial derivatives.\n",
    "\n",
    "\n",
    "$$f:\\mathbb{R}^N\\rightarrow \\mathbb{R}$$\n",
    "\n",
    "\n",
    "$$Gradient=(\\frac{\\delta y}{\\delta x_1},...,\\frac{\\delta y}{\\delta x_N})$$\n",
    "\n",
    "__Jacobian__ the derivative of a vector to vector function. The jacobian is a matrix the shape of each of the input vectors. The Jacobian tells the relation between each element in x to each element in y. Each row in the Jacobian is a gradient to a single scalar element of y.\n",
    "\n",
    "$$f:\\mathbb{R}^N\\rightarrow \\mathbb{R}^M$$\n",
    "\n",
    "<img src='imgs/jacobian.PNG' width=250>\n",
    "\n",
    "__Generalized Jacobian__ The dimensions of the Jacobian will be the cross product of the dimensions of the input and output.\n",
    "\n",
    "$$f:\\mathbb{R}^{N_1x...xN_D}\\rightarrow \\mathbb{R}^{M_1x...xM_B}$$\n",
    "\n",
    "$$Jacobian Dim: (M_1x...xM_B)x(N_1x...xN_D)$$\n",
    "\n",
    "\n",
    "*Note that we have separated the dimensions of ∂y/∂x into two groups: the\n",
    "first group matches the dimensions of y and the second group matches the\n",
    "dimensions of x. With this grouping, we can think of the generalized Jacobian\n",
    "as generalization of a matrix, where each  “row” has the same shape as y and\n",
    "each “column” has the same shape as x.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Automatic Differentiation ([paper](https://arxiv.org/pdf/1502.05767.pdf))\n",
    "\n",
    "* \"All numerical computations are ultimately compositions of a finite set of elementary operations for which derivatives are known (Verma, 2000; Griewank and Walther, 2008), and combining the derivatives of the constituent operations through the chain rule gives the derivative of the overall composition.Usually these elementary operations include the binary arithmetic operations, the unary sign switch, and transcendental functions such as the exponential, the logarithm, and the trigonometric functions.\"*\n",
    "\n",
    "AD can differnetiate closed-form expressions as well as algorithmic control flow with branches and loops. Control flow can be differentiated because any code execution will result in single flow of numerical computation w/ particular values for inputs, intermediate values, and output variables. \n",
    "\n",
    "* \"AD is blind with respect to any operation, including control flow statements, which do not directly alter numeric values.\"*\n",
    "\n",
    "__Forward Mode__ Associate each intermediate variable $v_1$ with a local derivative \n",
    "\n",
    "$$\\dot{v_1}=\\frac{\\delta v_1}{\\delta x_1}$$\n",
    "\n",
    "After local derivatives are stored, the final Jacobian can be computed in multiple passes with the stored\n",
    "\n",
    "Less storage required since intermediate dependencies do not need to be stored. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Reverse Mode__ reverse accumulation corresponds to the general backpropagation algorithm.\n",
    "\n",
    "*In reverse mode AD, derivatives are computed in the second phase of a two-phase process. In the first phase, the original function code is run forward, populating intermediate variables vi and recording the dependencies in the computational graph through a bookkeeping procedure. In the second phase, derivatives are calculated by propagating adjoints v¯i in reverse, from the outputs to the inputs.*\n",
    "\n",
    "Reverse Mode AD is computationally more efficient than forward mode, but requires more storage. In most cases backprop algorithms are used to compute the gradient of a function of form\n",
    "\n",
    "$$f:\\mathbb{R}^N\\rightarrow \\mathbb{R}$$\n",
    "\n",
    "For large N, reverse mode AD provides a highly efficient method of gradient computation.\n",
    "\n",
    "<img src=imgs/reverse_mode.PNG width=400>\n",
    "\n",
    "\n",
    "__Newton's Method__ make use of the gradient $\\nabla f$ and the Hessian $H_f$ with updates of the form $$\\Delta w=-\\eta H_f^{-1}\\nabla f$$\n",
    "\n",
    "Faster convergence at the cost of more compute.\n",
    "\n",
    "*An important direction for future work is to make use of nested AD techniques in machine learning, allowing differentiation to be nested arbitrarily deep with referential transparency (Siskind and Pearlmutter, 2008b; Pearlmutter and Siskind, 2008). Nested AD is highly relevant in hyperparameter optimization as it can effortlessly provide exact hypergradients, that is, derivatives of a training objective with respect to the hyperparameters of an optimization routine *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Efficient Backpropagation [paper](http://yann.lecun.com/exdb/publis/pdf/lecun-98b.pdf) old\n",
    "\n",
    "\n",
    "__A Few Practical Tricks__\n",
    "- Stochastic Learning has better results\n",
    "- Batch learning is faster (with GPU parallelization)\n",
    "- Shuffle the training sets \n",
    "- Normalize the inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Networks\n",
    "\n",
    "## Convolutional Neural Networks (Lecture 5)\n",
    "\n",
    "### Sources\n",
    "- [lecture-slides](http://cs231n.stanford.edu/slides/2019/cs231n_2019_lecture05.pdf)\n",
    "- [cnn-notes](http://cs231n.github.io/convolutional-networks/)\n",
    "- [lecture-video](https://www.youtube.com/watch?v=bNb2fEVKeEo&list=PL3FW7Lu3i5JvHM8ljYj-zLfQRF3EO8sYv&index=5)\n",
    "\n",
    "Learned this too many times. Moving forward.\n",
    "\n",
    "### Brief CNN Notes ([cnn-notes](http://cs231n.github.io/convolutional-networks/))\n",
    "*__Local Connectivity__. When dealing with high-dimensional inputs such as images, as we saw above it is impractical to connect neurons to all neurons in the previous volume. Instead, we will connect each neuron to only a local region of the input volume. The spatial extent of this connectivity is a hyperparameter called the __receptive field__ of the neuron*\n",
    "\n",
    "*The connections are local in space (along width and height), but always full along the entire depth of the input volume.*\n",
    "\n",
    "How are kernels/filters applied to the input data? Three parameters determine the size of the activation for a given input.\n",
    "\n",
    "*First, the depth of the output volume is a hyperparameter: it corresponds to the __number of filters__ we would like to use, each learning to look for something different in the input.*\n",
    "\n",
    "*Second, we must specify the __stride__ with which we slide the filter.*\n",
    "\n",
    "*As we will soon see, sometimes it will be convenient to pad the input volume with zeros around the border. The size of this __zero-padding__ is a hyperparameter.*\n",
    "\n",
    "__Parameter Sharing__ Allows conv nets to be more efficient than standard FC networks. Instead of connnecting a neuron to every pixel, share features that can be convolved across the image. Assumes that fe\n",
    "\n",
    "__1x1 convolutions__ Linear combination layer across the dimensions of the input. Often used to expand or shrink the depth dimension.\n",
    "\n",
    "__Pooling Layers__ progressively reduces the spatial volume of feature maps and reduces the amount of parameters (and therefore computation)\n",
    "\n",
    "*Getting rid of pooling. Many people dislike the pooling operation and think that we can get away without it. For example, Striving for Simplicity: The All Convolutional Net proposes to discard the pooling layer in favor of architecture that only consists of repeated CONV layers.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Network in Network ([paper](https://arxiv.org/pdf/1312.4400v3.pdf)) 2014\n",
    "\n",
    "__Network in Network__ is the stacking of multiple mlpconv layers. This involves a spatial convolution followed by a series of fully connected layers with nonlinearities that share features across the spatial regions.\n",
    "\n",
    "Spatial conv $\\rightarrow$ __1x1 convolutions__\n",
    "\n",
    "*The cross channel parametric pooling layer is also equivalent to a convolution layer with 1x1 convolution kernel. This interpretation makes it straightforawrd to understand the structure of NIN.*\n",
    "\n",
    "*This cascaded cross channel parameteric pooling structure allows complex and learnable\n",
    "interactions of cross channel information.*\n",
    "\n",
    "*In this paper, we propose another strategy called __global average pooling__ to replace the traditional\n",
    "fully connected layers in CNN. The idea is to generate one feature map for each corresponding\n",
    "category of the classification task in the last mlpconv layer. Instead of adding fully connected layers\n",
    "on top of the feature maps, we take the average of each feature map, and the resulting vector is fed\n",
    "directly into the softmax layer. One advantage of global average pooling over the fully connected\n",
    "layers is that it is more native to the convolution structure by enforcing correspondences between\n",
    "feature maps and categories. Thus the feature maps can be easily interpreted as categories confidence\n",
    "maps.*\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NetworkinNetwork(tf.keras.layers.Layer):\n",
    "    def __init__(self, unit_list, num_mlp_layers=3):\n",
    "        super(mlpConv, self).__init__()\n",
    "        self.spatial_conv = tf.keras.layers.Conv2D(unit_list[0], 3, padding='same')\n",
    "        self.mlp_layers = []\n",
    "        self.num_mlp_layers = num_mlp_layers\n",
    "        for i in range(num_mlp_layers):\n",
    "            self.mlp_layers.append(tf.keras.layers.Conv2D(unit_list[i],1, padding='same', activation='relu'))\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        x = self.spatial_conv(inputs) \n",
    "        for i in self.num_mlp_layers:\n",
    "            x = self.mlp_layers[i](x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### All Conv Net ([paper](https://arxiv.org/pdf/1412.6806.pdf) 2015)\n",
    "\n",
    "*We find that max-pooling can simply be replaced by a convolutional layer with increased stride without loss in accuracy on several image recognition benchmarks. Following this finding – and building on other recent work for finding simple network structures – we propose a new architecture that consists solely of convolutional layers and yields competitive or state of the art performance on several object recognition datasets*\n",
    "\n",
    "THeres some other interesting stuff here about the use of 1x1 convolutions and deconvoltions for kernel visualization.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:deep-learning]",
   "language": "python",
   "name": "conda-env-deep-learning-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
