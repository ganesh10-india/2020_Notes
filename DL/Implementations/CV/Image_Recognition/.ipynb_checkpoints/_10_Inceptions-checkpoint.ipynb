{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_datasets as tfds\n",
    "import tensorflow as     tf\n",
    "import math\n",
    "import numpy             as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.layers import *\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.cifar_functions import *\n",
    "\n",
    "#PREPARE THE CIFAR DATASET\n",
    "# download data and split into training and testing datasets\n",
    "dataset_train, info = tfds.load(\"cifar10\", split=tfds.Split.TRAIN, with_info=True)\n",
    "dataset_test,  info = tfds.load(\"cifar10\", split=tfds.Split.TEST,  with_info=True)\n",
    "\n",
    "dataset_train = dataset_train.map(pre_processing_train, num_parallel_calls=4)\n",
    "dataset_train = dataset_train.shuffle(buffer_size=TRAINING_SHUFFLE_BUFFER)\n",
    "dataset_train = dataset_train.batch(TRAINING_BATCH_SIZE)\n",
    "dataset_train = dataset_train.prefetch(buffer_size=3)\n",
    "\n",
    "# transform testing dataset\n",
    "dataset_test = dataset_test.map(pre_processing_test, num_parallel_calls=4)\n",
    "dataset_test = dataset_test.batch(TRAINING_BATCH_SIZE)\n",
    "dataset_test = dataset_test.prefetch(buffer_size=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inception V1 ([paper](https://arxiv.org/pdf/1409.4842v1.pdf))\n",
    "\n",
    "<img src=\"imgs/inceptionv1_block.png\">\n",
    "\n",
    "## Notes\n",
    "*The main idea of the Inception architecture is based on finding out how an optimal local sparse\n",
    "structure in a convolutional vision network can be approximated and covered by readily available\n",
    "dense components*\n",
    "\n",
    "\n",
    "*This leads to the second idea of the proposed architecture: __judiciously applying dimension reductions and projections wherever the computational requirements would increase too much otherwise.__\n",
    "This is based on the success of embeddings: even low dimensional embeddings might contain a lot\n",
    "of information about a relatively large image patch. However, embeddings represent information in\n",
    "a dense, compressed form and compressed information is harder to model. represent information in\n",
    "a dense, compressed form and compressed information is harder to model. We would like to keep\n",
    "our representation sparse at most places (as required by the conditions of [2]) and compress the\n",
    "signals only whenever they have to be aggregated en masse. That is, __1×1 convolutions are used to\n",
    "compute reductions before the expensive 3×3 and 5×5 convolutions.__*\n",
    "\n",
    "*One of the main beneficial aspects of this architecture is that it allows for increasing the number of\n",
    "units at each stage significantly without an uncontrolled blow-up in computational complexity. The\n",
    "ubiquitous use of dimension reduction allows for shielding the large number of input filters of the\n",
    "last stage to the next layer, first reducing their dimension before convolving over them with a large\n",
    "patch size. Another practically useful aspect of this design is that it aligns with the intuition that\n",
    "__visual information should be processed at various scales and then aggregated so that the next stage\n",
    "can abstract features from different scales simultaneously.__*\n",
    "\n",
    "*__By adding auxiliary classifiers connected to\n",
    "these intermediate layers, we would expect to encourage discrimination in the lower stages in the\n",
    "classifier, increase the gradient signal that gets propagated back, and provide additional regularization__*\n",
    "\n",
    "\n",
    "*Our training used asynchronous stochastic gradient descent with 0.9 momentum [17], fixed learning rate schedule (decreasing the learning rate by 4% every 8 epochs). Polyak averaging [13] was used to create the final model used at inference time.*\n",
    "\n",
    "\n",
    "<img src=\"imgs/lenet.PNG\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generic_tail(inputs, dims):\n",
    "    return Conv2D(dims, (3,3), activation='relu', padding='same')(inputs)\n",
    "\n",
    "def InceptionV1(inputs, squeeze_dims, out_dims, strides=(1,1)):\n",
    "    \"\"\"\n",
    "    Inception V1 Conv Block (GoogLeNet).\n",
    "    \"\"\"\n",
    "    x1 = conv_block(inputs, out_dims, kernel_size=(1,1))\n",
    "    \n",
    "    x2 = conv_block(inputs, squeeze_dims, kernel_size=(1,1))\n",
    "    x2 = conv_block(x2, out_dims, kernel_size=(3,3))\n",
    "    \n",
    "    x3 = conv_block(inputs, squeeze_dims, kernel_size=(1,1))\n",
    "    x3 = conv_block(x3, out_dims, kernel_size=(5,5))\n",
    "    \n",
    "    x4 = MaxPool2D(pool_size=(3,3), strides=(1,1))(inputs)\n",
    "    x4 = conv_block(inputs, out_dims, kernel_size=(1,1))\n",
    "    \n",
    "    concat = tf.concat([x1,x2,x3,x4],3)\n",
    "    return concat\n",
    "\n",
    "def generic_head(inputs, dims=None):\n",
    "    x = GlobalAveragePooling2D()(inputs)\n",
    "    return Dense(DATA_NUM_CLASSES, activation='softmax')(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inceptionv1 = VGG_Like_CNN(generic_tail,\n",
    "            InceptionV1,\n",
    "            generic_head,\n",
    "            input_shape = (DATA_CROP_ROWS, DATA_CROP_COLS, DATA_CHANNELS),\n",
    "            num_levels= 3,\n",
    "            num_downsamples=2,\n",
    "            block_repeats=BLOCK_REPEATS)\n",
    "\n",
    "resnext.compile(optimizer = tf.keras.optimizers.Adam(TRAINING_LR_MAX),\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "               metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Number of trainable parameters: \", get_num_params(inceptionv1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist = train(inceptionv1, dataset_train, dataset_test, 'inceptionv1', logs=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inception V2 ([paper](https://arxiv.org/pdf/1512.00567v3.pdf))\n",
    "\n",
    "*Avoid representational bottlenecks, especially early in\n",
    "the network. One should avoid\n",
    "bottlenecks with extreme compression. In general the\n",
    "representation size should gently decrease from the inputs to the outputs before reaching the final representation used for the task at hand.*\n",
    "\n",
    "*Spatial aggregation can be done over lower dimensional embeddings without much or any loss in representational power. For example, before performing a\n",
    "more spread out (e.g. 3 × 3) convolution, one can reduce the dimension of the input representation before\n",
    "the spatial aggregation without expecting serious adverse effects.*\n",
    "\n",
    "__Inception V2__ focused on: \n",
    "- avoiding representational bottlenecks with extreme compression. \n",
    "- converting the 5x5 convolution to two stacked 3x3 convolutions. \n",
    "- Also focuses on reducing compute by factorizing an nxn convolution to an nx1 and 1xn convolutions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def InceptionV2_fig5(inputs, squeeze_dims, out_dims, strides=(1,1)):\n",
    "    \"\"\"\n",
    "    Inception V2 Figure 5\n",
    "    \"\"\"\n",
    "    x1 = conv_block(inputs, squeeze_dims, kernel_size=(1,1))\n",
    "    \n",
    "    x2 = conv_block(inputs, squeeze_dims, kernel_size=(1,1))\n",
    "    x2 = conv_block(x2, out_dims, kernel_size=(3,3))\n",
    "    \n",
    "    x3 = conv_block(inputs, squeeze_dims, kernel_size=(1,1))\n",
    "    x3 = conv_block(x3, out_dims, kernel_size=(3,3))\n",
    "    x3 = conv_block(x3, out_dims, kernel_size=(3,3))\n",
    "    \n",
    "    x4 = MaxPool2D(pool_size=(3,3), strides=(1,1))(inputs)\n",
    "    x4 = conv_block(inputs, squeeze_dims, kernel_size=(1,1))\n",
    "    \n",
    "    concat = tf.concat([x1,x2,x3,x4],3)\n",
    "    return concat\n",
    "\n",
    "def InceptionV2_fig6(inputs, squeeze_dims, out_dims, strides=(1,1)):\n",
    "    \"\"\"\n",
    "    Inception V2 figure 6\n",
    "    \"\"\"\n",
    "    x1 = conv_block(inputs, squeeze_dims, kernel_size=(1,1))\n",
    "    \n",
    "    x2 = conv_block(inputs, squeeze_dims, kernel_size=(1,1))\n",
    "    x2 = conv_block(x2, out_dims, kernel_size=(3,1))\n",
    "    x2 = conv_block(x2, out_dims, kernel_size=(1,3))\n",
    "    \n",
    "    x3 = conv_block(inputs, squeeze_dims, kernel_size=(1,1))\n",
    "    x3 = conv_block(x3, out_dims, kernel_size=(3,1))\n",
    "    x3 = conv_block(x3, out_dims, kernel_size=(1,3))\n",
    "    x3 = conv_block(x3, out_dims, kernel_size=(3,1))\n",
    "    x3 = conv_block(x3, out_dims, kernel_size=(1,3))\n",
    "    \n",
    "    x4 = MaxPool2D(pool_size=(3,3), strides=(1,1))(inputs)\n",
    "    x4 = conv_block(inputs, squeeze_dims, kernel_size=(1,1))\n",
    "    \n",
    "    concat = tf.concat([x1,x2,x3,x4],3)\n",
    "    return concat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inception V4 ([paper](https://arxiv.org/pdf/1602.07261.pdf))\n",
    "\n",
    "## Notes\n",
    "\n",
    "Inception V4 introduces a new tail design that features:\n",
    "- parallel stride 2 3x3 conv and 3x3 max pool (concatenated)\n",
    "- parallel 3x3 conv and factorized 7x1 and 1x7 conv\n",
    "\n",
    "<img src=\"imgs/inceptionv4_tail.png\">\n",
    "\n",
    "There a"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:deep-learning]",
   "language": "python",
   "name": "conda-env-deep-learning-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
