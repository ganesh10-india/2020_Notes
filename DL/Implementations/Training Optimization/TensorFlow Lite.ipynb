{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-02T21:49:36.831426Z",
     "start_time": "2020-04-02T21:49:36.825468Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.1.0\n"
     ]
    }
   ],
   "source": [
    "#GETTING THE CIFAR DATASET READY FOR EXPERIMENTATION\n",
    "import tensorflow_datasets as tfds\n",
    "import tensorflow as     tf\n",
    "import math\n",
    "import numpy             as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.layers import *\n",
    "import pathlib\n",
    "\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-02T21:26:09.917697Z",
     "start_time": "2020-04-02T21:26:07.195578Z"
    }
   },
   "outputs": [],
   "source": [
    "from utils.cifar_functions import *\n",
    "\n",
    "#PREPARE THE CIFAR DATASET\n",
    "# download data and split into training and testing datasets\n",
    "dataset_train, info = tfds.load(\"cifar10\", split=tfds.Split.TRAIN, with_info=True)\n",
    "dataset_test,  info = tfds.load(\"cifar10\", split=tfds.Split.TEST,  with_info=True)\n",
    "\n",
    "dataset_train = dataset_train.map(pre_processing_train, num_parallel_calls=4)\n",
    "dataset_train = dataset_train.shuffle(buffer_size=TRAINING_SHUFFLE_BUFFER)\n",
    "dataset_train = dataset_train.batch(TRAINING_BATCH_SIZE)\n",
    "dataset_train = dataset_train.prefetch(buffer_size=3)\n",
    "\n",
    "# transform testing dataset\n",
    "dataset_test = dataset_test.map(pre_processing_test, num_parallel_calls=4)\n",
    "dataset_test = dataset_test.batch(TRAINING_BATCH_SIZE)\n",
    "dataset_test = dataset_test.prefetch(buffer_size=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TensorFlow Lite\n",
    "\n",
    "\n",
    "## Overview ([link](https://www.tensorflow.org/lite/guide/get_started#4_optimize_your_model_optional))\n",
    "\n",
    "To use a TensorFlow model you must convert it into TFLite format. You cannot create a model with TFLite, you must convert a tensorflow model to TFLite.\n",
    "\n",
    "TFLite is designed to execute models efficiently for low resoure settings. Converting models reduces their file size, further optimizations increase speed decrease size with some tradeoffs.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TensorFlow Lite Converter\n",
    "\n",
    "The converter can convert from many sources: Keras models and SavedModel directories. Converts the model int a `.tflite` file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-02T21:54:47.941687Z",
     "start_time": "2020-04-02T21:54:23.682254Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4129644"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#SavedModel from mobilenet_v2 implementation\n",
    "model_dir = \"F://Models/Model_Design/mobilenet_v2/\"\n",
    "model = tf.keras.models.load_model(model_dir)\n",
    "\n",
    "converter = tf.lite.TFLiteConverter.from_saved_model(model_dir)\n",
    "tflite_model = converter.convert()\n",
    "\n",
    "#write to .tflite file\n",
    "base_dir = pathlib.Path(model_dir).parent.parent / \"TFLite/\"\n",
    "base_dir.mkdir(parents=True, exist_ok=True)\n",
    "tflite_path = base_dir/'mobilenet_v2.tflite'\n",
    "tflite_path.write_bytes(tflite_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `Interpreter` object can be called with keywords `model_content=tflit_obj` or `model_path`. Next after the interpreter has been instantiated, `allocate_tensors()` will retrieve the tensors required in the model graph. Using `get_input/output_details()` will give the shape and index of the input/output tensors. Use this later when running inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-02T21:36:02.372792Z",
     "start_time": "2020-04-02T21:36:02.364160Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'name': 'input_2', 'index': 168, 'shape': array([ 1, 28, 28,  3]), 'dtype': <class 'numpy.float32'>, 'quantization': (0.0, 0)}]\n"
     ]
    }
   ],
   "source": [
    "# Load TFLite model and allocate tensors.\n",
    "interpreter = tf.lite.Interpreter(model_content=tflite_model)\n",
    "interpreter.allocate_tensors()\n",
    "\n",
    "# Get input and output tensors.\n",
    "input_details = interpreter.get_input_details()\n",
    "output_details = interpreter.get_output_details()\n",
    "\n",
    "# Test the TensorFlow Lite model on random input data.\n",
    "input_shape = input_details[0]['shape']\n",
    "print(input_details)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running Inference\n",
    "\n",
    "The __TensorFlow Lite Interpreter__ takes a model file and eecutes its operation. The interpreter has APIs in many different languages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-02T22:01:18.258123Z",
     "start_time": "2020-04-02T22:01:17.144008Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[9.2361607e-10 7.8270359e-06 4.3587871e-03 9.3027151e-01 1.0160868e-04\n",
      "  5.8098314e-05 6.5200336e-02 3.3402384e-10 1.2902692e-07 1.7488577e-06]]\n"
     ]
    }
   ],
   "source": [
    "input_data = np.array(np.random.random_sample(input_shape), dtype=np.float32)\n",
    "interpreter.set_tensor(input_details[0]['index'], input_data)\n",
    "\n",
    "interpreter.invoke()\n",
    "\n",
    "# The function `get_tensor()` returns a copy of the tensor data.\n",
    "# Use `tensor()` in order to get a pointer to the tensor.\n",
    "tflite_results = interpreter.get_tensor(output_details[0]['index'])\n",
    "\n",
    "# Test the TensorFlow model on random input data.\n",
    "tf_results = model(input_data)\n",
    "\n",
    "# Compare the result.\n",
    "for tf_result, tflite_result in zip(tf_results, tflite_results):\n",
    "    np.testing.assert_almost_equal(tf_result, tflite_result, decimal=5)\n",
    "\n",
    "print(tflite_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GPU Acceleration\n",
    "\n",
    "TFLite Interpreter can be specifes to make use of hardware acceleration o"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Optimization\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quantization\n",
    "\n",
    "https://www.tensorflow.org/model_optimization/guide/quantization/post_training\n",
    "\n",
    "https://www.tensorflow.org/lite/performance/post_training_quantization\n",
    "\n",
    "https://www.tensorflow.org/lite/performance/post_training_quant\n",
    "https://www.tensorflow.org/lite/performance/post_training_integer_quant\n",
    "https://www.tensorflow.org/lite/performance/post_training_float16_quant\n",
    "https://github.com/tensorflow/tensorflow/tree/r1.14/tensorflow/contrib/quantize\n",
    "[github tutorial](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/performance/post_training_quant.ipynb)\n",
    "\n",
    "TFLite allows quantization during model conversion. Activations are always stored in floating point. Some ops allow quantized kernels, prior to op, activations are quantized to 8-bits of precision dynamically, then dequantized to float after processing.\n",
    "\n",
    "With post training quantization, you must test to see if performance degradation is acceptable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-02T21:48:18.580451Z",
     "start_time": "2020-04-02T21:48:13.535515Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    157/Unknown - 5s 32ms/step - loss: 0.2933 - accuracy: 0.9215 5s 31ms/step - loss: 0.2971 - accu"
     ]
    }
   ],
   "source": [
    "#unquantized model performance\n",
    "test_loss, test_accuracy = model.evaluate(x=dataset_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-02T22:00:08.504723Z",
     "start_time": "2020-04-02T22:00:08.497117Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16.893877 MB\n"
     ]
    }
   ],
   "source": [
    "#SavedModel size\n",
    "out = sum(f.stat().st_size for f in pathlib.Path(model_dir).glob('**/*') if f.is_file() )\n",
    "print(out/1e6, \"MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-02T22:01:50.654508Z",
     "start_time": "2020-04-02T22:01:50.649478Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.129644 MB\n"
     ]
    }
   ],
   "source": [
    "#tflite size wout quantizaton (in bytes)\n",
    "out = tflite_path.stat().st_size\n",
    "print(out/1e6, \"MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-02T22:03:46.797624Z",
     "start_time": "2020-04-02T22:03:42.258430Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.091672 MB\n"
     ]
    }
   ],
   "source": [
    "#Set optimizer flag to optimize for size\n",
    "converter.optimizations = [tf.lite.Optimize.OPTIMIZE_FOR_SIZE]\n",
    "tflite_quant_model = converter.convert()\n",
    "tflite_model_quant_file = base_dir/\"mobilenet_v2_quant.tflite\"\n",
    "out = tflite_model_quant_file.write_bytes(tflite_quant_model)\n",
    "print(out/1e6, \"MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interpreter = tf.lite.Interpreter(model_path=str(tflite_path))\n",
    "interpreter.allocate_tensors()\n",
    "\n",
    "interpreter_quant = tf.lite.Interpreter(model_path=str(tflite_model_quant_file))\n",
    "interpreter.allocate_tensors()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pruning\n",
    "https://www.tensorflow.org/model_optimization/guide/pruning/train_sparse_models\n",
    "\n",
    "https://www.tensorflow.org/lite/performance/post_training_quantization\n",
    "\n",
    "https://www.tensorflow.org/model_optimization/guide/pruning/train_sparse_models\n",
    "\n",
    "https://www.tensorflow.org/model_optimization/guide/pruning/pruning_with_keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.tensorflow.org/lite/guide/hosted_models"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:deep-learning]",
   "language": "python",
   "name": "conda-env-deep-learning-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
