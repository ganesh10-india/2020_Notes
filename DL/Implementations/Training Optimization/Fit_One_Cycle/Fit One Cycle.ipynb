{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fit One Cycle\n",
    "\n",
    "## Resources\n",
    "- [The 1cycle Policy Sylvain Gugger](https://sgugger.github.io/the-1cycle-policy.html)\n",
    "- [FastAI Docs fitonecycle](https://docs.fast.ai/callbacks.one_cycle.html)\n",
    "- [1cycle-learning-rate-policy](https://iconof.com/1cycle-learning-rate-policy/)\n",
    "\n",
    "## The Problem of Learning Rates\n",
    "\n",
    "One method *learning rate scheduling* gradually decreases the learning rate until training concludes. This does method does not take into account the error surface during gradient descent.\n",
    "\n",
    "Another method *adaptive learning rate schedules* modify the weight update process to adjust the learning rate based on the dynamics of the error surface. 3. says that this is computationally expensive, but I don't see how that could be (more research)\n",
    "\n",
    "__Cyclical Learning Rates__ *this method lets the learning rate cyclically vary between reasonable boundary values. Training with cyclical learning rates instead of fixed values achieves improved classification accuracy without a need to tune and often in fewer iterations* [paper abstract](https://arxiv.org/abs/1506.01186)\n",
    "\n",
    "### Finding reasonable bounds\n",
    "\n",
    "__LR range test__ essentially just train the model for a few epochs while gradually increasing the learning rate. Weight for the model to diverge.\n",
    "\n",
    "## One Cycle Learning\n",
    "\n",
    "__Super Convergence__ [paper](https://arxiv.org/abs/1708.07120) NNs train faster with one learning rate cycle and a large maximum learning rate.  \n",
    "*The cycle’s size must be smaller than the total number of iterations/epochs. After the cycle is complete, for the remaining iteration/epochs learning rate should decrease even further, several orders of magnitude less than its initial value. Smith named this the 1cycle policy.*[*](https://iconof.com/1cycle-learning-rate-policy/)\n",
    "\n",
    "__Momentum__ *To accompany the movement toward larger learning rates, Leslie found in his experiments that decreasing the momentum led to better results. This supports the intuition that in that part of the training, we want the SGD to quickly go in new directions to find a flatter area, so the new gradients need to be given more weight. In practice, he recommends to pick two values likes 0.85 and 0.95, and decrease from the higher one to the lower one when we increase the learning rate, then go back to the higher momentum as the learning rate goes down.*[*](https://sgugger.github.io/the-1cycle-policy.html)\n",
    "\n",
    "__Learning Rate Finder__ * That's why when we run the Learning Rate Finder, it's very important to use it with the exact same conditions as during our training. For instance different batch sizes or weight decays will impact the results*[*](https://sgugger.github.io/the-1cycle-policy.html)\n",
    "\n",
    "__1cycle Regularization__ *Training with the 1cycle policy at high learning rates is a method of regularization in itself, so we shouldn't be surprised if we have to reduce the other forms of regularization we were previously using when we put it in place. It will however be more efficient, since we can train for a long time at large learning rates.*[*](https://sgugger.github.io/the-1cycle-policy.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TensorFlow Implementation\n",
    "\n",
    "\n",
    "## Learning Rate Finder\n",
    "\n",
    "*Over an epoch begin your SGD with a very low learning rate (like 10−8) but change it (by multiplying it by a certain factor for instance) at each mini-batch until it reaches a very high value (like 1 or 10). Record the loss each time at each iteration and once you're finished, plot those losses against the learning rate.* [*](https://sgugger.github.io/how-do-you-find-a-good-learning-rate.html)\n",
    "\n",
    "*the learning rate that corresponds to the minimum value is already a bit too high, since we are at the edge between improving and getting all over the place. We want to go one order of magnitude before, a value that's still aggressive (so that we train quickly) but still on the safe side from an explosion.*[*](https://sgugger.github.io/how-do-you-find-a-good-learning-rate.html)\n",
    "\n",
    "__Modifications__\n",
    "1. Smooth the recorder losses for each minibatch with exponentially weighted averages. $avg loss_{i} = \\beta*avg loss_{i-1} + (1-\\beta)*loss_{i}$ $\\beta$ between 0 and 1. Close to 1 means smoother. Plot the following \n",
    "$smoothed loss_{i}=\\frac{avg loss_{i}}{1-\\beta^{i+1}}$\n",
    "2. Stopping criteria... $smoothed loss_i>4*min smoothed loss$\n",
    "3. lr step increase - at each step $lr_i=lr_0*q^i$ where $q=(\\frac{lr_{N-1}}{lr_0})^{\\frac{1}{N-1}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-09T22:10:25.365666Z",
     "start_time": "2020-02-09T22:10:22.268105Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.1.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.callbacks import Callback\n",
    "import tensorflow.keras.backend as K\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-09T22:10:25.384646Z",
     "start_time": "2020-02-09T22:10:25.367660Z"
    }
   },
   "outputs": [],
   "source": [
    "class find_lr(Callback):\n",
    "    \"\"\"\n",
    "    Modified from https://sgugger.github.io/how-do-you-find-a-good-learning-rate.html\n",
    "    \n",
    "    Prograsses through a single epoch, increasing the learning rate until the \n",
    "    NN diverges. should be run in the same conditions as training (same batch size)\n",
    "    \n",
    "    The max_lr should be one order of magnitude smaller than the graph minimum.\n",
    "    \"\"\"\n",
    "    def __init__(self, num_batches,  init_value = 1e-6, final_value=10., beta = 0.98):\n",
    "        self.init_value = init_value\n",
    "        self.final_value = final_value\n",
    "        self.beta = beta\n",
    "        self.q = (final_value / init_value) ** (1/(num_batches-1)) #l.r. multiplier: see formula above. \n",
    "        self.avg_loss = 0.\n",
    "        self.best_loss = 0.\n",
    "        self.batch_num = 0\n",
    "        self.losses = []\n",
    "        self.log_lrs = []\n",
    "        \n",
    "    def on_train_begin(self, logs={}):\n",
    "        if not hasattr(self.model.optimizer, 'lr'):\n",
    "            raise ValueError('Optimizer must have a \"lr\" attribute.')\n",
    "        logs = logs or {}\n",
    "        K.set_value(self.model.optimizer.lr, self.init_value)\n",
    "         \n",
    "    def on_batch_end(self, batch, logs=None):\n",
    "        logs = logs or {}\n",
    "        self.batch_num += 1\n",
    "        lr = K.get_value(self.model.optimizer.lr)\n",
    "        \n",
    "        #Compute the smoothed loss\n",
    "        self.avg_loss = self.beta * self.avg_loss + (1-self.beta) * logs['loss']\n",
    "        smoothed_loss = self.avg_loss / (1 - self.beta**self.batch_num)\n",
    "\n",
    "        #Stop if the loss is exploding\n",
    "        if self.batch_num > 1 and smoothed_loss > 4 * self.best_loss:\n",
    "            return self.log_lrs, self.losses\n",
    "\n",
    "        #Record the best loss\n",
    "        if smoothed_loss < self.best_loss or self.batch_num==1:\n",
    "            self.best_loss = smoothed_loss\n",
    "\n",
    "        #Store the values\n",
    "        self.losses.append(smoothed_loss)\n",
    "        self.log_lrs.append(math.log10(lr))\n",
    "        \n",
    "        #increase the lr \n",
    "        K.set_value(self.model.optimizer.lr, lr * self.q )\n",
    "        \n",
    "    def plot_lr(self):\n",
    "        plt.xlabel('L.R. (log_10)')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.plot(self.log_lrs, self.losses)  \n",
    "\n",
    "        locs, labels = plt.xticks()\n",
    "        for i, lab in enumerate(labels):\n",
    "            labels[i] = \"1e{}\".format(str(int(locs[i])))\n",
    "        plt.xticks(locs, labels)\n",
    "        plt.xlim(-6,1)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementation Test\n",
    "\n",
    "https://github.com/nathanhubens/KerasOneCycle/blob/master/OneCycle.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-09T22:10:25.887987Z",
     "start_time": "2020-02-09T22:10:25.386756Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000 train samples\n",
      "10000 test samples\n",
      "(60000, 28, 28, 1)\n",
      "(60000, 10)\n"
     ]
    }
   ],
   "source": [
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data(path='mnist.npz')\n",
    "x_train = x_train.reshape(x_train.shape[0], 28, 28, 1)\n",
    "x_test = x_test.reshape(x_test.shape[0], 28, 28, 1)\n",
    "\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "x_train /= 255\n",
    "x_test /= 255\n",
    "\n",
    "num_classes = 10\n",
    "y_train = tf.keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = tf.keras.utils.to_categorical(y_test, num_classes)\n",
    "\n",
    "print(x_train.shape[0], 'train samples')\n",
    "print(x_test.shape[0], 'test samples')\n",
    "print(x_train.shape)\n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-09T22:10:25.896967Z",
     "start_time": "2020-02-09T22:10:25.889954Z"
    }
   },
   "outputs": [],
   "source": [
    "input_shape = (28, 28, 1)\n",
    "def Net():\n",
    "    model = tf.keras.models.Sequential([\n",
    "        tf.keras.layers.Conv2D(32, kernel_size=(3,3), \n",
    "                               activation='relu', input_shape = input_shape),\n",
    "        tf.keras.layers.Conv2D(64, kernel_size=(3,3), activation='relu'),\n",
    "        tf.keras.layers.MaxPool2D(pool_size=(3,3)),\n",
    "        tf.keras.layers.Dropout(0.25),\n",
    "        tf.keras.layers.Flatten(),\n",
    "        tf.keras.layers.Dense(128, activation='relu'),\n",
    "        tf.keras.layers.Dropout(0.3),\n",
    "        tf.keras.layers.Dense(10, activation='softmax')\n",
    "    ])\n",
    "    model.compile(loss = tf.keras.losses.categorical_crossentropy,\n",
    "                 optimizer = tf.keras.optimizers.SGD(),\n",
    "                 metrics=['acc'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-09T22:10:33.005703Z",
     "start_time": "2020-02-09T22:10:25.897933Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples\n",
      "60000/60000 [==============================] - 6s 96us/sample - loss: 2.1750 - acc: 0.2073\n"
     ]
    }
   ],
   "source": [
    "bs = 128\n",
    "model = Net()\n",
    "cb = find_lr(x_train.shape[0]/bs)\n",
    "hist = model.fit(x_train, y_train, batch_size=bs, epochs=1, callbacks=[cb])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-09T22:10:33.137324Z",
     "start_time": "2020-02-09T22:10:33.007673Z"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "cb.plot_lr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-09T22:10:33.159264Z",
     "start_time": "2020-02-09T22:10:33.139317Z"
    }
   },
   "outputs": [],
   "source": [
    "#####################\n",
    "##### OneCycle  #####\n",
    "#https://github.com/nathanhubens/KerasOneCycle/blob/master/utils/keras_OneCycle.py\n",
    "#####################\n",
    "\n",
    "          \n",
    "class OneCycle(Callback):\n",
    "    \"\"\"This callback implements a cyclical learning rate and momentum policy (CLR).\n",
    "    The method cycles the learning rate between two boundaries with\n",
    "    some constant frequency, as detailed in this paper (https://arxiv.org/abs/1506.01186).\n",
    "    The amplitude of the cycle can be scaled on a per-iteration \n",
    "    For more detail, please see paper.\n",
    "    \n",
    "    # Example\n",
    "        ```python\n",
    "            clr = OneCycle(min_lr=1e-3, max_lr=1e-2,\n",
    "                      min_mtm=0.85, max_mtm=0.95,\n",
    "                      annealing=0.1,step_size=np.ceil((X_train.shape[0]*epochs/batch_size)))\n",
    "            model.fit(X_train, Y_train, callbacks=[clr])\n",
    "        ```\n",
    "    \n",
    "    # Arguments\n",
    "        min_lr: initial learning rate which is the\n",
    "            lower boundary in the cycle.\n",
    "        max_lr: upper boundary in the cycle. Functionally,\n",
    "            it defines the cycle amplitude (max_lr - min_lr).\n",
    "        step_size: number of training iterations in the cycle. To define as `np.ceil((X_train.shape[0]*epochs/batch_size))`\n",
    "        max_mtm : initial value of the momentum    \n",
    "        min_mtm : lower boundary in the cycle.\n",
    "        annealing_stage : percentage of the iterations where the lr\n",
    "                    will decrease lower than its min_lr\n",
    "        annealing_rate : in annealing phase learning rate will be decreased to annealing_rate*min_lr\n",
    "                    \n",
    "        # References\n",
    "        Original paper: https://arxiv.org/pdf/1803.09820.pdf\n",
    "        Inspired by : https://sgugger.github.io/the-1cycle-policy.html#the-1cycle-policy\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, min_lr=1e-5, max_lr=1e-2, min_mtm = 0.85, max_mtm=0.95, training_iterations=1000.,\n",
    "                 annealing_stage=0.1, annealing_rate=0.01):\n",
    "\n",
    "        self.min_lr = min_lr\n",
    "        self.max_lr = max_lr\n",
    "        self.min_mtm = min_mtm\n",
    "        self.max_mtm = max_mtm\n",
    "        self.annealing_stage = annealing_stage\n",
    "        self.step_size = training_iterations*(1-self.annealing_stage)/2\n",
    "        self.min_annealing_lr = annealing_rate * min_lr\n",
    "        self.iterations = 0.\n",
    "        self.training_iterations = training_iterations\n",
    "        self.history = {}\n",
    "        \n",
    "    def clr(self):\n",
    "        if self.iterations < 2*self.step_size :\n",
    "            x = np.abs(self.iterations/self.step_size - 1)\n",
    "            return self.min_lr + (self.max_lr-self.min_lr)*(1-x)\n",
    "        else:\n",
    "            x = min(1, float(self.iterations - 2 * self.step_size) / (self.training_iterations - 2 * self.step_size))\n",
    "            return self.min_lr - (self.min_lr - self.min_annealing_lr) * x\n",
    "        \n",
    "    \n",
    "    def cmtm(self):\n",
    "        if self.iterations < 2*self.step_size :   \n",
    "            x = np.abs(self.iterations/self.step_size - 1)\n",
    "        else: \n",
    "            x=1\n",
    "        return self.min_mtm + (self.max_mtm-self.min_mtm)*(x)     \n",
    "        \n",
    "    def on_train_begin(self, logs={}):\n",
    "        if not hasattr(self.model.optimizer, 'lr'):\n",
    "            raise ValueError('Optimizer must have a \"lr\" attribute.')\n",
    "        logs = logs or {}\n",
    "        K.set_value(self.model.optimizer.lr, self.min_lr)\n",
    "        K.set_value(self.model.optimizer.momentum, self.max_mtm)\n",
    "         \n",
    "    def on_batch_end(self, batch, logs=None):\n",
    "        \n",
    "        logs = logs or {}\n",
    "        self.iterations += 1\n",
    "    \n",
    "        self.history.setdefault('lr', []).append(K.get_value(self.model.optimizer.lr))\n",
    "        self.history.setdefault('momentum', []).append(K.get_value(self.model.optimizer.momentum))\n",
    "        self.history.setdefault('iterations', []).append(self.iterations)\n",
    "\n",
    "        for k, v in logs.items():\n",
    "            self.history.setdefault(k, []).append(v)\n",
    "        \n",
    "        #increase/decrease the lr and momentum\n",
    "        K.set_value(self.model.optimizer.lr, self.clr()) \n",
    "        K.set_value(self.model.optimizer.momentum, self.cmtm())\n",
    "        \n",
    "    def plot_lr(self):\n",
    "        plt.xlabel('Training Iterations')\n",
    "        plt.ylabel('Learning Rate')\n",
    "        plt.title(\"CLR - '1 cycle' Policy\")\n",
    "        plt.plot(self.history['iterations'], self.history['lr'])\n",
    "        \n",
    "    def plot_mtm(self):\n",
    "        plt.xlabel('Training Iterations')\n",
    "        plt.ylabel('Momentum')\n",
    "        plt.title(\"CLR - '1 cycle' Policy\")\n",
    "        plt.plot(self.history['iterations'], self.history['momentum'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-09T22:10:38.514887Z",
     "start_time": "2020-02-09T22:10:33.161260Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "60000/60000 [==============================] - 5s 87us/sample - loss: 0.7941 - acc: 0.7475 - val_loss: 0.1934 - val_acc: 0.9424\n"
     ]
    }
   ],
   "source": [
    "bs = 128\n",
    "model = Net()\n",
    "cb = OneCycle(min_lr=1e-2,max_lr=1e-1)\n",
    "hist = model.fit(x_train, y_train, validation_data=(x_test,y_test), \n",
    "                 batch_size=bs, epochs=1, callbacks=[cb])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-09T22:12:39.741340Z",
     "start_time": "2020-02-09T22:12:39.736351Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'loss': [0.7941481598695119],\n",
       " 'acc': [0.7475],\n",
       " 'val_loss': [0.19344778183698655],\n",
       " 'val_acc': [0.9424]}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hist.history"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:deep-learning]",
   "language": "python",
   "name": "conda-env-deep-learning-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
