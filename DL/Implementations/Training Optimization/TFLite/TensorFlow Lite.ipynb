{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-06T18:41:28.677318Z",
     "start_time": "2020-04-06T18:41:25.301426Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.1.0\n"
     ]
    }
   ],
   "source": [
    "#GETTING THE CIFAR DATASET READY FOR EXPERIMENTATION\n",
    "import tensorflow_datasets as tfds\n",
    "import tensorflow as     tf\n",
    "import math\n",
    "import numpy             as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.layers import *\n",
    "import pathlib\n",
    "\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-06T18:41:30.153605Z",
     "start_time": "2020-04-06T18:41:28.679314Z"
    }
   },
   "outputs": [],
   "source": [
    "from utils.cifar_functions import *\n",
    "\n",
    "#PREPARE THE CIFAR DATASET\n",
    "# download data and split into training and testing datasets\n",
    "dataset_train, info = tfds.load(\"cifar10\", split=tfds.Split.TRAIN, with_info=True)\n",
    "dataset_test,  info = tfds.load(\"cifar10\", split=tfds.Split.TEST,  with_info=True)\n",
    "\n",
    "dataset_train = dataset_train.map(pre_processing_train, num_parallel_calls=4)\n",
    "dataset_train = dataset_train.shuffle(buffer_size=TRAINING_SHUFFLE_BUFFER)\n",
    "dataset_train = dataset_train.batch(TRAINING_BATCH_SIZE)\n",
    "dataset_train = dataset_train.prefetch(buffer_size=3)\n",
    "\n",
    "# transform testing dataset\n",
    "dataset_test = dataset_test.map(pre_processing_test, num_parallel_calls=4)\n",
    "dataset_test = dataset_test.batch(TRAINING_BATCH_SIZE)\n",
    "dataset_test = dataset_test.prefetch(buffer_size=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TensorFlow Lite\n",
    "\n",
    "\n",
    "## Overview ([link](https://www.tensorflow.org/lite/guide/get_started#4_optimize_your_model_optional))\n",
    "\n",
    "To use a TensorFlow model you must __convert it into TFLite format__. You cannot create a model with TFLite, you must convert an existing tensorflow model to TFLite.\n",
    "\n",
    "TFLite is designed to execute models efficiently for low resource settings. Converting models reduces their file size, further optimizations increase speed decrease size with some tradeoffs.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TensorFlow Lite Converter\n",
    "\n",
    "The converter can convert from: Keras models and SavedModel directories. Converts the model into a `.tflite` file.\n",
    "\n",
    "We will be working with an existing TensorFlow model. Specifically we will be using an implementation of MobileNet V2 from [this notebook](https://github.com/harrisonjansma/2020_Notes/blob/master/DL/Implementations/CV/Image_Recognition/0_ResNet_and_MobileNet_V2.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-06T18:41:41.054916Z",
     "start_time": "2020-04-06T18:41:30.155598Z"
    }
   },
   "outputs": [],
   "source": [
    "#SavedModel from mobilenet_v2 implementation\n",
    "model_dir = \"F://Models/Model_Design/mobilenet_v2/\"\n",
    "model = tf.keras.models.load_model(model_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-06T18:41:54.347764Z",
     "start_time": "2020-04-06T18:41:41.056911Z"
    }
   },
   "outputs": [],
   "source": [
    "#convert to tflite model\n",
    "converter = tf.lite.TFLiteConverter.from_saved_model(model_dir)\n",
    "tflite_model = converter.convert()\n",
    "\n",
    "#Create a TFLite subdirectory in F://Models\n",
    "base_dir = pathlib.Path(model_dir).parent.parent / \"TFLite/\"\n",
    "base_dir.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-06T18:41:54.370699Z",
     "start_time": "2020-04-06T18:41:54.348759Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4129644"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#save the converted model to .tflite file\n",
    "tflite_path = base_dir/'mobilenet_v2.tflite'\n",
    "tflite_path.write_bytes(tflite_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `Interpreter` object can be called with keywords `model_content=tflite_obj` or `model_path=path/to/file.tflite`. \n",
    "\n",
    "After the interpreter has been instantiated, `allocate_tensors()` will retrieve the tensors required in the model graph. Using `get_input/output_details()` will give the shape and index of the input/output tensors. Use this later when running inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-06T18:41:54.382694Z",
     "start_time": "2020-04-06T18:41:54.371697Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'name': 'input_2', 'index': 168, 'shape': array([ 1, 28, 28,  3]), 'dtype': <class 'numpy.float32'>, 'quantization': (0.0, 0)}]\n"
     ]
    }
   ],
   "source": [
    "# Load TFLite model and allocate tensors.\n",
    "interpreter = tf.lite.Interpreter(model_path=str(tflite_path))\n",
    "interpreter.allocate_tensors()\n",
    "\n",
    "# Get input and output tensors.\n",
    "input_details = interpreter.get_input_details()\n",
    "output_details = interpreter.get_output_details()\n",
    "\n",
    "#shows the retrieved input data\n",
    "input_shape = input_details[0]['shape']\n",
    "print(input_details)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TFLite Inference ([tutorial](https://www.tensorflow.org/lite/guide/inference))\n",
    "\n",
    "The __TensorFlow Lite Interpreter__ takes a model file and executes its operation. The interpreter has APIs in many different languages and is designed to be lean and dast.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-06T18:41:56.827815Z",
     "start_time": "2020-04-06T18:41:54.384688Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.3530305e-09 8.1705935e-03 2.9588188e-03]\n",
      "tf.Tensor([1.3530357e-09 8.1705702e-03 2.9588216e-03], shape=(3,), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "#create a random numpy array  of same shape as input\n",
    "input_data = np.array(np.random.random_sample(input_shape), dtype=np.float32)\n",
    "#sets the network-input tensor w/ above data\n",
    "interpreter.set_tensor(input_details[0]['index'], input_data)\n",
    "\n",
    "#release computation\n",
    "interpreter.invoke()\n",
    "\n",
    "# The function `get_tensor()` returns a copy of the tensor data.\n",
    "# Use `tensor()` in order to get a pointer to the tensor.\n",
    "tflite_results = interpreter.get_tensor(output_details[0]['index'])\n",
    "\n",
    "# Test the original TensorFlow model on random input data.\n",
    "tf_results = model(input_data)\n",
    "\n",
    "# Compare the results.\n",
    "for tf_result, tflite_result in zip(tf_results, tflite_results):\n",
    "    np.testing.assert_almost_equal(tf_result, tflite_result, decimal=5)\n",
    "\n",
    "print(tflite_results[0,:3])\n",
    "print(tf_results[0,:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GPU Acceleration\n",
    "\n",
    "TFLite Interpreter can be specifes to make use of hardware acceleration."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Optimization\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quantization\n",
    "\n",
    "[github tutorial](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/performance/post_training_quant.ipynb)\n",
    "\n",
    "TFLite allows quantization during model conversion. Activations are always stored in floating point. Some ops allow quantized kernels, prior to op, activations are quantized to 8-bits of precision dynamically, then dequantized to float after processing.\n",
    "\n",
    "With post training quantization, you must test to see if performance degradation is acceptable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Quantizing an existing model__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-06T18:42:10.617202Z",
     "start_time": "2020-04-06T18:41:56.830805Z"
    }
   },
   "outputs": [],
   "source": [
    "#Set optimizer flag to optimize for size\n",
    "converter = tf.lite.TFLiteConverter.from_saved_model(model_dir)\n",
    "converter.optimizations = [tf.lite.Optimize.OPTIMIZE_FOR_SIZE]\n",
    "tflite_quant_model = converter.convert()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-06T18:42:10.633160Z",
     "start_time": "2020-04-06T18:42:10.619197Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1091672"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tflite_model_quant_file = base_dir/\"mobilenet_v2_quant.tflite\"\n",
    "tflite_model_quant_file.write_bytes(tflite_quant_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Comparing model sizes__ (on disk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-06T18:42:10.644132Z",
     "start_time": "2020-04-06T18:42:10.635153Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16.893877 MB\n"
     ]
    }
   ],
   "source": [
    "#SavedModel size\n",
    "out = sum(f.stat().st_size for f in pathlib.Path(model_dir).glob('**/*') if f.is_file() )\n",
    "print(out/1e6, \"MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-06T18:42:10.654104Z",
     "start_time": "2020-04-06T18:42:10.645127Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.129644 MB\n"
     ]
    }
   ],
   "source": [
    "#tflite size w/out quantizaton \n",
    "out = tflite_path.stat().st_size\n",
    "print(out/1e6, \"MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-06T18:42:10.668066Z",
     "start_time": "2020-04-06T18:42:10.656099Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.091672 MB\n"
     ]
    }
   ],
   "source": [
    "#tflite size w quantization\n",
    "out = tflite_model_quant_file.stat().st_size\n",
    "print(out/1e6, \"MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Running Inference__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-06T18:42:10.686053Z",
     "start_time": "2020-04-06T18:42:10.671058Z"
    }
   },
   "outputs": [],
   "source": [
    "#instantiate the Interpreters for both quantized and unquantized models\n",
    "interpreter = tf.lite.Interpreter(model_path=str(tflite_path))\n",
    "interpreter.allocate_tensors()\n",
    "\n",
    "interpreter_quant = tf.lite.Interpreter(model_path=str(tflite_model_quant_file))\n",
    "interpreter_quant.allocate_tensors()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure that the data that is input is the same shape and datatype as the input tensor of the model. We can check this with `interpreter.get_input_details()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-06T18:42:10.702994Z",
     "start_time": "2020-04-06T18:42:10.688013Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'name': 'input_2',\n",
       "  'index': 168,\n",
       "  'shape': array([ 1, 28, 28,  3]),\n",
       "  'dtype': numpy.float32,\n",
       "  'quantization': (0.0, 0)}]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "interpreter.get_input_details()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Currently the dataset we used for testing `dataset_test` is full of 28x28 cifar10 images and there corresponding labels. Note that the batch size is greater than 1, so we will have to `.unbatch()` after we benchmark the original tensorflow model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-06T18:42:10.805718Z",
     "start_time": "2020-04-06T18:42:10.704987Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 28, 28, 3)\n",
      "<dtype: 'float32'>\n"
     ]
    }
   ],
   "source": [
    "#loooking at the dims of input data\n",
    "for im,lab in dataset_test:\n",
    "    print(im.shape)\n",
    "    print(im.dtype)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-06T18:42:10.813724Z",
     "start_time": "2020-04-06T18:42:10.807713Z"
    }
   },
   "outputs": [],
   "source": [
    "def evaluate_model(interpreter):\n",
    "    input_index = interpreter.get_input_details()[0]['index']\n",
    "    output_index = interpreter.get_output_details()[0]['index']\n",
    "    \n",
    "    num_correct=0\n",
    "    num_predictions=0\n",
    "    for image, label in dataset_test:\n",
    "        image = np.expand_dims(image.numpy(), axis=0)\n",
    "        interpreter.set_tensor(input_index, image)\n",
    "        \n",
    "        interpreter.invoke()\n",
    "\n",
    "        output = interpreter.tensor(output_index)\n",
    "        digit = np.argmax(output()[0])\n",
    "\n",
    "        if digit==label:\n",
    "            num_correct+=1\n",
    "        num_predictions+=1\n",
    "        \n",
    "    accuracy = num_correct * 1.0 / num_predictions\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-06T18:42:15.871371Z",
     "start_time": "2020-04-06T18:42:10.815718Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    157/Unknown - 5s 32ms/step - loss: 0.2933 - accuracy: 0.9215"
     ]
    }
   ],
   "source": [
    "#unquantized model performance\n",
    "test_loss, test_accuracy = model.evaluate(x=dataset_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-06T18:42:15.881846Z",
     "start_time": "2020-04-06T18:42:15.873363Z"
    }
   },
   "outputs": [],
   "source": [
    "#will only run inference one image at a time\n",
    "#with the tflite interpreter\n",
    "dataset_test = dataset_test.unbatch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-06T18:45:11.265625Z",
     "start_time": "2020-04-06T18:42:15.883829Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9215"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_model(interpreter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-06T18:55:10.887815Z",
     "start_time": "2020-04-06T18:45:11.267621Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9212"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#no error change but took 3 as long?\n",
    "evaluate_model(interpreter_quant)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pruning\n",
    "https://www.tensorflow.org/model_optimization/guide/pruning/train_sparse_models\n",
    "\n",
    "https://www.tensorflow.org/lite/performance/post_training_quantization\n",
    "\n",
    "https://www.tensorflow.org/model_optimization/guide/pruning/train_sparse_models\n",
    "\n",
    "https://www.tensorflow.org/model_optimization/guide/pruning/pruning_with_keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.tensorflow.org/lite/guide/hosted_models"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:deep-learning]",
   "language": "python",
   "name": "conda-env-deep-learning-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
