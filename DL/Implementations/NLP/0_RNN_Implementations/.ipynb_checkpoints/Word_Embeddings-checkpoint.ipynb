{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP \n",
    "-------------------\n",
    "## 1. Basic Embedding Model\n",
    "\n",
    "### Neural Network Language Model (Predict Next Word)\n",
    "\n",
    "[A Neural Probabilistic Language Model - Bengio (2003)](http://www.jmlr.org/papers/volume3/bengio03a/bengio03a.pdf)\n",
    "__Statistical language models__ learn the joint probability function of sequences of words in languages. Curse of dimensionality is a major challenge for this task. (Many, many possible combinations of words.)\n",
    "\n",
    "Traditional models were n-gram based, concatenated short sequences of words seen in the training set.\n",
    "\n",
    "__Learning a distributed representation for words__ the model learns a representation for each word and the probability function for words sequences. Unseen sentences can be represented if composed of words similair in representation to previously seen words.\n",
    "\n",
    "#### Intro\n",
    "Curse of dimensionality is a bitch in langauge models. The  task is much harder for discrete variables because conditional prob. may not exhibit local smoothness, and there are many, many combinations of variables.\n",
    "\n",
    "*Pˆ(wT ) =∏Pˆ(wt|wt−1)*\n",
    "\n",
    "**statistical language model** can be represnted as the conditional prob. of the next words given the previous ones. This can be improved by accounting for word order. \n",
    "\n",
    "**n-grams** play off the idea that temporally closer words in a sequence are statistically more dependent. (ie there are common themes of what words occur next to each other.) n-gram models construct conditional probabilities of the next word given the large numebr of contexts (ie n-grams of the last n-1 words.)\n",
    "\n",
    "What happens when a new combination of n words appears\n",
    "that was not seen in the training corpus?\n",
    "\n",
    "#### Distributed Representations\n",
    "\n",
    "1. associate with each word in the vocabulary a *distributed word feature vector* (a real valued vector in Rm),\n",
    "\n",
    "2. express the *joint probability function* of word sequences in terms of the feature vectors of these words in the sequence, and\n",
    "\n",
    "3. learn simultaneously the word feature vectors and the parameters of that probability function.\n",
    "\n",
    "The feature vector represents different aspects of the word: each word is associated with a point\n",
    "in a vector space. The probability function is expressed as a product of conditional probabilities of the next word given the previous ones, (e.g. using a multilayer neural network to predict the next word given the previous ones, in the experiments). This function has parameters that can be iteratively tuned in order to maximize the log-likelihood of the training data.\n",
    "\n",
    ". In the proposed model, it will so generalize because “similar” words\n",
    "are expected to have a similar feature vector, and because the probability function is a smooth\n",
    "function of these feature values, a small change in the features will induce a small change in the\n",
    "probability. Therefore, the presence of only one of the above sentences in the training data will increase the probability, not only of that sentence, but also of its combinatorial number of “neighbors”\n",
    "in sentence space\n",
    "\n",
    ". In the model proposed here, instead\n",
    "of characterizing the similarity with a discrete random or deterministic variable (which corresponds\n",
    "to a soft or hard partition of the set of words), we use a continuous real-vector for each word,\n",
    "\n",
    "\n",
    "#### A Neural Model\n",
    "\n",
    "We decompose the function f(wt ,··· ,wt−n+1) = Pˆ(wt|wt−1\n",
    "1 ) in two parts:\n",
    "1. A mapping C from any element i of V to a real vector C(i) ∈ Rm. It represents the distributedfeature vectors associated with each word in the vocabulary. In practice, C is represented bya |V| ×m matrix of free parameters\n",
    "\n",
    "2. The probability function over words, expressed with C: a function g maps an input sequence\n",
    "of feature vectors for words in context, (C(wt−n+1),··· ,C(wt−1)), to a conditional probability\n",
    "distribution over words in V for the next word wt.\n",
    "\n",
    "The function g may be implemented by a\n",
    "feed-forward or recurrent neural network or another parametrized function, with parameters ω. The\n",
    "overall parameter set is θ = (C,ω). Training is achieved by looking for θ that maximizes the training corpus penalized log-likelihood:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = [\"I like burritos\", \"I love coffee\", 'I hate cheese']\n",
    "\n",
    "word_list = ' '.join(sentences).split()\n",
    "word_list = list(set(word_list))\n",
    "word_dict = {w: i for i, w in enumerate(word_list)} #word encodings\n",
    "number_dict = {i: w for i, w in enumerate(word_list)}\n",
    "\n",
    "n_class = len(word_dict)\n",
    "\n",
    "n_step = 2 #number of steps\n",
    "n_hidden = 2 # number of hidden units\n",
    "\n",
    "def make_batch(sentences):\n",
    "    input_batch=[]\n",
    "    target_batch=[]\n",
    "    for sen in sentences: #grab a sentence/\n",
    "        word =sen.split()  #get words into list\n",
    "        Input = [word_dict[n] for n in word[:-1]] #map to word_dict for all words except last\n",
    "        target = word_dict[word[-1]] #get encoding of target word\n",
    "        input_batch.append(np.eye(n_class)[Input]) #OHE for the words before target\n",
    "        #2D array of OHE vectors for each word (len_sentence-1, n_class)\n",
    "        target_batch.append(np.eye(n_class)[target])\n",
    "    return input_batch, target_batch\n",
    "\n",
    "#----------------------------------------------\n",
    "#TENSORFLOW\n",
    "#model\n",
    "X = tf.placeholder(tf.float32, [None, n_step, n_class]) #2d matrix with OHE for each word up to target\n",
    "Y = tf.placeholder(tf.float32, [None, n_class]) #OHE shape (n_class)\n",
    "\n",
    "Input = tf.reshape(X,shape=[-1, n_step*n_class]) #stacks the OHE vectors of sentence to one dim shape, [batch_size, n_sep*n_class]\n",
    "H = tf.Variable(tf.random_normal([n_step*n_class, n_hidden])) #hidden coeffs, one coeff for each input dim\n",
    "#2D array n_step*n_class coeffs one for each input, n_hidden is number of hidden units\n",
    "d = tf.Variable(tf.random_normal([n_hidden])) #outputs for each hidden unit\n",
    "U = tf.Variable(tf.random_normal([n_hidden, n_class])) #coefficients mapping hidden ouputs to word_classes\n",
    "b = tf.Variable(tf.random_normal([n_class])) #Model output OHE for word_class prediction\n",
    "\n",
    "tanh = tf.nn.tanh(d+tf.matmul(Input,H)) #activation function for the hidden layer\n",
    "#??? WHY add to d?\n",
    "model =tf.matmul(tanh, U) + b #multiply hidden activ. output with terminal coeffs.\n",
    "# add to terminal bias?\n",
    "\n",
    "#reduce_mean computes the mean of elements in Tensor across axis\n",
    "cost  = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=model,labels=Y)) #softmax maps outputs to bounded probs for each class.\n",
    "#entropy measures error of each class prob to the True class label Y\n",
    "optimizer = tf.train.AdamOptimizer(0.001).minimize(cost) #optimization function LR 0.001\n",
    "prediction = tf.argmax(model,1) #outputs the maximum probability class\n",
    "\n",
    "#training\n",
    "init = tf.global_variables_initializer()\n",
    "sess = tf.Session()\n",
    "sess.run(init)\n",
    "\n",
    "input_batch, target_batch = make_batch(sentences) #here n_step is 2 since all sentences have length 3 (predict last word)\n",
    "for epoch in range(10000):\n",
    "    _,loss = sess.run([optimizer, cost], feed_dict={X:input_batch, Y:target_batch})\n",
    "    if (epoch+1)%1000==0:\n",
    "        print(\"Epoch:\", '%04d' %(epoch+1), 'cost =','{:.6f}'.format(loss))\n",
    "\n",
    "#Predictions\n",
    "predict = sess.run([prediction], feed_dict={X: input_batch})\n",
    "\n",
    "Input = [sen.split()[:2] for sen in sentences]\n",
    "print(Input,'->', [number_dict[n] for n in predict[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#--------------------------------------------\n",
    "#PYTORCH\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "\n",
    "dtype = torch.FloatTensor\n",
    "\n",
    "class NNLM(nn.Module):\n",
    "    der __init__(self):\n",
    "        super(NNLM,self).__inint__()\n",
    "        self.C = nn.Embedding()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word2Vec (Skip-Gram)  Embedding Words and Show Graph 2013\n",
    "\n",
    "[Distributed Representations of Words and Phrases\n",
    "and their Compositionality](https://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf)\n",
    "\n",
    "\n",
    "**Distributed vector representations** learn continuous vectors for each word in a language that can be reused across all contexts (distributed). The first implementation learned representations by predicting the next word given prior, (since the task was also to build a probablistic model that predicts the next word) **If we double down on learning a representation, we can use skip-gram to encode more information about the context of a word.**\n",
    "\n",
    "These embeddings encode syntactic and semmantic word relationships.\n",
    "\n",
    "#### Introduction\n",
    "Distributed representations of words in a vector space help learning algorithms to achieve better\n",
    "performance in natural language processing tasks by grouping similar words.\n",
    "\n",
    "The skip-gram model was introduced to learn distr. repr. from large amounts of unstructured text. Unlike previous implementations, training the skip-gram model does not involve dense matrix multiplication. (->efficient)\n",
    "\n",
    "The word representations computed using neural networks are very interesting because the learned\n",
    "vectors explicitly encode many linguistic regularities and patterns.\n",
    "\n",
    "\n",
    "#### Skip-Gram Model\n",
    "Skip-gram model objective is to find word representations that are useful for predicting the surrounding words in a sentence or document.\n",
    "\n",
    "<img src='img/skip-gram.png'>\n",
    "\n",
    " More formally, given a sequence of training words w1, w2, w3, . . . , wT , the objective of the Skip-gram model is to maximize the average log probability\n",
    "<img src='img/skip-func.png'>\n",
    "where c is the size of the training context. Larger\n",
    "c results in more training examples and thus can lead to a higher accuracy, at the expense of the training time.\n",
    "\n",
    "\n",
    "##### Hierarchical Softmax"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
