{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cuda\n",
    "\n",
    "### CUDA Semantics\n",
    "[link](https://pytorch.org/docs/stable/notes/cuda.html)\n",
    "`torch.cuda` sets up and executes CUDA operations. Keeps track of current CUDA device, and allocates all CUDA tensors will be created on that device. Once a tendor is allocated, you can do operations on it irrespective of the selected CUDA device, and the results will always be placed on the same device as the tensor.\n",
    "\n",
    "Cross-GPU operations are not allowed, unless you enable peer-to-peer memory access\n",
    "\n",
    "\n",
    "### Asynchronous Execution\n",
    "\n",
    "By default, GPU operations are asynchronous. When you call a function that uses the GPU, the operations are enqueued to the particular device, but not necessarily executed until later. This allows us to execute more computations in parallel, including operations on CPU or other GPUs.\n",
    "\n",
    "In general, the effect of asynchronous computation is invisible to the caller, because (1) each device executes operations in the order they are queued, and (2) PyTorch automatically performs necessary synchronization when copying data between CPU and GPU or between two GPUs. Hence, computation will proceed as if every operation was executed synchronously.\n",
    "\n",
    "### Memory Management\n",
    "\n",
    "PyTorch uses a __caching memory allocator__ to speed up memory allocations. This allows fast memory deallocation without device synchronizations. However, the unused memory managed by the allocator will still show as if used in nvidia-smi. You can use `memory_allocated()` and `max_memory_allocated()` to monitor memory occupied by tensors, and use `memory_cached()` and `max_memory_cached()` to monitor memory managed by the caching allocator. Calling `empty_cache()` __releases all unused cached memory from PyTorch__ so that those can be used by other GPU applications. However, the occupied GPU memory by tensors will not be freed so it can not increase the amount of GPU memory available for PyTorch.\n",
    "\n",
    "### Device-agnostic Code\n",
    "Can use `torch.cuda.is_available()` then set device to 'cpu' or 'cuda' based on the result.\n",
    "\n",
    "### Pinned memory buffers\n",
    "\n",
    "Host to GPU copies are much faster when they originate from pinned (page-locked) memory. CPU tensors and storages expose a `pin_memory()` method, that returns a copy of the object, with data put in a pinned region.\n",
    "\n",
    "Also, __once you pin a tensor or storage, you can use asynchronous GPU copies__. Just pass an additional `non_blocking=True` argument to a `to()` or a `cuda()` call. This can be used to overlap data transfers with computation.\n",
    "\n",
    "You can make the DataLoader return batches placed in pinned memory by passing `pin_memory=True` to its constructor.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-19T20:02:16.567709Z",
     "start_time": "2019-10-19T20:02:16.115221Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch \n",
    "device = torch.device('cuda')\n",
    "\n",
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking GPU Memory\n",
    "`torch.cuda.max_memory_allocated(device=None)`\n",
    "\n",
    "Returns the maximum GPU memory occupied by tensors in bytes for a given device.\n",
    "\n",
    "By default, this returns the peak allocated memory since the beginning of this program. `reset_max_memory_allocated()` can be used to reset the starting point in tracking this metric. For example, these two functions can measure the peak allocated memory usage of each iteration in a training loop.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-19T20:04:42.115570Z",
     "start_time": "2019-10-19T20:04:42.110583Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.max_memory_allocated())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-19T20:09:57.115558Z",
     "start_time": "2019-10-19T20:09:57.110571Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "108000256\n"
     ]
    }
   ],
   "source": [
    "x = torch.rand(300,300,300, device=device)\n",
    "torch.cuda.reset_max_memory_allocated()\n",
    "print(torch.cuda.max_memory_allocated())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-19T20:10:35.799668Z",
     "start_time": "2019-10-19T20:10:35.794681Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "108000256"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Returns the current GPU memory occupied by tensors in bytes for a given device.\n",
    "torch.cuda.memory_allocated()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`torch.cuda.max_memory_cached(device=None)`\n",
    "\n",
    "Returns the maximum GPU memory managed by the caching allocator in bytes for a given device.\n",
    "\n",
    "By default, this returns the peak cached memory since the beginning of this program. `reset_max_memory_cached()` can be used to reset the starting point in tracking this metric. For example, these two functions can measure the peak cached memory amount of each iteration in a training loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-19T20:09:59.249438Z",
     "start_time": "2019-10-19T20:09:59.244485Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "220200960\n"
     ]
    }
   ],
   "source": [
    "torch.cuda.reset_max_memory_cached()\n",
    "print(torch.cuda.max_memory_cached())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-19T20:08:28.212017Z",
     "start_time": "2019-10-19T20:08:28.199049Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "111149056"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.memory_cached()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow]",
   "language": "python",
   "name": "conda-env-tensorflow-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
